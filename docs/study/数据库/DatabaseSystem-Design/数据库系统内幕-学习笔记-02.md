# 第二部分 分布式系统

​	没有分布式系统，我们将无法拨打电话、转账或远距离交换信息。我们每天都在使用分布式系统。有时候，即使没有明确说明，任何客户端/服务器架构的应用程序其实都是分布式系统。

​	对于许多现代软件系统，垂直扩展(将软件运行在更大更快的机器上，配备更多的CPU、RAM或更快的磁盘)是不现实的。更大的机器也更贵，更难以置换，而且可能需要特殊的维护。一个替代选项是<u>水平扩展：将软件运行在多个用网络相连的机器上，而在逻辑上视为单个实体</u>。

​	分布式系统有各种规模，少则几台机器，多则上百台机器。系统参与者的特性也各不相同，可以是手持式或传感器设备，也可能是高性能计算机。

​	数据库系统运行在单个节点上的时代已经过去很久了，大多数现代数据库系统拥有多个以集群方式相互连接的节点，以增加存储容量、提升性能和增强可用性。

​	尽管某些分布式计算的理论突破不是新生事物，但大部分的实际应用还是在近期才出现的。今天，这一主题越来越受到人们的关注，我们也看到更多的相关研究与新的发展正在进行。

---

+ 第二部分基本定义

  ​	分布式系统有若干个参与者(participant，有时也称为进程、节点或副本)，每个参与者都有其自己的本地状态。参与者通过在通信链路上交换消息来进行彼此间的通信。

  ​	进程可以用时钟来获取时间，它可以是逻辑的，也可以是物理的。<u>逻辑时钟是用单调递增的计数器实现的。物理时钟也被称为挂钟(wall clock)，它与物理世界的时间概念紧密相连，可以通过进程本地的方式(例如通过操作系统)获取到</u>。

  ​	说到分布式系统就不得不提到一点：系统中的各个部分彼此分开放置，这本身就带来了很大的困难。远程进程间的通信链路可能既慢又不可靠，这导致确定远程进程的状态变得更复杂。

  ​	**分布式系统领域中的大多数研究都与"没有什么是完全可靠的"这一事实有关：通信信道可能会延迟、乱序甚至无法传递消息；进程可能会暂停、变慢、崩溃、失控或突然停止响应**。

  ​	并行编程和分布式编程领域中有许多共同主题，因为CPU也可以看作一个具备通信链路、进程和通信协议的微型的分布式系统。你将在11.5节中看到分布式编程和并发编程的许多相似之处。但是多数的原语无法在二者之间直接复用，这是因为远程通信的成本要大得多，并且通信链路和进程是不可靠的。

  ​	为了克服分布式环境的困难，我们需要用到一类特殊的算法——分布式算法，它定义了本地和远程的状态以及执行的概念，即便在不稳定的网络或发生组件故障的情况下也能工作。为了解释这些算法，我们会用到术语状态和步骤(或阶段， phase)，并描述它们之间的转换(transition)。毎个进程都在本地执行算法步骤，而本地执行以及进程之间的交互构成了分布式算法。

  ​	分布式算法描述了本地的行为和多个独立节点的交互过程。节点通过相互发送消息进行通信。算法定义了参与者的角色、要交换的消息、状态、转换、执行步骤、传递介质的特性、时序假设、故障模型，以及其他描述进程和进程间交互的特性。

  ​	分布式算法有很多不同的用途:

  + **协调**

    监督若干工作者的动作和行为的进程。

  + **合作**

    多个参与者互相依靠，共同完成任务。

  + **分发**

    进程相互配合，将信息快速而可靠地分发给感兴趣的进程。

  + **共识**

    在多个进程间达成共识。

  ​	本书中，我们将站在使用的角度讨论算法，相比纯学术的材料，我们更倾向于实用的方法。首先，我们会介绍所有必要的抽象，包括进程和它们间的联系，并逐步构建出更复杂的通信模式。我们将会从UDP开始：使用UDP时，发送者无法确认它的消息是否已送达目的地。最终，在系统中达成共识，即多个进程都认同某个特定值。

# 8. 简介与概述

## 8.1 并发执行

​	一旦两个执行线程都能访问变量，除非我们在线程间同步这些步骤，否则这些并发步骤的执行结果是无法预知的。

​	即便仅在单个节点上，我们就已经遇到了分布式系统中的第一个问题：并发。每个并发程序都具有分布式系统的某些特性。线程访问共享状态，在本地执行一些运算，再将结果传回共享变量。

​	为了精确定义执行历史并减少可能的结果数量，我们需要一致性模型。一致性模型描述并发执行的过程，并且确定了运算执行以及对其他参与者可见的顺序。使用不同的一致性模型，我们可以约束或放松系统可能的状态数量。

​	分布式系统和并发计算在术语和学术研究上有许多重叠之处，但也存在一些差异。并发系统中存在共享内存，进程可以用它来交换信息。在分布式系统中，各个进程拥有自己的本地状态，参与者之间通过传递消息进行通信。

> **并发与并行**
>
> ​	我们经常互换使用并发和并行计算这两个术语，但是这两个概念在语义上有细微的差异。当两个步骤序列并发执行时，二者都在进行中，但任意时刻都只有其中一个在执行。当两个步骤序列并行执行时，它们的步骤可以(在某一时刻)同时执行。并发的操作时间上存在重叠，而并行的操作由多个处理器执行[WEIKUM01]
>
> ​	Erlang编程语言的创建者 Joe Armstrong举过一个例子：并发执行就像一台咖啡机前排了两队，而并行执行就像两台咖啡机前排了两队。即便如此，绝大部分资料都用术语"并发"来描述拥有多个并行执行线程的系统，而"并行"这个词则很少见。

---

+ 分布式系统中的共享状态

  ​	我们可以尝试在分布式系统中引入共享内存的概念，例如，单一信息源(比如数据库)。

  ​	即使我们解决了并发访问的问题，我们依然无法保证所有进程都是同步的。

  ​	为了访问数据库，进程需要通过通信介质发送和接受消息，以查询或修改状态。但是，如果一个进程很久都没有从数据库得到响应会如何？为了回答这个问题，我们首先要定义什么是很久。为此，必须<u>从同步性的角度来描述系统：通信是否是全异步的？是否存在某些时序假设？如果存在的话，这些时序假设将允许我们引入操作超时和重试机制</u>。

  ​	我们无从知晓数据库没有响应是因为过载、不可用、响应太慢还是网络问题。这描述了崩溃的本质——进程可能以各种方式崩溃：可能因某种原因无法继续执行后面的算法步骤；可能遇到了临时性的故障；也可能是消息丟失。我们需要定义一个**故障模型**并描述故障可能发生的方式，然后再决定如何处理它们。

  ​	<u>如果系统在故障发生时仍然能继续正常运行，我们将这样的特性称为**容错性**</u>。故障是不可避免的，所以我们需要构建出具有可靠组件的系统。消除单点故障，比如前文提到的单节点数据库，可能是我们朝此方向迈出的第一步。我们可以引入一些冗余，增设备份数据库。然而这就引出了另一个问题：如何使共享状态的多个副本保持同步?

  ​	到目前为止，在我们这个简单系统中引入共享状态所带来的问题比答案还多。现在我们知道，共享状态不像引入数据库那样简单，还必须采取更细化的方法，即基于消息传递来描述各个独立进程之间的交互。

## 8.2 分布式计算的误区

​	理想情况下，当两台计算机在网络上通信时，一切都能正常工作：进程开启一个连接、发送数据、收到响应，每个人都很开心。但是假设所有操作总会成功并且没有任何错误是很危险的，因为当某些东西出问题时，我们的假设也就不成立了，那时系统的行为将变得难以预测。

​	大多数时候，假设网络可靠是合理的。网络至少在一定程度上可靠才能有用。我们都曾经历过这样的情况，当我们尝试连接到远程服务器时，却收到了一个"网络不可达"的错误。即使能建立连接，一个成功的初始连接也无法保证这条链路是稳定的，连接随时可能中断。消息可能送达了对端，但对端的响应却可能丢失了，也有可能在对端的响应发送之前连接就中断了。

​	网络交换机会有故障，电缆可能断开，网络配置也随时可能发生变化。我们构建系统时需要适当地处理所有这些情况。

​	连接可以是稳定的，但我们不能期望远程调用能像本地调用一样快。我们应尽可能少地对延迟做出假设，并且永远不要假设延迟为零。一条消息要想到达远程服务器，需要穿过若干个软件层和一个物理媒介(比如光纤或电缆)，所有这些操作都不是瞬间完成的。

​	Michael Lewis在他所著的书Flash Boys(Simon and Schuster公司出版)中讲述了这样个故事，公司花费数百万美元把延迟降低几毫秒，从而能比竞争对手更快地访问交易所。这是一个把延迟作为竞争优势的绝佳例子，然而值得一提的是，根据其他一些研究，比如文献[BARTLETTI6]，过时报价套利(通过比竞争对手更快地得知价格并执行交易来获取利润)并不能使快速交易者从市场中获利。

​	从上述教训当中学习，我们增加了重试和重连机制，并去掉了关于瞬间执行的假设，但是事实证明这还不够：**当我们增加消息的数量、发送速率和大小，并向现有网络中添加新的进程时，我们不应该假设带宽是无限的**。

> 1994年， Peter Deutsch发布过一个如今很有名的断言列表，标题为"分布式计算的误区"，描述了分布式计算中易被忽视的一些方面。除了网络可靠性、延迟和带宽假设，它还提到了其他问题，比如，网络的安全性、可能存在的攻击者、有意或无意的拓扑变化都可能打破我们的一些假设，这些假设包括：某一资源存在性和所在位置，网络传输所消耗的时间和资源，以及最后——存在一个拥有整个网络的知识和控制权的权威个体。

​	Deutsch的列表可以说非常详尽，但它侧重于通过链路传递消息时可能出错的地方。这些担忧是合理的，而且描述了最通用、最底层的复杂性，但不幸的是，在设计和实现分布式系统时，我们还做出了很多其他假设，这些假设也可能在运行中导致问题。

### 8.2.1 处理

​	在远程进程响应刚刚收到的消息之前，它还需要在本地执行一些工作，因此我们不能假定处理是瞬时完成的。只考虑网络延迟还不够，因为远程进程执行的操作也不是立即完成的。

​	此外，我们还无法保证消息送达后会立刻被处理。消息可能会进入远程服务器的等待队列中，等到所有更早到达的消息处理完后才被处理。

​	节点可能相距很近，也可能很远，各节点可能有不同的CPU、内存和磁盘配置，可能运行不同的软件版本和配置。我们不能期望它们以相同的速度处理请求。<u>如果完成一项任务需要等待几个并行工作的远程服务器响应，则**整个执行的完成时间取决于最慢的服务器**</u>。

​	与普遍存在的看法相反，队列容量并非是无限的，堆积更多的请求不会对系统有任何好处。当生产者产生消息的速度大于消费者能够处理的速度时，我们可以使用**背压(backpressure)**策略减慢生产者的速度。背压是分布式系统中人们了解和应用最少的概念之一，通常是事后才建立，而不是将其视为系统设计必需的一个组成部分。

​	尽管增加队列容量听起来像是个好主意——可以帮助我们管道化、并行化以及有效地调度请求，但是，如果消息仅仅是停在队列中等待处理，什么也不会发生。**增大队列大小可能对延迟产生负面影响，因为这并不会改善处理速度**。

​	通常，进程本地队列是用于实现以下目标：

+ **解耦**

  使接收和处理在时间上分开，并各自独立发生。

+ **流水线化**

  不同阶段的请求由系统中独立的部分处理。负责接收消息的子系统不用阻塞到上条消息处理完成。

+ **吸收瞬时突发流量**

  系统负载可能经常变化，但是请求到达的间隔时间对负责处理请求的组件是隐藏的。总体的系统延迟会由于排队而增加，但这通常仍比响应失败并重试请求更好。

​	队列大小取决于工作负载和应用程序。对于相对稳定的工作负载，我们可以通过测量任务处理时间以及各任务的平均排队时间来确定队列大小，从而确保在提升吞吐量的同时，延迟仍保持在可接受的范围内。在这种情况下，队列大小相对较小。对于不可预测的工作负载，可能会出现任务提交的突发流量，这时队列大小也应当考虑突发流量和高负载。

​	即使远程服务器可以快速地处理请求，也并不意味着我们总是能获得正面的响应。它也可能回应一个失败：无法进行写操作、要查找的值不存在或是触发了bug。总之，即使是最顺利的情况也需要我们的关注。

### 8.2.2 时钟和时间

​	假设不同的远程计算机上的时钟都同步也很危险。再加上延迟为零以及处理是瞬时的这些假设，将会导致不同的特质，尤其是在时序和实时数据处理中。例如，当从时间感知不同的参与者收集和聚合数据时，你必须了解它们之间的时间漂移并相应地对时间进行归一化，而不是依赖源时间戳。除非使用特殊的髙精度时间源，否则不能依赖时间戳进行同步或排序。当然，这并不意味着我们完全不能或不该依赖时间：说到底，<u>任何同步系统都依靠本地时钟实现超时</u>。

​	**我们必须始终注意进程之间可能存在的时间误差，以及传递和处理消息所需的时间**。例如，Spanner(参见13.5节)使用特殊的时间API，该API返回时间戳和不确定性界限以施加严格的事务顺序。一<u>些故障检测算法依赖于共享的时间概念，要求时钟漂移始终在允许的范围内才能确保正确性</u>[GUPTA01]。

​	除了分布式系统中的时钟同步非常困难之外，当前时间也在不断变化：你可以从操作系统请求当前的POSIX时间戳，并在执行几个步骤后请求另一个当前时间戳，两次结果是不同的。尽管这是一个明显的现象，但是了解时间的来源以及时间戳捕获的确切时刻至关重要。

​	了解时钟源是否是单调的(即永远不会后退)，以及与调度时间相关的操作可能偏移多少，可能也会有所帮助。

### 8.2.3 状态一致性

​	之前说到的假设大多属于"几乎总是错的"一类，但是，还有一些假设最好归入"并非总是对的"一类。这类假设帮助我们走思维捷径，通过以特定方式思考来简化模型，忽略某些棘手的边缘情形。

​	分布式算法并不总是保证状态严格一致。<u>一些方法具有较宽松的约束，允许各副本之间的状态存在分歧，并依赖冲突解决(检测和解决系统内分歧状态的能力)和读取时数据修复(读取期间，当各副本响应不同结果时，使副本恢复同步的能力)</u>。有关这些概念的更多信息参见第12章。假定状态在节点间完全一致可能会导致难以察觉的bug。

​	最终一致的分布式数据库可能具有这样的逻辑：读取时通过查询Quorum的节点来处理副本不一致，但是假定数据库表结构和集群视图是强一致的。除非我们确保这些信息的一致性，否则依赖该假设可能会造成严重的后果。

​	例如， Apache Cassandra曾有一个bug，其原因是表结构变更在不同时刻传播到各个服务器。如果在表结构传播过程中尝试从数据库读取数据，则可能会读到损坏的数据，因为一台服务器以某种表结构进行编码，而另一台服务器使用不同的表结构对其进行解码。

​	另一个例子是由环的视图分歧引起的bug：如果一个节点假定另一个节点保存了某个键的数据记录，但另一个节点具有不同的集群视图，此时读写数据可能会导致数据记录被错误放置，或是获得一个空的响应，虽然数据实际上好端端地存放在另一个节点上。

​	即使完全的解决方案成本很高，我们也最好事先考虑各种可能的问题。通过了解和处理这些情况，你能以更自然的方式解决问题，比如内置防护措施或修改设计。

### 8.2.4 本地和远程执行

​	将复杂性隐藏在API内部可能很危险。例如，对于本地数据集上的一个迭代器，即使你对存储引擎不熟悉，也可以合理地推测内部行为。理解远程数据集上的迭代过程则是个完全不同的问题：你需要理解一致性、传递语义、数据协调、分页、合并、并发访问含义以及许多其他事情。

​	简单地将两者隐藏在同一个接口后，即便有用，也可能会产生误导。调试、配置和可观察性可能需要额外的API参数。我们应该始终牢记，**本地执行和远程执行是不同的**[ WALDO96]。

​	**隐藏远程调用最明显的问题是延迟：远程调用的成本比本地调用高很多倍，因为它涉及<u>双向网络传输、序列化/反序列化</u>以及许多其他步骤**。交错使用本地调用和阻塞的远程调用可能会导致性能下降和预期之外的副作用[VINOSKIO08]。

### 8.2.5 处理故障的需要

​	刚开始构建系统的时候，我们可以假设所有节点都可以正常工作，但如果总是这么想就很危险了。在长时间运行的系统中，节点可能会关机维护(通常会有个优雅关闭的过程)或因为种种原因(例如软件问题、内存耗尽(out-of-memory killer [KERRISK10])、运行时bug、硬件问题等)而崩溃。<u>进程会发生故障，而你能做的最好的事情就是做好准备并知道如何处理它们</u>。

​	如果远程服务器没有响应，我们并不总是知道确切的原因。这可能是由系统崩溃、网络故障、远程进程或中间链路太慢导致的。<u>一些分布式算法使用**心跳协议**和**故障检测**机制来确定哪些参与者还活着且可达</u>。

### 8.2.6 网络分区和部分故障

​	<u>当两个或更多服务器无法相互通信时，我们称这种情况为**网络分区**</u>。Seth Gilbert和Nancy Lynch在 Perspectives on the CAP Theorem[GILBERT12]中区分了以下两种情况：两个参与者无法相互通信；几组参与者彼此隔开，无法交换消息并继续运行算法。

​	网络的总体不可靠性(数据包丢失、重传、延迟难以预测)令人烦恼但尚可容忍，而网络分区则会造成更多的麻烦，因为各个独立的分组可以继续执行并产生冲突的结果。**网络链路的故障也可能是不对称的：消息仍然能从一个进程传递到另一个进程，反之则不行**。

​	为了构建在一个或多个进程出现故障的情况下仍健壮的系统，我们必须考虑**部分故障**的情况[TANENBAUM06]，如何让系统在部分不可用或运行不正常的情况下仍能继续工作。

​	故障很难检测，并且在系统的不同部分看来，不总是以相同的方式可见。设计高可用性系统时，我们应该始终考虑边缘情形：如果我们确实复制了数据却没有收到确认该怎么办？要重试吗？在发送了确认的节点上，数据仍可用于读取吗？

​	**墨菲定律告诉我们故障一定会发生**。编程界又补充道，故障将以最坏的方式发生。因此，作为分布式系统工程师，我们的工作是尽可能减少可能出现错误的场景，并为故障做好准备——包括这些故障可能导致的破坏。

​	避免一切故障是不可能的，但我们仍可以构建一个弹性的系统，使之在故障出现时仍然能正常运行。**<u>设计应对故障的最佳方式是进行故障测试</u>**。我们无法考虑清楚毎种可能的故障场景，并预测多个进程的行为。最好的解决方法就是通过测试工具来制造网络分区、模拟比特位腐烂[GRAY05]、增加延迟、使时钟发生偏移以及放大相对处理速度。现实世界中分布式系统的设置可能是对抗性的、不友好的，甚至是“有创造性的”(然而以非常敌对的方式)，因此测试工作应当尝试覆盖尽可能多的场景。

> 过去几年中出现了一些开源项目，它们能帮助我们构造出各种故障场景。
>
> + Toxiproxy用于<u>模拟网络问题</u>：<u>限制带宽、引入延迟、超时</u>等。
> +  Chaos Monkey的方法更为激进，它通过<u>随机关闭服务使工程师直面生产环境故障的风险</u>。
> + CharybdeFS<u>模拟文件系统及硬件错误与故障</u>。你可以用这些工具来测试软件以确保在这些故障出现时软件仍能正确工作。 
>
> + CrashMonkey是一个与文件系统无关的记录-重放-测试框架，用于测试持久性文件的数据及元数据一致性。

​	设计分布式系统时，我们必须认真考虑容错性、弹性，以及可能的故障场景和边缘情形。类似于"足够多的眼睛，就可让所有问题浮现"，我们可以说足够大的集群最终定会命中所有可能的问题。与此同时，只要有足够多的测试，我们最终能够发现每个存在的问题。

### * 8.2.7 级联故障

​	我们做不到总是完全隔离故障：**被高负载压垮的进程会增加集群其余部分的负载，从而使其他节点更有可能发生故障。级联故障能够从系统的一部分传播到另一部分，扩大了问题的范围**。

​	有时，级联故障甚至可能来源于完全善意的目的。例如，某个节点离线了一段时间，因而没有接收到最近的更新。当它恢复在线时，乐于助人的其他节点希望帮助它追赶上最近的变化，于是开始向它发送缺失的数据，而这又导致网络资源耗尽，或是导致该节点启动后短时间内再次发生故障。

> ​	为了防止系统的故障扩散并妥善处理故障场景，我们可以使用**断路器(或熔断机制)**。在电气工程中，断路器可通过中断电流来保护昂贵且难以更换的部件，使其免受电流过载或短路的影响。在软件开发中，熔断机制会监视故障，并使用**回退(fallback)**机制保护整个系统：避免使用出故障的服务，给它一些时间进行恢复，并妥善处理失败的调用。

​	<u>当与某一台服务器的连接失败或服务器没有响应时，客户端将开始循环重连。那时候，过载的服务器已经难以应付新的连接请求，因而客户端的循环重试也无济于事。为了避免这一情况，我们可以使用**退避(backoff)**策略，客户端不要立即重试，而是等待一段时间</u>。

​	**退避通过合理安排重试、增加后续请求之间的时间窗口来避免问题扩大**。

​	退避用于增加单个客户端的请求间隔。但是，<u>使用相同退避策略的多个客户端也会产生大量负载</u>。为了防止多个客户端在退避期之后同时重试，我们可以引入**抖动(Jitter)**。<u>抖动在退避上增加了一个小的随机时间间隔，从而降低了多个客户端同时醒来并重试的可能性</u>。

​	<u>硬件故障、比特位腐烂和软件错误都会导致数据损坏，而损坏的数据会通过标准的传递机制传播</u>。如果没有适当的验证机制，系统可能将损坏的数据传播到其他节点，甚至可能覆盖未损坏的数据记录。为了避免这一情况，我们应该采用**校验和(checksum)**以及验证机制，来验证节点之间交换的任何内容的完整性。

​	**通过计划和协调执行可以避免过载和热点问题。<u>相比于让各个对等节点独立执行操作步骤，我们可以用协调器来依据可用资源准备一份执行计划，并根据过去的执行数据来预测负载</u>**。

​	总之，我们应该始终考虑这样的情形：系统某一部分的故障可能导致其他地方也出现问题。我们应该为系统装备上**熔断**、**退避**、**验证**和**协调**机制。<u>处理被隔离的小问题总比从大规模故障中恢复更简单</u>。

​	我们用整整一节讨论了分布式系统中的问题和潜在的故障场景，但是我们应当将其视为警告，而不是被它们吓跑。

​	了解什么会出问题，并仔细设计和测试我们的系统，可以让它更健壮、更具弹性。了解这些问题可以帮助你在开发过程中识别、发现潜在的问题根源，也能帮助你在生产环境中调试

## * 8.3 分布式系统抽象

​	讨论编程语言时，我们使用<u>通用术语</u>并用<u>函数</u>、<u>运算符</u>、<u>类</u>、<u>变量</u>和<u>指针</u>来定义我们的程序。通用的词汇可以帮助我们避免每次都为了描述某些东西而发明新词。我们的定义越精确、越没有歧异，听众也就越容易理解。

​	在开始学习算法之前，我们首先要了解分布式系统中的词汇：这些定义你会经常在演讲书籍和论文中遇到。

---

+ **链路**

  ​	网络是不可靠的：消息会丢失、延迟或被打乱。记住这一点之后，我们来尝试构建几种通信协议。我们从最不可靠的协议开始，确定它们可能处于的状态，然后找出可以为协议增加的东西使它提供更好的保证。

  **公平损失链路**

  ​	我们可以从两个进程开始，它们之间以**链路**相连。进程可以相互发送消息。任何通信介质都是不完美的，消息可能丢失或延迟。

  ​	看看我们能得到什么样的保证。消息M被发送之后(从发送方的角度来看)，它可能处于以下状态之一：

  + 还未送达进程B(但会在某个时间点送达)
  + 在途中丢失且不可恢复
  + 成功送达远程进程

  ​	注意，发送方没有任何方法确定消息是否已经送达。在分布式系统的术语中，这种链路称为**公平损失(fair-loss)**。这种链路具有以下属性：

  **公平损失**

  ​	如果发送方和接收方都是正确的，且发送方无限多次重复发送，则消息最终会被送达。

  **有限重复**

  ​	发送的消息不会被送达无限次。

  **不会无中生有**

  ​	链路不会自己生成消息。换句话说，它不会传递一个从未发送过的消息。

  ​	公平损失链路是一种很有用的抽象，它是构建具有更强保证的通信协议的基石。我们可以假设该链路不会在通信双方之间*系统性地*丢弃消息，也不会创建新消息。但与此同时，我们也不能完全依靠它。这可能让你想起了用户数据报协议(UDP)，UDP允许我们从一个进程发送消息到另一个进程，但在协议层面上不提供可靠的传输语义。

  **消息确认**

  ​	为了改善这一情况、更清晰地获得消息状态，我们可以引入**确认(acknowledgment)机制**：接收方通知发送方消息已送达。为此，我们需要双向通信信道，并增加一些措施以区分不同的消息，例如序列号——单调递增的唯一消息标识符。

  > ​	每个消息只要有唯一标识符就足够了。序列号只是唯一标识符的一种特殊情况，即使用计数器来获取标识符，从而实现唯一性。<u>当使用哈希算法来唯一地标识消息时，我们应当考虑可能的冲突，并确保能消除歧义</u>。

  ​	现在，进程A可以发送消息M(n)，其中n是单调递增的消息计数器。B收到消息后立即向A发送确认ACK(n)。

  ​	确认消息，就像原始消息一样，也有可能在途中丟失。消息可能处于的状态数会稍有变化。在A收到确认之前，该消息仍处于我们前面提到的三种状态之一，但是，一旦A收到确认，就可以确信该消息已送达B。

  **消息重传**

  ​	增加确认机制仍不足以保证通信协议完全可靠：发送的消息仍可能会丢失，远程进程也可能在确认之前发生故障。为了解决该问题并提供送达保证，我们可以尝试**重传**(retransmit)。重传是指发送方重试可能失败的操作。我们之所以说可能失败，是因为发送方并不能真的知道有没有失败，因为我们要讨论的链路<u>不使用确认机制</u>。

  ​	进程A发送消息M之后，它将等到超时T被触发，然后尝试再次发送同一条消息。假设进程之间的链路完好无损，进程间的网络分区不会无限持续下去，并且并非所有数据包都丢失，我们可以认为，从发送方的角度看，消息要么尚未送达进程B，要么已经成功送达。由于A一直在尝试发送消息，可以认为传输过程中不会发生不可恢复的消息丢失。

  ​	在分布式系统的术语中，这种抽象称为**顽固链路(stubborn link)**。之所以称为顽固，是因为发件人会<u>无限期地反复发送消息</u>，但是，由于这种抽象非常不切实际，因此我们需要将重试与确认结合起来。

  **重传的问题**

  ​	每当我们发送消息时，在收到远程进程的确认之前，我们无从得知消息的状态：可能已被处理，可能马上就要处理，也可能已经丟失，甚至可能在收到消息之前远程进程就崩溃了——上述的任意状态都是可能的。我们可以重试操作、再次发送消息，但这可能导致消息重复。<u>只有当我们要执行的操作是幂等时，处理重复消息才是安全的</u>。

  ​	<u>幂等(dempotent)的操作可以执行多次而产生相同的结果，且不会产生其他副作用</u>。例如，服务器关机操作可以是幂等的，第一次调用将发起关机，而所有后续调用都不会产生任何其他影响。

  ​	如果毎个操作都是幂等的，那我们可以少考虑一些传递语义，更多地依赖重传来实现容错，并以完全反应式的方式构建系统：为某些信号触发相应的操作，而不会引起预期之外的副作用。但是，操作不一定是幂等的，简单地假设它们幂等可能会导致集群范围的副作用。例如，向客户的信用卡收费不是幂等操作，绝对不可以重复收费多次。

  ​	在存在部分故障和网络分区的情况下，幂等性尤其重要，因为我们无法总是确定远程操作的确切状态——是成功还是失败，还是会马上被执行——我们只能等待更长的时间。<u>**保证毎个操作都是幂等的是不切实际的，因此我们需要在不改变实际操作语义的情况下，提供与幂等性等价的保证**。为此，我们可以使用去重来避免多次处理消息</u>。

  **消息顺序**

  ​	不可靠的网络给我们带来了两个问题：一是消息可能会乱序到达；二是由于重传某些消息可能会多次送达。我们已经引入了序列号，利用这些消息标识符我们可以在接收方确保先进先出(FIFO)的顺序。由于每条消息都有一个序列号，因此接收方可以跟踪下列信息：

  + n<sub>consecutive</sub>表示最大连续序列号：所有小于或等于该序列号的消息都已经收到，这些消息可以按顺序放到正确的位置上。
  + n<sub>processed</sub>表示最大已处理序列号：所有小于或等于该序列号的消息都已经按照原来的顺序被处理。此序列号可以用于去重。

  ​	如果收到的消息序列号不连续，接收方会将其放入重新排序缓冲区。例如，它在接收到序列号为3的消息后收到消息5，那我们就知道4还是缺失的，因此我们将5放在一旁，直到4到来，然后就能构造出原本的消息顺序。<u>由于通信构建在公平损失链路之上，可以认为n<sub>consecutive</sub>和n<sub>max_seen</sub>之间的消息最终一定会送达</u>。

  ​	接收方可以安全地丢弃收到的序列号小于等于n<sub>consecutive</sub>的消息，因为这些消息确定已经送达了。去重的工作原理是检查带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	去重的工作原理是检査带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	在分布式系统的术语中，这种类型的链路称为**完美链路**，它提供以下保证[CACHIN11]：

  **可靠传递**

  ​	正确的进程A发送一次到正确的进程B的每个消息**最终**都会被传递。

  **没有重复**

  ​	消息不会被传送多次。

  **不会无中生有**

  ​	与其他种类的链路一样，它只能传递实际由发送者发送过的消息。

  ​	这可能会让你想起TCP协议(但是，TCP仅在单个会话内保证可靠传递)。当然，上述模型仅仅是一种用于说明原理的简化表示。TCP中处理消息确认的模型更为复杂，它按组进行确认以减少协议层面的开销。另外，TCP具有选择性确认、流控、拥塞控制错误检测等很多其他功能，这些不在我们的讨论范围之内。

  **严格一次传递**

  ​	<u>关于是否可以做到严格一次传递(exactly-once delivery)这个问题已经有很多讨论。这里，语义和精确的措辞非常重要。**由于链路故障可能导致传递消息的第一次尝试无法成功，因此大多数实际的系统都采用至少一次传递(at-least-once delivery)，它确保了发送方将重试直到收到确认为止，否则就认为对方没有收到该消息**。还有一种传递语义是最多一次(at-most-once)：发送方仅仅发送消息而不期待得到任何确认</u>。

  ​	TCP协议的原理是将消息分成数据包，一个一个传输，然后在接收端将它们拼接到起。TCP可能会尝试重传某些数据包，并且可能有不止一次的传输会成功。由于TCP用序列号标记毎个数据包，即使某些数据包被发送多次，它也可以对其进行去重，确保接收方只会看到并处理一次该消息。<u>在TCP中，此保证仅对单个会话有效：如果消息被确认并处理，但是发送方在收到确认消息前连接就中断了，则应用程序并不知道此传递成功，取决于其逻辑，它可能会尝试再次发送消息</u>。

  ​	这意味着严格一次处理是个有趣的问题，因为重复的传送(或数据包传输)没有副作用，仅仅是链路尽力而为的产物。举个例子，<u>如果数据库节点仅接收到记录但还没将它持久化。在这种情况下传递已经完成了，但除非该记录可以被査到(换句话说，除非消息被传递并且处理了)，否则这次传递毫无用处</u>。

  ​	为了确保严格一次传递，各节点需要一个共同知识[HALPERN90]：每个节点都知道某件事，每个节点都知道其他所有节点也都知道这件事。<u>用简化的术语来说，**节点必须在记录状态上达成共识**：两个节点都认为该记录已经或者还未被持久化。正如本章之后会说的，这在**理论上是不可能的**，但在实践中，我们仍通过放宽协调的要求来使用这一概念</u>。

  ​	各种关于是否是严格一次发送的误解，大多是因为从不同协议和抽象层次上考虑该问题，以及对“传递”的不同定义。**要想建立可靠的链路，不可能不重复传送某些消息**。但是，<u>我们可以通过仅处理毎个消息一次并忽略重复消息，使得从发送方的角度来看是严格一次发送</u>。

  ​	现在，在建立了实现可靠通信的方法之后，我们可以继续前进，探寻实现分布式系统中进程间一致性和共识的方法。

## 8.4 两将军问题

> [【翻译】两军问题（Two Generals’ Problem） | 知研片语 (liuzhaocn.com)](https://www.liuzhaocn.com/?p=1235)

​	一个被广泛称为两将军问题的思想实验，是对分布式系统一致性的最著名的描述之一。

​	这个思想实验表明，**<u>如果链路可能发生故障并且通信是异步的，则不可能在通信的双方之间达成共识</u>**。尽管TCP具有完美链路的性质，但是务必记住：完美链路尽管被称为完美链路，并不能保证完美的传递。它们也不能保证参与方一直活着，而只关心传输本身。

​	想象现在有两支军队，分别由两位将军领导，准备进攻一座要塞城市。两支军队分别位于城市的两侧，只有在同时进攻的情况下才能获胜。

​	两位将军通过信使进行通信。他们已经制定了攻击计划，现在唯一需要达成共识的就是是否执行计划。该问题的变体包括：其中一位将军的级别较高，但需要确保攻击是有协调的；或者两位将军需要就确切时间达成共识。这些细节不会改变问题的定义：将军们需要达成一项共识。

​	将军们只需要对“他们都会发起进攻”这一事实达成共识。否则，攻击将无法成功。将军A发出一条消息MSG(N)，表明如果对方也同意的话，就在指定的时间发起进攻。

​	将军A送出信使之后，他不知道信使是否已经到达：信使可能会被抓而无法传达消息。当将军B收到消息时，他必须发送确认ACK(MSG(N))。一条消息由一方发送并由另一方确认。

​	传递确认消息的信使也可能会被抓而无法传达消息。B无从得知信使是否已成功送达确认消息。

​	为了确认这一点，B必须等待ACK(ACK(MSG(N)))，一个二阶的确认，用于确认A收到了确认。

​	**<u>无论将军们互相发送多少确认，他们始终距离安全地发起攻击还差一个ACK。将军们注定要怀疑最后一个确认消息是否已送达目的地</u>**。

​	注意我们没有做任何时序上的假设:将军间的通信是完全异步的。并没有一个上限约束将军必须在多长时间内做出回应。

## 8.5 FLP不可能定理

​	Fisher、Lynch和Paterson在论文中描述了一个著名的问题：FLP不可能问题[FISCHERI85] (FLP是作者姓氏的首字母)，论文讨论了一种共识形式：各进程启动时有一个初始值，并尝试就新值达成共识。算法完成后，所有正常进程上的新值必须相同。

​	如果网络完全可靠，很容易对特定值达成共识。但实际上，系统容易出现各式各样的故障，例如消息丟失、重复、阒络分区，以及进程缓慢或崩溃。

​	共识协议描述了这样一个系统：给定初始状态的多个进程，它将所有进程带入决定状态。一个正确的共识协议必须具备以下三个属性：

+ 一致性

  ​	协议达成的决定必须是一致的：每个进程都做出了决定且所有进程决定的值是相同的。否则我们就尚未达成共识。

+ 有效性

  ​	达成共识的值必须由某一个参与者提出，这意味着系统本身不能“提出”值。这也意味着这个值不是无关紧要(trivial)的：进程不能总是决定某个预定义的默认值。

+ 终止性

  ​	只有当所有进程都达到决定状态时，协议才算完成。

​	<u>文献[FISCHER85]假定处理过程是完全异步的，进程之间没有共享的时间概念。这样的系统中的算法不能基于超时，并且一个进程无法确定另一个进程是崩溃了还是仅仅运行太慢。论文表明，在这些假设下，不存在任何协议能保证在有限时间内达成共识。**完全异步的共识算法甚至无法容忍一个远程进程无通知地突然崩溃**</u>。

​	**<u>如果我们不给进程完成算法步骤设定一个时间上限，那么就无法可靠地检测出进程故障，也不存在确定性的共识算法</u>**。

​	但是，FLP不可能定理并不意味着我们要收拾东西回家(由于达成共识是不可能的)。**<u>它仅仅意味着我们不能总是在有限的时间内在一个异步系统中达成共识</u>**。实践中，系统至少会表现出一定程度的同步性，而要想解决共识问题还需要一个更完善的模型。

## 8.6 系统同步性

​	从FLP不可能定理中可以看出<u>**时序假设**是分布式系统的关键特征之一</u>。在异步系统中，我们不知道进程运行的相对速度，也不能保证在有限时间内或以特定顺序传递消息。进程可能要花无限长的时间来响应，而且无法总是可靠地检测到进程故障。

​	对异步系统的主要批评在于上述假设不切实际：进程不可能具有任意不同的处理速度，链路传递消息的时间也不会无限长。依赖时间能够简化推理，并提供时间上限的保证。

​	在异步模型中不一定能解决共识问题[FISCHER85]。而且，不一定能设计出高效的异步算法。<u>对于某些任务，切实可行的解决方案很可能需要依赖时间</u>[ ARJOMANDIS83]。

​	我们可以放宽一些假设，认为系统是同步的。为此我们引入了时间的概念。在同步模型下对系统进行推理要容易得多。它假定各进程的处理速度相近、传输延迟是有限的，并且消息传递不会花任意长的时间。

​	同步系统也可以表示为同步的进程本地时钟：两个进程本地时间源之间的时间差存在上限[CACHIN11]。

​	**在同步模型中设计系统可以使用超时机制**。我们可以构建更复杂的抽象，例如**领导者选举、共识、故障检测**以及基于它们的其他抽象。<u>这使得最佳情况的场景更加健壮，但是如果时序假设不成立则可能导致故障</u>。

​	例如：Raft共识算法(参见14.4节)中，可能最终有多个进程认为它们是领导者，为了解决该问题，我们强制滞后的进程接受其他进程成为领导者；故障检测算法(参见第9章)。可能会错误地将活动进程标记为故障，反之亦然。设计系统时，我们必须考虑这些可能性。

​	异步和同步模型的性质可以组合使用，我们可以将系统视为部分同步的。部分同步的系统具有同步系统的某些属性，但是消息传递、时钟漂移和相对处理速度的边界范围可能并不精确，并且仅在大多数时候成立[DWORK88]。

​	同步是分布式系统的基本属性：它对性能、扩展性和一般可解性有影响，并且有许多对系统正常工作来说是必要的因素。本书中讨论的一些算法就工作在同步系统的假设下。

## 8.7 故障模型

​	我们一直在提到故障这个词，但到目前为止，它还是一个十分宽泛的概念，可能包含多种含义。就像我们可以做出不同的时序假设那样，我们也可以假设存在不同种类的故障。故障模型准确地描述了分布式系统中的进程可能以怎样的方式崩溃，并基于这些假设来开发算法。例如，我们可以假设进程可能崩溃并且永远无法恢复，或者可以预期它将在一段时间后恢复，或者它可能会失控并且产生错误的值。

​	分布式系统中，进程互相依赖以共同执行算法，因此故障可能导致整个系统的执行错误。

​	我们将讨论分布式系统中现有的多种故障模型，例如崩溃、遗漏和任意故障。这个列表并非面面俱到，但它涵盖了在实际中的大多数重要场景。

### 8.7.1 崩溃故障

​	通常，我们期望进程正确执行算法的所有步骤。最简单的崩溃方式是进程停止执行接下来的算法步骤，并且不再发送任何消息给其他进程。换句话说，该进程崩溃了。大多数情况下，我们使用**崩溃-停止(crash-stop)**进程抽象的假设，它规定一旦进程崩溃就会保持这种状态。

​	该模型不假定该进程无法恢复，也不阻拦或试图阻止恢复。这仅仅意味着该算法的正确性或活动性不依赖于恢复过程。实际上，并没有什么东西会去阻止进程恢复、追上系统状态以及参与下一次的算法执行。

​	失败的进程无法再继续参与当前这一轮的协作。为恢复的进程分配一个新的、不同的ID不会使模型等价于崩溃-恢复模型(之后会讨论)，因为大多数算法使用预定义的进程列表，并且依据最多可容忍的故障数明确定义了故障的语义[CACHIN11]。

​	**崩溃-恢复(crash-recovery)**是另一种的进程抽象。在这个抽象中，进程停止执行算法步骤，但会在稍后恢复并尝试执行剩下的步骤。要想让恢复成为可能，需要在系统中引入持久状态以及恢复协议[SKEEN83]。允许崩溃-恢复的算法需要考虑所有可能的恢复状态，因为恢复的进程会尝试从最后一个已知的步骤开始继续执行。

​	想利用恢复的算法必须同时考虑状态和进程ID。在这种情况下，崩溃恢复也可以看作是遗漏故障的一种特殊情况，因为<u>从另一个进程的角度看，不可达的进程与崩溃再恢复的进程没什么区别</u>。

### 8.7.2 遗漏故障

​	另一个故障模式是**遗漏故障(omission fault)**。该模型假设故障进程跳过了某些算法步骤，或者无法执行这些步骤，或者执行过程对其他参与者不可见，或者无法与其他参与者通信。遗漏故障中包含了由于网络链路故障、交换机故障或网络拥塞而导致的网络分区。网络分区可以表示为单个进程或进程组之间的消息遗漏。进程崩溃可以模拟为遗漏所有该进程收发的消息。

​	如果进程的运行速度慢于其他参与者，发送响应比预期迟得多，那么对于系统的其余部分来说，这个节点看起来丢三落四的。慢节点没有完全停止，而是发送结果太慢，常常与其他节点不同步。

​	<u>如果本应执行某些步骤的算法跳过了这些步骤或者执行结果不可见时，就发生了遗漏故障</u>。例如，消息在送往接收方的途中丢失，而发送方就像消息发送成功时那样，没有再次发送而是继续运行，即使消息已经不可恢复地丟失了。遗漏故障也可能是由间歇性停顿、网络过载、队列满等引起的。

### 8.7.3 任意故障

​	最难以解决的故障种类是**任意故障**或**拜占庭故障(Byzantine fault)**：<u>进程继续执行算法步骤，但是以与违背算法的方式(例如，共识算法中的进程决定一个从未由任何参与者提出过的值)</u>。

​	此类故障可能是由于软件bug或运行不同版本算法的进程，在这种情况下，故障很容易被发现和理解。如果我们无法控制所有进程，并且其中一个进程有意地误导其他进程，则发现和理解故障会变得非常困难。

​	你可能在航空航天工业中听说过拜占庭式的容错：飞机和航天器的系统不会直接使用子部件传来的值，而是会对结果进行交叉验证。另一个广泛的应用是加密货币[GILAD17]，那里没有中央权威，节点被多方控制，并且敌对的参与者有强烈的动机通过提供错误响应来欺骗系统。

### 8.7.4 故障处理

​	我们可以通过构成进程组、在算法中引入冗余来掩盖故障：即使其中一个进程发生故障，用户也不会注意到[CHRISTIAN91]。

​	故障可能会带来一些性能损失：正常的执行依赖于进程可响应，而且系统必须回退到较慢的执行路径来处理故障和纠正错误。故障往往可以通过一些方式来避免，例如：代码审査、广泛的测试、引入超时重试机制确保消息送达，以及确保各算法步骤在本地按顺序执行。

​	我们这里介绍的大多数算法都基于崩溃-故障模型，并通过**引入冗余来解决故障**。这些假设帮助我们创造性能更好、更易于理解和实现的算法

## 8.8 本章小结

​	本章中，我们讨论了一些分布式系统的术语，并介绍了一些基本概念。我们讨论了分布式系统的固有困难和复杂性，这是由于系统组件不可靠性导致的：链路可能无法传递消息、进程可能崩溃、网络可能发生分区。

​	这些术语应该足够让我们继续讨论。本书的剩余部分将讨论分布式系统中常见的解决方案:我们将先回想下哪些地方可能会出问题，然后看看有哪些可用的选项。

# * 9. 故障检测

​	为了使系统对故障做出适当的反应，应该及时检测故障。其他进程可能联系发生错误的进程，即使它无法响应，这会增加延迟并降低整个系统的可用性。

​	在异步分布式系统中检测故障(即不做任何时序假设)是极其困难的，因为我们无法判断进程是崩溃了，还是运行缓慢而需要无限长的时间来响应。我们在8.5节中讨论了与此相关的问题。

​	诸如死亡(dead)、失效(failed)和崩溃(crashed)等术语通常用于描述完全停止执行其步骤的进程。而<u>诸如无响应(unresponsive)、有故障(faulty)和缓慢(slow)等术语用于描述可疑进程，这些进程可能实际上已经死亡</u>。

​	故障可能发生在链路层上(进程之间的消息丢失或传递缓慢)，或者发生在进程层上(进程崩溃或运行缓慢)，而缓慢可能不一定能与故障区分开来。这意味着在如下两个方面总是面临一个杈衡：

+ 将活着的进程错误地怀疑为死的(产生假阳性)
+ 推迟将无响应的进程标记为死的并期望它最终做出响应(产生假阴性)。

​	**故障检测器(failure detector)**是一个本地子系统，其负责识别故障或无法到达的进程，将它们排除在算法之外，并在维持安全性的同时保证算法的活动性。

​	<u>**活动性**和**安全性**是描述算法解决特定问题的能力及其输出正确性的属性</u>。

​	更正式地说，**活动性(liveness)是一种保证特定预期事件必须发生的属性**。例如，如果其中一个进程发生故障，则故障检测器必须检测到该故障。**安全性(safety)保证意外事件不会发生**，例如，如果一个故障检测器已经将一个进程标记为死亡，那么这个进程实际上必须是死亡的[LAMPORT77， RAYNAL99， FREILING11]。

​	从实际的角度来看，排除发生故障的进程有助于避免不必要的工作，防止错误传播和级联故障，而排除疑似故障的活动进程会降低系统可用性。

​	故障检测算法应该表现出几个基本特性。首先，每一个没有问题的成员最终都应该注意到进程故障，并且算法应该能够向前推进并最终得出结果。这种特性称为**完备性（completeness）**。

​	我们可以通过算法的效率来判断其优劣：故障检测器识别进程故障的速度有多快。另一种方法是观察算法的准确性：是否精确地检测到了进程故障。换句话说，如果一个算法错误地认为一个活着的进程发生了故障或者不能检测出实际已发生的故障，那么它就是不准确的。

​	我们可以把效率和准确度之间的关系看作是一个可调参数：一个更高效的算法可能更不准确，而一个更准确的算法通常更不髙效。**建立既准确又高效的故障检测器被证明是不可能的**。同时，<u>故障检测器是允许产生假阳性的(即，错误地将活着的进程识别为故障，或者反过来)</u>[CHANDRA96]。

​	故障检测器是许多共识和原子广播算法的必要前提和组成部分，我们将在本书后面讨论这些算法。

​	许多分布式系统使用**心跳(heartbeat)**来实现故障检测器。由于其简单性和很强的完备性，这种方法非常普遍。我们在这里讨论的算法<u>假设不存在拜占庭式故障：进程不会试图故意谎报它们自己及相邻进程的状态</u>。

## 9.1 心跳和ping

​	我们可以通过触发如下两个周期性过程之一来查询远程进程的状态：

+ 我们可以触发一个**ping**，它将消息发送到远程进程，通过在指定的时间段内是否得到响应来检查它们是否仍处于活动状态。
+ 我们可以触发一个**心跳**，即进程通过向其对等方发送消息来主动通知其仍在运行。

​	在这里我们将使用ping作为例子，使用心跳也可以解决相同的问题并产生相似的结果。

​	每个进程维护一个其他进程的列表(存活、死亡和疑似死亡)，并且用每个进程最新的响应时间对这个列表进行更新。如果一个进程在较长的时间内无法响应一个ping消息，它会被标记为疑似死亡(suspected)。

​	许多故障检测算法都是基于心跳和超时的。例如，用于构建分布式系统的流行框架Akka实现了一个截止时间故障检测器(deadline failure detector)，这一检测器使用心跳机制，如果进程在某一固定时间间隔内未能成功注册，它将报告进程故障。

​	<u>这种方法有几个潜在的缺点：它的精度依赖于对ping频率和超时的仔细挑选，并且它不能从其他进程的角度捕获进程的可见性(参见9.1.2节)</u>。

### * 9.1.1 无超时的故障检测器

​	一些算法避免依赖超时来检测故障。例如Heartbeat，一种**无超时(timeout-free)**故障检测器[AGUILERA97]，该算法仅对心跳计数并允许应用程序基于心跳计数器向量中的数据来检测进程故障。由于该算法是无超时的，因此它能在异步系统假设下运行。

​	该算法假设任意两个正确的进程用公平路径(fair path)相互连接，该路径只包含公平链路(即如果无限频繁地通过该链路发送一条消息，该消息也会无限频繁地被接收)，并且每个进程都能意识到网络中所有其他进程的存在。

​	每个进程维护一个邻居列表和与其相关联的计数器。首先，进程向邻居发送心跳消息，毎个消息都包含心跳到目前为止所经过的路径。初始消息包含路径中的第一个发件人和一个唯一标识符，该标识符可用于避免同一消息被广播多次。

​	当进程接收到新的心跳消息时，它会递增路径中所有参与者的计数器，将心跳信号发送到尚未参与的进程，并将其自身追加到路径中。进程一旦看到所有已知进程已经接收到消息(换句话说，进程ID出现在路径中)，就会停止传播该消息。

​	<u>由于消息是通过不同的进程传播的，并且心跳路径包含从相邻进程接收的聚合信息，因此即使两个进程之间的直接链路出现故障，我们也可以(正确地)将无法到达的进程标记为活动进程</u>。

​	心跳计数器表示系统的全局和归一化视图。这个视图捕获了心跳是如何在节点间传播的，让我们可以对进程进行比较。然而，<u>这种方法的一个缺点是，对心跳计数器进行解释可能相当棘手：**需要选择一个能够产生可靠结果的阈值**。除非我们能做到这一点，否则算法会错误地将活动进程标记为疑似死亡</u>。

### * 9.1.2 外包心跳

​	可扩展弱一致性感染式进程组成员协议(Scalable Weakly Consistent Infection-style Process Group Membership Protocol，SWIM)[GUPTA01使用的是另一种方法。它使用外包心跳(outsourced heartbeat)来提高可靠性，**<u>利用的是从其相邻进程的角度查看到的进程活动性(liveness)信息。这种方法不需要进程知道网络中的所有其他进程，只需要知道其连接的对等进程的子集</u>**。

​	进程P1向进程P2发送ping消息。P2不响应该消息，因此P1继续选择多个随机成员(P3和P4)发送ping消息。这些随机成员会尝试向P2发送心跳消息，如果P2响应，则将确认转发回P1。

​	这允许我们将直接和间接可达性都考虑在内。例如，如果有进程P1、P2和P3，我们可以同时从P1和P2的角度检查P3的状态。

​	通过将决策责任分布到成员组中，外包心跳可以做到可靠的故障检测。这种方法不需要向很多对等进程广播消息。由于外包心跳请求可以并行触发，这种方法可以快速收集更多关于疑似死亡进程的信息，进而让我们做出更准确的决策。

## * 9.2 phi增量故障检测器

> 某种意义上有点类似Google TCP的BBR拥塞控制算法。
>
> [谈TCP BBR拥塞控制算法_yang_oh的博客-CSDN博客_bbr拥塞控制算法](https://blog.csdn.net/u013032097/article/details/96212770)

​	phi增量(φ-accrual)故障检测器[HAYASHIBARA04]不是将节点故障视为二元判断问题(即进程只能处于两种状态：在线或宕机)，而是<u>用连续范围来捕获被监视进程崩溃的概率</u>。它的工作方式是**维护一个滑动窗口，从对等进程收集最近心跳的到达时间**。该信息用于估算下一个心跳的到达时间，将该近似值与实际到达时间进行比较，并计算可疑程度φ：代表在给定当前网络条件下，故障检测器对故障的置信度。

​	该算法的原理是：<u>收集和采样到达时间，创建出一个可用于对节点健康状况做出可靠判断的视图，然后使用这些釆样结果计算φ的值：如果该值达到阈值，则节点被标记为宕机。**通过调整标记节点为疑似死亡的阈值，这种故障检测器能够动态地适应变化的网络条件**</u>。

​	从架构的角度来看，phi增量故障检测器可以看作三个子系统的组合。

+ **监控**

  通过ping、心跳或请求-响应采样来收集进程存活信息。

+ **解释**

  决定是否将该进程标记为疑似死亡。

+ **行动**

  每当标记进程为疑似死亡时执行的回调。

​	监控进程将数据样本(假定是正态分布)收集并储存在心跳到达时间的固定大小窗口中。新到达的心跳被添加到窗口中，同时最早的心跳数据点被丢弃。

​	<u>通过确定样本的均值和方差，可以从采样窗口估算出分布参数。该信息用于计算在前个消息到达之后t个时间单位内消息到达的概率。基于这个信息我们能计算出φ，它描述了我们对一个进程活动性做出正确决定的可能性。换句话说，有多大的可能性犯错——接收到一个与计算出的假设相矛盾的心跳</u>。

​	这种方法是由日本高级科学技术研究所的研究人员开发的，现在已用于许多分布式系统中，例如，Cassandra和Akka(连同前面提到的截止时间故障检测器)。

## * 9.3 Gossip和故障检测

> 有点像OSPF动态路由协议。
>
> [动态路由协议_百度百科 (baidu.com)](https://baike.baidu.com/item/动态路由协议/2915884?fr=aladdin)

​	另一种避免依赖单节点视图做出决策的方法是Gossip式的故障检测服务[VANRENESSE98]它使用Gossip(参见12.6节)来收集和分发相邻进程的状态。

​	<u>每个成员维护一个其他成员的列表：它们的心跳计数器(heartbeat counter)和时间戳</u>。

​	<u>时间戳列出了心跳计数器上次递增的时间。每个成员定期递增其心跳计数器，并将其列表分发给随机相邻节点。在接收到消息时，相邻节点将列表与它自己的列表进行合并，更新其他相邻节点的心跳计数器</u>。

​	节点还定期检查状态列表和心跳计数器。如果任何节点在足够长的时间里没有更新其计数器，就认为它发生了故障。超时时间应当谨慎选择，以将误报的概率降至最低。<u>成员之间通信的频率(换句话说，最坏情况下所使用的带宽)是有上限的，并且最多可以随着系统中的进程数线性增长</u>。

​	通过这样的方式，我们就可以检测出崩溃的以及任何其他集群成员都无法访问的节点。这个决策是可靠的，因为集群的视图是来自多个节点的聚合。如果两台主机之间的链路出现故障，心跳仍然可以通过其他进程传播。**使用Gossip来传播系统状态增加了系统中的消息数量，但使得信息传播更可靠**。

## * 9.4 反向故障检测

​	由于并不总是能传播故障的信息，并且通过通知每个成员来进行传播可能成本较高，因此出现了一种称为FUSE(Failure Notification Service，故障通知服务)[DUNAGANO4]的方法，它专注于可靠且廉价的故障传播，即使在网络分区的情况下也能工作。

​	<u>为了检测进程故障，该方法将所有活动进程进行分组。**如果其中一组变得不可用，则所有参与者都能检测到该故障**。换句话说，每次检测到单个进程故障时，它被转换并传播为**组故障**</u>。它可以检测任何形式的网络中断、网络分区和节点故障。

​	<u>组中的进程定期向其他成员发送ping消息，以查询它们是否仍处于活动状态。**如果其中一个成员由于崩溃、网络分区或链路故障而无法响应此消息，则发出这个ping的成员本身将停止响应ping消息**</u>。

​	展示了四个通信进程：

+ 初始状态：所有进程都处于活动状态并可以通信。
+ P2崩溃并停止响应ping消息。
+ P4检测到P2的故障并停止响应自己收到的ping消息。
+ 最终，P1和P3注意到P4和P2都没有响应，将进程故障传播到整个组。

​	**所有故障都通过系统从故障源传播到所有其他参与者**。<u>参与者逐渐停止响应ping消息，将单个节点故障转换为组故障</u>。

​	在这里，我们**<u>利用不通信作为一种传播的手段</u>**。这种方法的一个优点是保证每个成员都能了解组的故障并对其做出充分的反应。它的一个缺点是：<u>将单个进程与其他进程分开的链路故障也可能会被转换为组故障，但这其实也可以被看作是一个优点，应由具体的用例所决定</u>。应用程序可以使用其自身对故障传播的定义来应对这种情况。

## 9.5 本章小结

​	故障检测器在任何分布式系统中都是一个重要的组成部分。正如FLP不可能定理所说的，**<u>异步系统中没有协议能够保证一致性</u>**。

​	<u>故障检测器有助于扩展模型，允许我们通过在**准确性**和**完备性**之间进行权衡来解决一致性问题</u>。文献[CHANDRA96]描述了这一领域的一个重要发现，证明了故障检测器是有用的，它表明，<u>**即使使用一个犯了无限多个错误的故障检测器，解决共识问题仍然是可能的**</u>。

​	我们介绍了几种故障检测算法，每种算法都使用不同的方法：一些专注于通过直接通信来检测故障，而一些则使用广播或Gossip来传播信息，还有一些使用沉默(换句话说，不再通信)作为传播手段。现在，我们知道可以使用心跳、ping、截止时间、连续范围等方法，毎一种方法都有自己的优点:简单性、准确性或精确性。

# * 10. 领导者选举

​	同步的代价可能会非常大：如果算法的每一个步骤都需要联系其他参与者，那么结果定是产生相当显著的通信开销。在大型且地理分布的网络中尤其如此。<u>为了减少同步开销和达成决定所需消息的往返次数，一些算法依赖于**领导者(有时称为协调者)进程**的存在，该进程负责执行或协调分布式算法的各个步骤</u>。

​	一般来说，分布式系统中的进程是对等的，任何进程都可以接管领导者的角色。进程可以长期担任领导者，但这不是一个永久的角色。通常情况下，进程可以一直担任领导者直到崩溃为止。崩溃后，任何其他进程都可以开始新一轮的选举，如果其当选，就可以担任领导者并继续执行上个领导者遗留的工作。

​	选举算法的**活动性(liveness)**保证了大多数时候会有一个领导者，选举最终会完成(即<u>系统不应该无限期地处于选举状态</u>)。

​	理想情况下，我们也希望获得安全性，保证一次最多只能有一个领导者，并完全消除<u>脑裂(两个目的相同的领导者被选举出来，但彼此不知情)</u>的可能性。然而在实践中，许多领导者选举算法违反了这一协定。

​	可以使用领导者进程来实现广播中消息的全序。领导者收集并保存全局状态，接收消息，并将消息分发给各个进程。它还可以用于协调发生在系统故障后、初始化期间或重要状态变更时的系统重组。

​	系统初始化时将会触发选举，第一次选择领导者。当上一个领导者崩溃或通信失败时也会触发选举。选举必须是确定性的：<u>选举过程中必须只产生一个领导者。这一决定需要对所有参与者都有效</u>。

​	<u>尽管领导者选举和分布式锁(即对共享资源的独占所有权)从理论角度看可能很相似，但它们略有不同。如果一个进程因为执行**临界区**而持有锁，那么对于其他进程来说，知道现在到底是谁持有锁并不重要，只要满足活动性(即锁最终将被释放并允许其他人获得它)就可以了。**相比之下，选出的进程具有一些特殊性质，必须让所有其他参与者都知道，因此新当选的领导者必须将其角色通知所有对等进程**</u>。

​	<u>如果分布式锁算法对某个进程或进程组有任何偏好，不被偏好的进程最终将产生对共享资源的饥饿，这与活动性相矛盾。相比之下，领导者可以保持领导的角色直到停止或崩溃，长期存活的领导者是更好的</u>。

​	**在系统中具有稳定的领导者有助于避免远程参与者之间的状态同步，减少交换消息的数量，并能通过单进程(而不是对等进程间的协调)来驱动执行**。

​	在具有领导权的系统中，一个潜在的问题是，**领导者可能会成为瓶颈**。<u>为了克服这种情况，许多系统将数据划分在不相交的独立副本集中(参见13.6节)。每个副本集都有自己的领导者，而不是在整个系统范围上使用一个领导者</u>。使用这种方法的系统之一是Spanner(参见13.5节)。

​	因为毎一个领导者进程最终都会发生故障，所以故障必须被检测、报告和处理：系统必须选择另一个领导者来替换故障领导者。

​	一些算法，例如ZAB(参见14.2.2节)、 Multi-Paxos(参见14.34节)或Raft(参见14.4节)，<u>使用**临时领导者**来减少参与者达成一致所需的消息数量</u>。然而，这些算法都使用算法特有的手段来进行领导者选举、故障检测以及解决竞争领导者的进程之间的冲突。

## 10.1 霸道选举算法

​	有一个被称为霸道选举算法(bully algorithm)的领导者选举算法，<u>它使用进程排名来认定新的领导者。每个进程都有一个唯一的排名。在选举过程中，排名最高的进程成为领导者[MOLINA82]</u>。

​	这种算法以其简单性而闻名。之所以被命名为霸道(bully)，是因为排名最高的节点"霸道"地强迫其他节点接受它。它有时也被称为君主领导者选举：在前一个君主不复存在后，排名最高的兄弟姐妹成为君主。

​	如果一个进程注意到系统中没有领导者(该系统从未被初始化)或以前的领导者已停止响应，则按如下三个步骤进行选举：

1. 进程将选举消息发送到具有较高标识符的进程。
2. 进程等待较高排名的进程进行响应。如果没有排名更高的进程响应，则继续执行步骤3。否则，进程通知它所了解到的排名最高的进程，让它继续执行步骤3。
3. 进程假定没有排名更高的活动进程了，因此将新的领导者通知给所有排名更低的进程。

​	霸道领导者选举算法：

+ a）进程3注意到前一个领导者6已崩溃，于是向具有更高标识符的进程发送选举消息来开始新的选举。
+ b）4和5回应Alive(存活)消息，因为它们具有比3更高的排名。
+ c）3通知在此轮响应中排名最高的进程5。
+ d）5被选为新的领导者，它广播Elected(选举完成)消息，将选举结果通知排名较低的进程。

​	这种算法的一个明显问题是，在存在网络分区的情况下，它违反了安全性保证(一次最多只能选举一个领导者)。很容易出现这样的情况：<u>节点将被分成两个或多个独立运行的子集，每个子集选举出了这个子集的领导者。这种情况被称为**脑裂(split brain)**</u>。

​	该算法的另一个问题是对排名较高节点有强烈的偏向性，<u>如果这些节点不稳定，可能导致算法一直处于重新选举状态</u>。一个不稳定的高排名节点提议出任领导者，此后不久发生故障，稍后又再次赢得选举，然后再次发生故障，整个过程重复进行。可以通过分发主机质量的度量并在选举时将其纳入考虑因素来解决这一问题。

## 10.2 依次故障转移

​	霸道算法有许多改进其各种性质的版本。例如，我们可以使用多个依次(next-in-line)替代进程作为故障转移候选来缩短重选过程[GHOLIPOUR09]。

​	<u>每个选举出的领导者都提供一个故障转移节点的列表。当其中一个进程检测到领导者故障时，它通过向列表中排名最高的备选进程发送消息来发起新一轮选举。如果提议的备选进程中有一个是活动的，它就会成为一个新的领导者，而不必经过整个选举回合</u>。

​	如果检测到领导者故障的进程本身是列表中排名最高的进程，则它可以立即通知其他进程新领导者的信息。

优化过的流程：

+ a）6是具有指定备选进程{5，4}的领导者，它崩溃了。3注意到此故障并联系5，它是列表中排名最高的备选进程。
+ b）5响应3，它处于活动状态，以防止它联系备选进程列表中的其他节点
+ c）5通知其他节点它是新的领导者。

​	因此，如果下一个进程仍然活着，我们在选举期间需要的步骤就更少。

## 10.3 候选节点/普通节点优化

​	另一种算法试图通过将节点分成两个子集，即**候选节点(candidate)**和**普通节点(ordinary)**，来降低所需的消息数量，只有候选节点的其中之一最终可以成为领导者[MURSHED12]。

​	普通进程通过联系候选节点来发起选举，收集它们的响应，选择排名最高的活动候选节点作为新的领导者，然后通知其余节点选举结果。

​	**为了解决同时发生多个选举的问题，该算法建议使用一个特定于进程的延迟变量δ(各进程的δ差异很大)，使得其中一个节点可以在其他节点之前发起选举**。δ通常大于消息的往返时间。高优先级的节点具有较低的δ，反之则具有较高的δ。

​	选举过程的步骤（假设1、2、6都是候选节点；3、4、5都是普通节点）：

+ a）普通进程集合里的进程4注意到领导者进程6的故障。它通过联系候选节点集合里的所有剩余进程来发起新的选举回合。
+ b）候选进程发送响应通知4它们仍然活动。
+ c）4将新的领导者2通知给所有进程。

## * 10.4 邀请算法

​	邀请算法(invitation algorithm)允许进程"邀请"其他进程加入它们的组，而不是试图超越它们的排名。这种算法从定义上就**允许多个领导者存在**，因为每个组都有自己的领导者。

​	<u>每个进程一开始都是一个新组的领导者，组内唯一的成员是这个进程本身。组领导者联系不属于该组的对等进程，邀请其加入。如果对等进程本身是领导者，则合并两个组。否则，被联系的进程会回复组领导者ID，从而让两个组的领导者以较少的步骤建立联系并合并两个组</u>。

​	邀请算法的执行步骤：

+ a）开始时，有四个进程成为各有一名成员的组的领导者。1邀请2加入它所在组，3邀请4加入它所在组。
+ b）2加入进程1的组，4加入进程3的组。1作为第一组的领导者去联系另一组的领导者3。随后，该组剩余的成员(在本例中为4)被通知关于新的组领导的信息。
+ c）两个组被合并，并且1成为扩展后组的领导者。

​	因为组被合并了，是建议组合并的进程还是另一个进程成为新的领导者都无关紧要。为了将合并组所需的消息数量保持在最低限度，较大组的领导者可以成为新组的领导者。这样，只需要把领导者变更的消息通知给较小组的进程。

​	与其他讨论过的算法类似，该算法允许各个进程处于多个组中，并允许多个领导者存在。邀请算法允许创建进程组并合并它们，而不必从头开始触发新的选举，这减少了完成选举所需的消息数量。

## 10.5 环算法

​	在环算法(ring algorithm)[CHANG79]中，系统中所有的节点形成一个环，并且知道环拓扑(即它们在环中的前驱和后继)。当进程检测到领导者故障时，它发起新的选举。选举消息沿着环向下转发：每个进程联系它的后继节点(环中离它最近的下一个节点)。如果该节点不可用，则进程跳过不可达的节点，并尝试联系环中之后的节点，直到最终有一个节点响应为止。

​	节点联系其兄弟节点，沿着环收集活动节点的集合，并在将集合传递到下一个节点之前将其自身添加到该集合中，类似于9.1.1节中描述的故障检测算法那样，节点在将其传递到下一个节点之前将其标识符附加到路径中。

​	该算法通过完全遍历环来进行。当消息返回到开始选举的节点时，从活动节点集中选择排名最高的节点作为领导者。一个遍历的例子（假设1-2-3-4-5-6～1成环，6故障后，5和1成相邻节点）：

+ a）先前的领导者6发生了故障，并且每个进程都具有该环的视图。
+ b）3通过开始遍历来发起选举。每一步中，都维护一个在路径上遍历过的节点集合。5无法达到6，所以它跳过6直接联系1。
+ c）由于5是排名最高的节点，3会发起另一轮消息传递来广播关于新领导者的信息。

​	**这种算法的变体包括收集单个排名最高的标识符，而不是一组活动节点，以节省空间**：因为max函数是可交换的，所以知道当前遍历过的节点的排名最大值就足够了。当算法返回到已经开始选举的节点时，最后已知的最高标识符会再次在环上循环传递。

​	**由于环可以划分为两个或更多的部分，因此可能出现脑裂这种不安全的情况**。

​	正如你所看到的，一个具有领导者的系统要正确行使职能，我们就需要知道目前领导者的状况(它是否还活着)，因为，要想将进程组织起来继续执行算法，领导者必须是活动且可达的。要检测领导者崩溃，我们可以使用故障检测算法(参见第9章)。

## 10.6 本章小结

​	领导者选举是分布式系统中的一个重要课题，使用特定领导者可以减少协调开销并提高算法的性能。每轮选举的成本可能很高，但由于它们不经常发生，因此不会对整个系统的性能产生负面影响。<u>单个领导者可能会成为瓶颈，但大多数时候可以通过对数据进行分区来解决(对不同分区或不同的动作使用不同的领导者)</u>。

​	<u>不幸的是，我们在本章中讨论的所有算法都容易出现**脑裂**的问题</u>：**我们可能会在独立的子网中选出两个领导者，它们彼此都不知道对方的存在**。<u>为了避免脑裂，我们必须获得整个集群范围内的多数票</u>。

​	<u>许多共识算法，包括Multi-paxos和Raft，都依赖于一个领导者来进行协调。但领导者选举不就是共识本身吗?要选举一个领导者，我们需要就其身份达成共识。如果我们能就领导者的身份达成共识，我们就能用同样的方法在其他任何事情上达成共识[ABRAHAM13]</u>。

​	一个领导者的身份可能会在其他进程不知晓的情况下发生变化，因此进程本地关于领导者的知识是否仍然有效是个问题。为了解决这个问题，<u>我们需要将领导者选举与故障检测相结合</u>。例如，稳定领导者选举(stable leader election)算法使用具有唯一稳定领导者的回合和基于超时的故障检测，以保证领导者只要不崩溃且可访问，就能维持其地位[AGUILERA01]。

​	**依赖于领导者选举的算法通常允许存在多个领导者，并试图尽可能快地解决领导者之间的冲突**。例如，对于 Multi-Paxos(参见14.3.4节)就是这样的，其中只有两个冲突的领导者(提议者)中的一个可以继续执行，这些冲突通过收集第二个Quorum的投票来解决，保证来自两个不同提议者的值不会被同时接受。

​	在Raft(参见14.4节)中，领导者可以发现其任期过时(这意味着系统中存在不同的领导者)，并将其任期更新为最新的任期。

​	在这两种情况下，拥有一个领导者是确保活动性的一种方式(如果当前的领导者发生故障，我们需要一个新的领导者)，而且其他进程不应该花费无限长的时间来了解领导者是否真的发生了故障。安全性的缺失和允许多个领导者是一种性能优化：算法可以继续进行复制阶段，而安全性则通过检测和解决冲突来保证。

​	我们将会在第14章里更深入地讨论共识和共识范畴内的领导者选举。

# 11. 复制和一致性

​	在继续讨论共识和原子提交算法之前，让我们学习最后一块缺失的部分：一致性模型(consistency model)。一致性模型非常重要，因为它们解释了多数据副本系统的可见性语义和行为。

​	容错(fault tolerance)描述了这样一种特性：当系统中的部分组件发生故障时，系统仍然能继续正确地运行。使系统具有容错性并不是一件容易的事情，而使现有的系统具备容错能力则更加困难。<u>容错的主要目标是从系统中消除单点故障，并确保关键任务组件有冗余</u>。通常，冗余对用户来说是完全透明的。

​	系统可以存储多个数据副本，当其中一台机器发生故障时，通过另一台机器可以进行**故障转移(failover)**，这样系统就可以继续正确运行。<u>在具有单一真相来源(source of truth)的系统中(例如，主/副本数据库)，可以通过将副本晋升为新的主数据库来显式地完成故障转移</u>。有些系统则不需要显式的重新配置，它们通过在读写查询期间收集多个参与者的响应来确保一致性。

​	**数据复制(replication)**通过在系统中维护多个数据副本来引入冗余。然而，由于原子地更新数据的多个副本是一个等同于共识的问题[MILOSEVIC11]，因此<u>**数据库中的每个操作都执行共识操作可能成本相当高**</u>。我们可以探索一些性价比更高且更灵活的方法，允许参与者之间存在某种程度的差异，但使数据从用户的角度看起来是一致的。

​	在多数据中心部署中，复制尤为重要。在这种情况下，**跨地域复制(geo-replication)**有多种用途：它通过提供冗余来提高可用性，增强抵御一个或多个数据中心的故障的能力。它还<u>可以将数据副本放置在离客户端更近的物理位置以减少延迟</u>。

​	当数据记录被修改时，其副本必须被相应地更新。在谈论复制时，我们最关心这三种事件： **写入、副本更新和读取**。这些操作触发了由客户端发起的一系列事件。在某些情况下，从客户端角度看，更新副本可能发生在写操作完成之后，但这仍然不能改变这样个事实：<u>客户端必须能够以特定的顺序观察到发生过的操作</u>。

## 11.1 实现可用性

​	我们已经讨论了分布式系统的误区，并确定了许多可能出错的事情。在现实世界中，节点不一定处于活动状态或能够相互通信。然而，**间歇性故障不应影响可用性**：从用户的角度来看，系统作为一个整体会继续运行，就像什么都没有发生一样。

​	系统可用性是一个非常重要的属性：在软件工程中，我们总是努力实现高可用性，并尽量减少停机时间。工程团队夸耀他们软件的正常运行时间指标。我们之所以那么关心可用性是出于如下几个原因：软件已经成为我们社会不可分割的一部分；没有它，许多重要的事情都难以进行，例如银行业务、通信、旅行等。

​	对于公司来说，缺乏可用性可能意味着失去客户或金钱：如果电商系统出现故障，你就无法在那里购物；如果你的银行网站没有响应，你就无法转账。

​	为了使系统具有高可用性，我们需要以这样一种方式设计我们的系统——允许它优雅地处理一个或多个参与者出现故障或不可用的情况。为此，我们需要引入**冗余**和**复制**。然而，<u>一旦我们添加冗余，就会面临**多数据副本同步**的问题，并且必须实现**恢复机制**</u>。

## 11.2 臭名昭著的CAP理论

​	**可用性**是一个衡量系统成功响应请求能力的属性。可用性的理论定义提到了最终响应，但是，在现实世界的系统中，我们当然希望避免等待无限长的时间。

​	理想情况下，我们希望毎个操作都是一致的。**一致性在这里定义为原子性或可线性化linearizable)的一致性**(参见11.5.2节)。<u>可线性化历史能够表示为一个可以保持原始操作顺序的瞬时操作序列</u>。可线性化简化了对可能的系统状态的推理，使分布式系统看起来就像是在单机系统上运行一样。

​	我们希望在容忍网络分区的同时实现一致性和可用性。网络可能被分裂为几个部分，在这些部分之间，进程不能相互通信：被分隔的节点之间发送的一些消息将无法到达目的地。

​	<u>**可用性**要求任何无故障的节点交付结果，而**一致性**要求结果是可线性化的</u>。 Eric Brewer提出的CAP猜想讨论了一致性(consistency)、可用性(availablity)和分区容忍性(partition tolerance)之间的权衡[BREWER00]。

​	<u>在一个异步系统中，可用性要求是不可能被满足的，而在网络分区存在的情况下，我们无法实现一个同时保证可用性和一致性的系统[GILBERT02]</u>。构建系统时，我们可以在提供尽力而为(best effort)可用性的同时保证强一致性，或者在提供尽力而为一致性的同时保证可用性[GILBERT12]。在这里，尽力而为意味着：如果一切正常，系统将不会故意违反任何保证，但是在网络分区的情况下，允许系统削弱和违反保证。

​	换句话说，CAP描述了一系列潜在的可选项，而在这些选项的两端是以下两种系统：

+ **一致性和分区容忍系统(CP系统)**

  CP系统更倾向拒绝请求，而不是提供可能不一致的数据。

+ **可用性和分区容忍系统(AP系统)**

  AP系统放松了一致性要求，允许在请求期间提供可能不一致的值。

​	CP系统的一个例子是共识算法的实现，它要求多数派节点的参与才能进行：系统总是一致的，但在网络分区的情况下可能不可用。

​	而AP系统的一个例子是，只要有一个副本活着，数据库就一定能进行读写，这最终可能导致数据丢失或返回不一致的结果。

​	PACELC猜想[ABADI12]是CAP的一个扩展，它指出在网络分区(P)存在的情况下可用性和一致性(AC)之间存在一个选择。否则(Else，E)，即使在没有网络分区的情况下系统运行正常，我们仍然要在延迟(Latency，L)和一致性(C)之间做出选择。

### 11.2.1 小心使用CAP

​	需要注意的是，<u>CAP讨论的是网络分区，而不是节点崩溃或任何其他类型的故障(如崩溃恢复)。与集群其他节点分隔开的节点可以做出不一致的响应，但崩溃的节点根本不会响应</u>。一方面，这意味着没有必要考虑任何宕掉的节点需要面对一致性问题。而另一方面，现实世界中的情况并非如此：还有许多不同的故障场景(其中一些可以用网络分区来模拟)。

​	CAP意味着，即使所有节点都启动了，**<u>只要它们之间有连接性问题，我们仍可能面临一致性问题</u>**。这是因为<u>我们期望毎个没有故障的节点都能正确响应，而不考虑有多少节点可能宕掉</u>。

​	CAP猜想有时用一个三角形来表示，就好像我们可以转动一个旋钮，或多或少地得到所有这三个参数中的两个。然而，尽管我们可以转动旋钮用一致性换取可用性，但**分区容忍性是一个实际上我们无法调节或用任何东西来交换的属性**[HALE10]。

> CAP中的一致性定义与ACID(参见第5章)定义的一致性完全不同。ACID致性描述了事务一致性：事务将数据库从一个有效状态带到另一个有效状态，保持所有数据库的约束(如唯一性约束和引用完整性)。而在CAP中，它意味着操作是**原子的**(操作全部成功或失败)和**一致的**(操作从不让数据处于不一致的状态)。

​	CAP中的可用性也不同于前面提到的高可用性[KLEPPMANN15]。CAP的定义对执行延迟没有限制。另外，与CAP相反，数据库中的可用性并不要求每个非故障节点响应每个请求。

​	CAP猜想用于解释分布式系统、推理故障场景和评估可能的情况，放弃一致性并不意味着系统可以提供不可预测的结果，它们之间有所区别。

​	如果使用正确，声称具备可用性的数据库仍然能够提供来自副本的一致结果，前提是要存在足够多的活着的副本。当然，还有更复杂的故障场景，**CAP猜想只是一条经验法则，并不一定能说明全部事实**。

### 11.2.2 收成与产量

​	CAP猜想仅以它们最强的形式讨论一致性和可用性：可线性化和系统最终响应每一个请求的能力。

​	这迫使我们在这两个属性之间做出艰难的权衡。然而，有些应用程序可以从稍微放松的假设中获益，我们可以用它们较弱的形式来思考这些属性。

​	系统不一定非得在一致或可用中二选一，也可以提供更宽松的保证。我们可以定义两个可调度量：收成(harvest)和产量(yield)，在两者之间进行选择仍然可以形成正确的行为[FOX99]：

+ 收成

  收成定义査询的完成程度：如果查询必须返回100行，但由于某些节点不可用而只能获取99行，这仍然比查询完全失败而不返回任何内容要好。

+ 产量

  产量指成功完成的请求数与尝试请求总数之比。产量与正常运行时间(uptime)不同，例如一个繁忙的节点没有宕机，但仍然可能无法响应某些请求。

​	这就把权衡的重点从绝对条件变成了相对条件。我们可以用收成换取产量，并允许某些请求返回不完整的数据。<u>提高产量的一个方法是只从可用的分区返回查询结果</u>(参见13.6节)。例如，如果存储某些用户记录的节点子集宕机，我们仍然可以继续为其他用户处理请求。或者，我们可以要求，对关键应用程序的数据必须完整返回，但对其他请求允许出现一些偏离。

​	定义和权衡收成与产量，并在二者之间做出慎重的决定，有助于我们建立容错性更好的系统。

## 11.3 共享内存

​	对于客户端来说，分布式系统在储存数据时仿佛拥有共享的存储，类似于单节点的系统。节点间通信和消息传递则被抽象出来并发生在幕后。这就造成了一种共享内存的假象。

​	可通过读写操作来访问的单个存储单元通常被称为寄存器(register)。我们可以将分布式数据库中的共享内存视为一个寄存器阵列。

​	我们用调用((invocation)和完成(completion)事件来标识每个操作。如果调用该操作的进程在操作完成之前崩溃，则将该操作定义为失败操作。<u>如果一个操作的调用和完成事件都发生在另一个操作被调用之前，我们说这个操作在另一个操作之前，并且这两个操作是顺序的(sequential)。否则说它们是并发的</u>。

​	进程P1和P2执行不同的操作：

+ a)进程P2执行的操作在进程P1执行的操作已经完成之后才开始，两个操作是顺序
+ b)两个操作之间有重叠，因此这两个操作是并发的。
+ c)进程P2执行的操作在P1执行的操作之后开始，并在P1执行的操作完成之前结束，这两个操作也是并发的。

​	多个读取者或写入者可以同时访问寄存器。对寄存器的读写操作不是即时的，即需要些时间。由不同进程执行的并发读写操作不是串行的(serial)：根据操作重叠时寄存器的行为，它们的顺序可能不同，并可能产生不同的结果。根据寄存器在并发操作下的行为，我们将寄存器分为以下三类：

+ 安全寄存器

  在并发写操作期间，对安全寄存器的读取可能返回寄存器范围内的任意值(这听起来不太实用，但可能描述出了异步系统的语义——不限定执行的顺序)。**具有二元值的安全寄存器在读写过程中可能会出现闪烁**(flickering，即结果交替返回两个值)。

+ 常规寄存器

  常规寄存器有稍强一点的保证：<u>读操作只能返回最近完成的写操作所写的值，或者与当前读操作重叠的写操作所写的值</u>。在这种情况下，**系统有一些顺序的概念，但是写结果并不同时对所有读取者可见**(例如，在复制数据库中可能会发生这样的情况：主进程接受写操作，并将其复制给提供读服务的工作者进程)。

+ 原子寄存器

  **原子寄存器保证可线性化：每个写操作都对应一个时刻，在此之前，每个读操作返回一个旧值，在此之后，每个读操作返回一个新值。原子性是简化系统状态推断的基本性质**。

## * 11.4 顺序

​	当我们看到一系列事件时，我们对它们的执行顺序会有直观感受。然而，在分布式系统中，理解顺序并不总是那么容易，因为很难准确地知道什么时候发生了什么事情，并且也很难在整个集群中立即获得这些信息。每个参与者可能都有自己的状态视图，因此我们必须査看毎个操作，并根据其调用和完成事件定义顺序，并描述其操作边界。

​	让我们这样定义一个系统，在其中的进程可以对共享寄存器执行read(register)和write(register， value)操作，每个进程按顺序执行自己的一组操作(即，必须完成每个操作之后才能启动下一个操作)。顺序进程执行的组合形成了一个全局历史，这其中，操作可能并发执行。

​	**考虑一致性模型的最简单方法是讨论读和写操作及其重叠的方式：读操作没有副作用，而写操作会改变寄存器状态**。这能帮助我们推断写入后数据何时变得可读。例如，考虑两个进程并发执行如下事件的历史记录：

```pseudocode
进程1:				进程2:
write(x,1)		read(x)
							read(x)
```

​	单看这些事件无法确定这两种情况下read(x)运算的结果是什么。有几种可能的历史：

+ 写操作在两次读操作之前完成。
+ 写操作和两次读操作交错进行，而且可能在两次读操作之间执行。
+ 两次读操作都在写操作之前完成。

​	即使我们只有一份数据，也并不能很容易地回答会发生什么。在一个复制系统中，我们会有更多可能的状态组合，当多个进程读写数据时情况将变得更加复杂。

​	如果所有这些操作都是由单个进程执行的，我们可以强制一个严格的事件顺序，但是对于多个进程则很难这样做。潜在的困难可以分为两组：

+ 操作可能会重叠。
+ 不重叠调用的影响可能不会立即显现。

​	**为了推理操作顺序并明确描述可能的结果，必须定义一致性模型**。我们用共享内存和并发系统来讨论分布式系统中的并发性，因为大多数一致性定义和规则仍然是适用的。尽管并发系统和分布式系统之间存在着许多重叠的术语，但由于通信方式、性能和可靠性等方面的差异，我们无法直接应用大多数并发算法。

## 11.5 一致性模型

​	由于共享内存寄存器上的操作允许重叠，我们需要定义清楚语义：如果多个客户端同时(或在短时间内)读取或修改数据的不同副本将会发生什么。这个问题没有唯一正确的答案，因为这些语义根据应用程序的不同而不同，但是在一致性模型的上下文中它们都得到了很好的研究。

​	一致性模型(consistency model)提供不同的语义和保证。你可以将一致性模型看作是参与者之间的契约：每个副本要做什么才能满足所需的语义，以及用户在发出读写操作时可以期望得到什么结果。

​	一致性模型描述了在存在多份数据和并发访问的情况下可能岀现的返回结果。在本节，我们将讨论单一操作(single-operation)一致性模型。

​	毎个模型都描述了系统行为与我们期望的(或感觉自然的)行为之间存在的差距。它帮助我们区分交错操作的"所有可能的历史"和"在模型X下允许的历史"，这显著地简化了关于状态变化可见性的推理。

​	我们可以从状态的角度考虑一致性，描述哪些状态不变式是可以接受的，并建立不同副本上的数据拷贝之间所准许的关系。或者，我们也可以考虑操作一致性，它提供一个数据存储的外部视图，描述了操作并约束它们发生的顺序[TANENBAUM06，AGUILERA16]。

​	<u>如果没有全局时钟，很难给分布式操作一个精确且确定的顺序</u>。这就像是数据的狭义相对论：每个参与者对状态和时间都有自己的视角。

​	**<u>从理论上讲，每当我们想要改变系统状态时，可以先获取一个系统范围的锁，但这是非常不实际的。相反，我们使用一组规则、定义和约束来限制可能的历史和结果的数量</u>**。

​	一致性模型为我们在11.2节中讨论的内容增加了另一个维度。现在我们不仅要兼顾一致性和可用性，还要从**同步代价**的方面考虑一致性[ATTIYA94]。同步代价可能包括时延执行额外操作所花费的**CPU周期**、**用于持久化恢复信息的磁盘I/O**、**等待时间**、**网络I/O**，以及一切可以通过避免同步而节省的代价。

​	首先，我们将重点关注操作结果的可见性和传播。回到并发读写的例子，通过将写入按照依赖关系一个接一个地放置，或定义一个新值被传播出去的时间点，我们就能限制可能的历史数量。

​	我们从执行数据读写操作的进程(客户端)的角度来讨论一致性模型。由于我们在数据复制的上下文中讨论一致性，因此我们假设数据库可以有多个副本。

### 11.5.1 严格一致性

​	严格一致性(strict consistency)相当于完全透明的复制：**<u>任何进程的任何写入都可以立即被任何进程的后续的读操作读取</u>**。<u>它涉及全局时钟的概念，如果在时刻t1有write(x，1)，则任何read(x)的操作将在时刻t2>t1时返回新写入的值t1</u>。

​	不幸的是，**<u>这只是一个理论模型，且不可能实现</u>**，因为物理定律和分布式系统的工作方式限制了事情发生的速度[SINHA97]。

### * 11.5.2 可线性化

​	可线性化(linearizability)是最强的单对象、单操作一致性模型。**该模型下，在写操作开始和结束之间的某个时间点，写操作的效果严格一次性地对所有读取者可见，没有客户端会观察到状态转移、部分(即未完成的、仍在进行中的)或不完全(即在完成之前中断的)的写入操作**[LEE15]。

​	并发操作表现为可见性属性所支持的可能的顺序历史记录之一。可线性化有一定的不确定性，因为可能存在不止一种方式来对事件进行排序[HERLIHY90]。

​	如果两个操作重叠，它们可以按任何顺序生效。<u>在写操作完成之后发生的所有读操作都可以观察到该操作的结果</u>。**一旦一个读操作返回一个特定的值，在它之后的所有读操作返回的值至少都与它返回的那个值一样新**[BAILIS14a]。

​	**在全局历史中，并发事件发生的顺序有一定的灵活性，但它们不能被任意地重新排序**。操作的结果生效不能早于操作的开始时间，因为谁也不能预测未来。同时，结果必须在完成之前生效，否则，我们无法定义一个线性化点。

​	可线性化既尊重顺序性进程局部的操作顺序，也尊重相对于其他进程的并行操作的顺序，因此它定义了事件的全序关系(total order)。

​	这个顺序应该是一致的，这意味着共享值的每次读取都应该返回在此之前写入此共享变量的最新值，或者返回与此读取重叠的写操作所写入的值。**对共享变量的可线性化写访问也意味着互斥：在两个并发写之间，只有一个可以先进行**。

​	即使操作是并发并存在重叠，它们的效果也会以一种看似连续的方式变得可见。当然，没有一个操作是瞬间发生的，但操作看起来仍然是原子的。

**线性化点**

​	<u>可线性化最重要的一个特征是可见性：一旦操作完成，每个参与者都必须能看到它，并且系统不能“穿越到过去”还原它或使它对某些参与者不可见。换句话说，**可线性化禁止读取过时的数据**，它要求读取是单调的</u>。

​	这种一致性模型最好用原子的(即：不间断、不可分割)操作来解释。<u>操作不一定是瞬时的(也因为并不存在瞬时的东西)，但是它们的效果必须在某个时间点变得可见，从而造成操作是瞬时的错觉。这个时刻称为**线性化点(linearization point)**</u>。

​	越过写操作的线性化点(换句话说，当该值对于其他进程变得可见时)，每个进程必然看到该操作所写的值或更新的值(如果在该值之后还有其他的写操作)。<u>一个可见值应保持稳定，直到下一个值变得可见，寄存器不应在这两个临近的状态之间反复</u>。

> 当下大多数编程语言都提供了原子原语，允许原子写操作和原子比较-交换(CAS)操作。原子写操作不考虑当前寄存器值，这与CAS不同，CAS仅在前一个值没有变化时才从一个值变成另一个值[HERLIHY94]。读取值后修改它，并随后用CAS写入，这比简单地检查和设置值要复杂得多，因为可能存在**ABA问题**[DECHEⅥ10]：如果CAS期望A出现在寄存器中，那么值A将最终被设置，即使通过其他两个并发写入操作设置了值B并切换回值A。换句话说，值A本身不变并不能保证自上次读取以来该值没有被改过。

​	<u>线性化点起到切分点的作用，在此之后，操作效果变得可见。我们可以通过使用**锁**来保护临界区、**原子读/写**或**读-修改-写(read-modify-write)原语**来实现它</u>。

**可线性化的代价**

​	**<u>当今许多系统避免实现可线性化</u>**。

​	**在默认情况下，即使是CPU在访问主存时也不提供可线性化一致性**。这是因<u>为**同步指令开销大、速度慢，并且涉及跨节点CPU流量和缓存失效**</u>。

​	然而，**可以使用底层原语来实现可线性化**[MCKENNEY05a，MCKENNEY05b]。

​	在并发编程中，你可以使用CAS操作来引入可线性化。**<u>许多算法的工作方式都是先准备结果，然后使用CAS交换指针使结果可见</u>**。例如，并发队列可以这样实现：创建一个链表节点，然后原子性地将其追加到列表尾部[KHANCHANDANI8]。

​	在分布式系统中，可线性化需要协调冲突和顺序。它可以使用**共识算法**来实现：客户端使用消息与复制存储进行交互，**共识模块**负责确保应用的操作在整个集群中是一致且相同的。**毎个写操作将在它的调用和完成事件之间的某个时间点瞬间出现且仅出现一次**[HOWARD14]。

​	有趣的是，可线性化在其传统理解中被视为*局部性质*，并且意味着其是独立实现和验证的单元组合。**<u>合并可线性化的历史将产生一个同样可线性化的历史</u>**[HERLIHY90]。换句话说，其中所有对象都是可线性化的系统，也是可线性化的。这是一个非常有用的属性，但是我们应该记住，**它的范围仅限于单个对象**，而且即使对两个独立对象的操作是可线性化的，<u>涉及两个对象的操作还必须依赖于额外的同步手段</u>。

> **可重用可线性化基础设施**
>
> ​	**可重用可线性化基础设施(Reusable Infrastructure For Linearizability，RIFL)是一种用于实现可线性化远程过程调用(RPC)的机制**[LEE15]。在RIFL中，消息用客户端ID和客户端本地单调递增的序列号唯一标识。
>
> ​	为了分配客户端ID，RIFL使用由系统层面的服务颁发的租约——规避重复序列号的唯一标识符。如果发生故障的客户端试图使用过期的租约执行操作，它的操作将不会被提交：客户端必须接收新的租约，然后重试。
>
> ​	<u>如果服务器在写操作回复确认消息前崩溃，客户端可能会尝试重试此操作，而不知道此操作已被应用</u>。甚至会出现这样的情况：客户端C1写入值V1，但没有收到确认。同时，客户端C2写入值Ⅴ2。如果C1重试其操作并成功写入V1，则C2的写入操作将丢失。为了避免这种情况，系统需要防止重复执行重试操作。**当客户端重试操作时，<u>RIFL返回一个完成对象(completion object)来指示与它相关联的操作已经被执行并返回结果，而不是再次应用操作</u>**。
>
> ​	<u>**完成对象**与**实际数据记录**一起储存在持久存储中</u>。但是，它们的生存期是不同的：在发出请求的客户端保证它不会重试相关联的操作之前，或者服务器检测到客户端崩溃之前，完成对象都应该存在。而在这两种情况下，所有与它相关联的完成对象都可以被安全地删除。**创建完成对象的操作应该与它所关联数据记录的变化在一起原子地执行**。
>
> ​	**客户端必须定期续订租约，以表明它们的活动性**。<u>如果客户端未能续订其租约，它将被标记为已崩溃的，并且所有与其租约相关联的数据都将被垃圾回收</u>。租约有个有限的生存期，以确保属于故障进程的操作不会被永远保留在日志中。**如果出现故障的客户端试图使用到期的租约继续运行，其结果将不会被提交，客户端将不得不从头开始**。
>
> ​	<u>**RIFL的优点在于，通过保证RPC不会被多次执行**，一个操作可以通过确保其**结果原子可见**而**实现可线性化**，并且它的大部分实现细节独立于底层存储系统</u>。

### * 11.5.3 顺序一致性

​	实现可线性化代价可能过高，但是可以在放松模型的同时仍然提供相当强的一致性保证。<u>*顺序一致性(sequential consistency)*允许对操作进行排序，就好像它们是以某种串行顺序执行的一样，并要求属于某一进程的操作在排序后保持先后顺序不变(不同进程的操作之间，先后顺序可能改变)</u>。

​	进程可以观察到其他参与者执行操作的顺序与它们自己的历史相一致，但是从全局角度看，这个视图可能会过时任意长时间[KINGSBURY18a]。<u>进程之间的执行顺序是未定义的，因为这里没有共享的时间概念</u>。

​	顺序一致性最初是在并发这个领域中引入的，被描述为一种正确执行多处理器程序的方式。最初的描述要求对同一单元的内存请求在队列中排序(FIFO，到达顺序)，对独立内存单元的重叠写操作没有施加全局排序，并且允许读操作从内存单元中获取值(如果队列非空则从队列中获取最新值)[LAMPORT79]。这个例子有助于理解顺序一致性的语义。<u>操作可以以不同的方式排序(取决于到达顺序，甚至可以在两个写入同时到达的情况下任意排序)，但**所有进程都以相同的顺序观察到操作**</u>。

​	每个进程都可以按照自己的程序指定的顺序发出读写请求，这是很符合直觉的，因为任何非并发的单线程程序都一个接一个地执行它的步骤。<u>从同一进程传播的所有写操作都按此进程提交它们的顺序出现。**来自不同来源的操作可以任意排序，但从读取者的角度来看，这个顺序是一致的**</u>。

> ​	顺序一致性常与可线性化混淆，因为两者具有相似的语义。顺序一致性与可线性化一样，要求操作的全局有序性，而<u>可线性化要求每个过程的**局部有序性**与**全局有序性**一致</u>。换句话说，可线性化遵循操作的真实顺序。而在顺序一致性条件下，顺序仅适用于来源相同的写入[VIOTTI16]。另一个重要的区别在于组合：<u>我们可以组合可线性化的历史，并且仍然期望结果是可线性化的，而**顺序一致的调度是不可组合的**[ATTIYA94]</u>。

​	**<u>读到过时值(stale read)可以用副本差异来解释，例如：即使写入以相同的顺序传播到不同的副本，它们也可能在不同的时间到达</u>**。

​	**<u>顺序一致性与可线性化的主要区别在于没有全局强制的时间界限</u>**。在可线性化中，一个操作必须在其挂钟时间范围内变得有效。当写入操作W1完成时，它的结果必须已经被应用，并且每个读取者应该能够看到至少与W1所写入的值一样新的值。类似地，在一个读操作R1返回之后，在它之后执行的任何读操作都应该返回R1已经看到的值或更晚的值(当然，这也必须遵循同样的规则)。

​	顺序一致性放宽了这一要求：操作的结果可以在操作完成后才变得可见，只要从单个参与者的角度来看顺序是一致的。**同一来源的写操作程序不能互相"跳过"：它们的程序顺序(相对于自己的执行进程)必须保持不变。另一个限制是<u>操作出现的顺序必须对所有读取者一致</u>**。

​	**<u>与可线性化相似，现代CPU在默认情况下不保证顺序一致性</u>**，而且，由于处理器可以重新排序指令，我们应该使用**<u>内存屏障</u>**(也称为栅栏(fences))来确保写入操作对并发线程按顺序可见[DREPPER07，GEORGOPOULOS16]。

### * 11.5.4 因果一致性

​	尽管通常没有必要使用全局操作顺序，但在某些操作之间建立顺序可能是必要的。在因果一致性(causal consistency)模型下，所有过程都必须以相同的顺序看到因果相关的操作。没有因果关系的并发写入可以被不同的进程以不同的顺序观察到。

​	首先，让我们来看看为什么需要因果关系，以及没有因果关系的写操作是如何传播的。进程P1和P2进行非因果顺序的写操作。这些操作的结果可以在不同的时间无序地传播到读取者。进程P3先看到值1，然后才看到值2，而P4将首先看到值2，然后才看到值1。

​	一个有因果关系的写入的例子。除了写入值之外，我们现在还必须指定一个<u>逻辑时钟值</u>，以便在操作之间建立因果顺序。P1以写入操作 write(x，0，1) -> t1开始，该写入操作从初始值开始。P2执行另一个写操作 write(x，t1，2)，并指定其逻辑排序在t1之后，要求操作仅按逻辑时钟建立的顺序传播。

​	这样便在这些操作之间建立了因果顺序。<u>即使后一个写入比前一个传播得更快，在它的所有依赖项到达之前，该写入是不可见的，并且事件顺序会从它们的逻辑时间戳重建</u>。换句话说，**在逻辑上建立了发生在前(happened-before)的关系，而不使用物理时钟并且所有进程都认同这个顺序**。

​	**<u>在因果一致的系统中，我们获得了应用程序的会话保证，确保数据库视图与它自己的行动相一致，即使它在不同的、潜在的不一致的服务器上执行读写请求</u>**[TERRY94]。这保证是：单调读、单调写、读自己写(read-your-wite)、读后写(write-follow-read)。你可以在11.6节中找到关于这些会话模型的更多信息。

​	**<u>因果一致性可以使用逻辑时钟(logical clock)[LAMPORT78]来实现</u>**，在每个消息中发送上下文元数据，并总结哪些操作在逻辑上先于当前操作。<u>当从服务器接收到更新时，更新包含了上下文的最新版本。任何操作只有在其前面的所有操作都已应用的情况下才能被处理</u>。上下文不匹配的消息将缓存在服务器上，因为传递这些消息还为时过早。

​	实现因果一致性的两个重要且经常被引用的项目是Clusters of Order-Preserving Servers(COPS)[LLOYD11]和 Eiger[LLOYD13]。这两个项目都通过一个库实现因果关系(被实现为用户要连接的前端服务器)，并跟踪依赖关系以确保一致性。COPS通过键版本跟踪依赖关系，而Eiger则建立操作顺序(Eiger中的操作可能依赖于在其他节点上执行的操作，例如，在多分区事务的情况下)。这两个项目都不像最终一致的存储那样暴露无序操作。相反，<u>它们检测和处理冲突：在COPS中，这通过检查键顺序和使用应用程序特定的函数来完成；而 Eiger实现了“最后写胜出” (last-write-wins)规则</u>。

**向量时钟**

​	建立因果顺序使得系统可以在消息无序传递的情况下重新构建事件序列，填补消息之间的空隙，避免在某些消息仍然缺失的情况下发布操作结果。例如，如果消息{M1(∅，t1)，M2(M1，t2)，M3(M2，t3)}(均指定了依赖关系)是因果相关的，但以乱序传播，则进程会将它们缓冲起来，直到收集到所有的操作依赖项并恢复消息的因果顺序[KINGSBURY18b]。许多数据库，例如Dynamo[DECANDIA07]和Riak[SHEEHY10a]，使用向量时钟[LAMPORT78， MATTERN88]来建立因果顺序。

​	**向量时钟(vector clock)**是一种用于在事件之间建立偏序、检测和解决事件链之间分歧的结构。利用向量时钟，我们可以模拟公共时间和全局状态，将异步事件表示为同步事件。<u>进程维护逻辑时钟的向量，其中**毎个时钟对应一个进程**</u>。每个时钟从初始值开始每次新事件到达(例如发生写入)时递增。当从其他进程接收到时钟向量时，进程将其本地向量中的各个值更新为接收到向量中对应进程时钟的最大值(即，传输节点曾经看到的最大时钟值)。

​	<u>为了使用向量时钟来解决冲突，每当我们对数据库进行写入时，首先检查写入键的值是否已经在本地存在。如果前值已经存在，我们将一个新版本追加到版本向量中，从而建立两个写入之间的因果关系。否则，我们启动一个新的事件链，并用单个版本初始化该值</u>。

​	我们在两种场景下讨论了一致性问题：共享内存寄存器的访问以及真实(依据挂钟时间)的操作顺序。讨论过程中，我们首先提到了**潜在的副本差异**。因为<u>只有对相同内存位置的写操作才必须排序，所以如果值是独立的，就不会出现写冲突的情况</u>[LAMPORT79]。

​	<u>由于我们要找的是一个能够提高可用性和性能的一致性模型，**我们必须允许副本出现分歧**——不仅因为读到过期数据，还因为接受潜在冲突的写操作</u>。因此系统被允许创建两个独立的事件链。从一个副本的角度来看，我们看到历史为1，5，7，8，而另一个副本认为是1，5，3。Riak允许用户查看不同的历史记录并解决冲突[DAILY13]。

> ​	为了实现因果一致性，我们必须存储因果关系历史，支持垃圾收集，并要求用户在冲突的情况下解决分歧。
>
> ​	而**向量时钟可以告诉你冲突已经发生，但并不提出确切的解决方法**，因为解决语义通常是应用程序特定的。
>
> ​	因此，一些最终致的数据库，例如Apache Cassandra，不会对操作进行因果排序，而是使用“**最后写胜出**”规则来解决冲突[ELLIS13]。

## * 11.6 会话模型

​	从值传播的角度考虑一致性对于数据库开发者很有用，因为它有助于我们去理解和施加所需的数据不变式，但是有些事情却是从客户端角度看更容易理解和解释。这里，我们可以从单个客户端而不是多个客户端的角度来看待分布式系统。

​	**会话模型(session mode)**[VIOTTI16] (也称为以客户端为中心的一致性模型[TANENBAUM06])帮助我们从客户端的角度推理分布式系统的状态：*每个客户端在发出读写操作时如何观察系统的状态*。

​	<u>如果说到目前为止我们讨论的其他一致性模型聚焦在解释同时存在多个客户端时的操作顺序，那么以客户端为中心的一致性，则是聚焦在单个客户端如何与系统交互</u>。我们仍然假设每个客户端的操作是顺序的：它必须在完成一个操作之后才能开始执行下一个操作。如果客户端在其操作完成之前崩溃或失去与服务器的连接，我们不对未完成操作的状态做任何假设。

​	在分布式系统中，客户端通常可以连接到任何可用的副本，并且如果最近对一个副本的写入结果没有传播到另一个副本，则客户端可能无法观察到该写入所做的状态更改。

​	<u>一个合理的期望是，客户端发出的毎一个写操作都对它自己可见。这个假设在**读自己写(read-own-write)一致性模型**下成立，该模型声明：当写操作完成后，在**相同或其他副本**上的每个读操作都必须能够观察到写入的值。例如，在 **write(x，V)之后立即执行的read(x)将返回值V**</u>。

​	**单调读(monotonic read)模型**限制值的可见性，它声明：如果read(×)已经观察到值V，则接下来的读必须观察到至少与V一样新的值或某个更新的值。

​	**单调写(monotonic write)模型**假定同一客户端写入的值以写入操作执行的顺序出现。如果根据客户端会话顺序，在 write(x，V1)操作之后进行 write(x，V2)操作，则它们的效果必须以相同的顺序(即先V1后V2)对所有其他进程可见。如果没有这种假设，旧数据则可能“复活”，从而导致数据丢失。

​	**读后写(Write-follow-read**，有时称为会话因果关系)确保写入被排在之前的读取操作观察到的写入之后。<u>例如，如果write(x，V2)操作排在返回V1的read(x)操作之后，则write(x，V2)将排在write(x，V1)之后</u>。

> ​	会话模型对由不同进程(客户端)或来自不同逻辑会话的操作没有任何假设[TANENBAUM14]。这些模型从单个进程的角度描述操作排序。然而，系统中的毎一个进程都必须有相同的保证。换句话说，如果P1能够读取它自己的写操作，那么P2也应该能够读取它自己的写操作。

​	将单调读、单调写和读自己写结合起来，可以提供流水线随机访问存储器(Pipelined RAM，PRAM)一致性[LIPTON88， BRZEZINSKI03]，也称为**FIFO一致性**。<u>PRAM保证来自一个进程的写操作将按照该进程执行它们的顺序传播。与顺序一致性不同的是，来自不同进程的写入可以被以不同的顺序观察到</u>。

​	以客户端为中心的一致性模型所描述的特性很有用，大多数情况下，**分布式系统的开发者利用这些特性来验证他们的系统并简化其使用**。

## * 11.7 最终一致性

​	在多处理器编程和分布式系统中，同步都是昂贵的。正如在11.5节中所讨论的，我们可以放松一致性保证，允许节点之间存在一些差异。例如，顺序一致性允许读取以不同的速度传播。

​	**在最终一致性(eventual consistency)下，更新将异步地在系统中传播**。形式上说，它声明如果没有对数据项执行额外的更新，最终(eventually)所有的访问都会返回最新写入的值[VOGELS09]。在冲突的情况下，最新值的概念可能会改变，因为会取决于使用何种冲突解决策略(如最后写胜出或使用向量时钟(参见11.5.4节))来协调分歧副本中的值。

​	<u>*最终*是描述值传播的一个有趣的术语，因为它没有指定它必须发生的硬性时间限制。如果传递服务仅仅提供了一个“最终”的保证，这听起来就很不可靠。**然而在实践中，这个模型工作得很好，当下许多数据库都是最终一致的**</u>。

## * 11.8 可调一致性

​	最终一致性的系统有时用CAP来描述：你可以用可用性来换取一致性，反之亦然(参见1.2节)。从服务器端的角度来看，最终一致的系统通常实现可调一致性，其中使用三个变量调节数据的复制、读取和写入：

+ **复制因子N**

  储存数据副本的节点数。

+ **写入一致性W**

  使写入成功需要确认的节点数。

+ **读取一致性R**

  使读取成功需要响应的节点数。

​	<u>**通过选择一致性级别使之满足R+W>N，系统可以保证返回最近写入的值，因为读取集合和写入集合之间总是存在重叠**</u>。例如，如果N=3，W=2，R=2，则系统仅能容忍一个节点的故障。三个节点中的两个必须确认写操作。理想情况下，系统还会异步地将写复制到第三个节点。如果第三个节点宕机，反熵机制(参见第12章)最终会传播这个写入。

​	读取时，至少需要有三个副本中的两个来处理请求，才能回复一致的结果。任何一组节点都将至少包含一个具有给定键的最新记录的节点。

> ​	当执行写操作时，协调者应该将其提交给N个节点，但是**在继续操作之前只等待W个节点**(或者在协调者也是副本的情况下等待W-1个节点)。其余的写操作可以异步完成或失败。类似地，当执行读取时，协调者必须收集至少R个响应。某些数据库使用*推测执行(speculative execution)*提交额外的读取请求以减少协调者响应延迟。这意味着如果最初提交的读请求中的一个失败或到达缓慢，推测请求可以被计入R。

​	<u>写入较多的系统有时可能选择W=1，R=N，这允许写操作仅由一个节点确认，但也会要求所有副本(甚至可能出现故障的副本)都可用于读操作。对于W=N，R=1的组合也是如此：写入必须应用到所有副本才算成功，因此便可以从任何节点读取最新值</u>。

​	**增加读或写一致性级别会增加延迟，并提高请求期间对节点可用性的要求**。而降低一致性级别则可以提高系统的可用性，同时牺牲一致性。

> Quorum
>
> ​	**由[N/2]+1个节点组成的一致性级别称为Quorum，即多数派节点**。网络分区或节点故障的情况下，在2f+1个节点的系统中，如果不可用的节点数不大于f，则活动节点还可以继续接受写入或读取。换句话说，这样的系统最多能容忍f个节点故障。
>
> ​	当使用Quorum执行读写操作时，系统不能容忍多数派节点的故障。例如，如果总共有三个副本，其中两个已宕机，则读写操作将无法达到读写一致性所需的节点数，因为三个节点中只有一个节点能够响应请求。
>
> ​	在不完全写入的情况下，使用Quorum进行读写并不能保证单调性。如果在向三个副本中的一个副本写入值之后，某个写操作失败，则Quorum读取可能返回不完全操作的结果或旧值，具体取决于所联系的副本。由于后续的对相同值的读取不需要联系同一个副本，因此它们返回的值可能交替出现两种结果。**为了实现读单调性(在牺牲可用性的情况下)，我们需要使用阻塞读修复(参见12.1节)**。

## * 11.9 见证者副本

​	在读取一致性上使用Quorum能够提高可用性：即使某些节点宕机，数据库系统仍然可以接受读取和写入。要求多数派参与保证了在任意多数派集合中至少有一个节点是重叠的，因此任何 Quorum读取都将观察到最近完成的 Quorum写入操作。但是，<u>使用复制和多数派会增加存储成本：我们必须在每个副本上储存数据的一份拷贝</u>。如果我们的复制因子是5，则我们必须存储5个拷贝。

​	<u>我们可以通过使用见证者副本(witness replica)来降低存储成本。我们不需要在毎个副本上存储数据记录的拷贝，而是可以将副本分为拷贝副本(copy replica)和见证者副本</u>。拷贝副本仍然持有以前的数据记录。在正常操作下，<u>见证者副本仅储存一些记录，表示写操作发生过的事实</u>。然而，当拷贝副本的数量太少时，可能会发生一种情况，例如我们有三个拷贝副本和两个见证者副本，此时两个拷贝副本宕机，我们最终的Quorum是个拷贝副本和两个见证者副本。

​	而在写入超时或拷贝副本发生故障的情况下，为了临时存储记录，可以升级见证者副本以临时代替故障或超时的拷贝副本。一旦原来的拷贝副本恢复，升级的副本就可以回退到它以前的状态，或者也可以让恢复后的副本成为见证者副本。

​	让我们考虑一个三节点的复制系统，其中两个节点保存数据拷贝，第三个节点是见证者副本：[1c，2c，3w]。我们试图进行写操作，但2c暂时不可用，无法完成这个写操作在这种情况下，我们将这条记录临时储存在见证者副本3w上。无论2c何时恢复，修复机制都可以使其恢复到最新状态，并从见证者副本上移除多余的拷贝。

​	<u>另一种情况，我们尝试执行读取，要读的记录出现在1c和3W上，但不在2c上。由于任意两个副本足以构成 Quorum，只要有任意两个节点的子集可用，无论是两个拷贝副本[1c，2c]，还是一个拷贝副本和一个见证者副本(即[1c，3w]或[2c，3w])，我们都可以保证提供一致的结果。如果从[1c，2c]读取，我们从1c处获取最新的记录，并可以将其复制到2c，因为2c上缺少这个值。如果只有[2c，3w]可用，则可从3W获取最新记录。要恢复原来的节点配置并让2c追上最新数据，可以将该记录复制到2c并从见证者副本3w中删除</u>。

​	更一般地，只要我们遵循如下两个规则，那么n个拷贝副本和m个见证者副本就能达到与n+m个副本相同的可用性保证：

+ **使用多数派(即N/2+1个参与者)执行读和写操作**。
+ **该Quorum中至少有一个副本是拷贝副本**。

​	能这么做的原因是数据一定要么在拷贝副本上，要么在见证者副本上。在发生故障时，修复机制会更新拷贝副本，在此期间数据储存在见证者副本上。

​	使用见证者副本可以降低存储成本，同时保持一致性约束。这种方法有几种实现，例如Spanner[CORBETT12]和Apache Cassandra。

## * 11.10 强最终一致性和CRDT

> [CRDT——解决最终一致问题的利器_hellozhxy的博客-CSDN博客](https://blog.csdn.net/hellozhxy/article/details/103527186)
>
> [聊聊CRDT - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1422458)

​	我们讨论了几种强一致性模型，例如可线性化和串行化，以及弱一致性的一种形式：最终一致性。在两者之间还存在一种可能的中间地带——强最终一致性，提供了两种模式各自的一些好处。该模型下，更新可能会延迟或无序地传播到其他节点，但当所有更新最终传播到目标节点时，它们之间的冲突可以被解决，并且它们合并后将产生相同的有效状态[GOMES17]。

​	在某些条件下，我们可以放宽一致性要求，允许操作保留附加的状态，该附加状态使得我们可以在操作执行后协调(或者说合并)出现分歧的状态。这种方法最突出的一个例子是在Redis[Blyikoglu13]中实现的*无冲突复制数据类型*(Conflict-Free ReplicatedData Type，CRDT，参见文献[SHAPIRO11a])。

​	**CRDT是一种特殊的数据结构，它排除了冲突的存在，允许以任意顺序应用对它的操作而不改变结果。这个特性在分布式系统中非常有用**。例如，在使用无冲突复制计数器的多节点系统中，我们可以独立地增加毎个节点上的计数器值，即使它们由于网络分区而不能相互通信。一旦恢复通信，来自所有节点的结果就可以进行协调与合并，分区期间应用的任何操作都不会丢失。

​	这样的特性使得CRDT在最终一致性系统中很有用，因为<u>它允许系统中的副本状态临时出现分歧</u>。副本可以在本地执行操作，而无须事先与其他节点同步，操作最终会(可能无序地)传播到所有其他副本。CRDT让我们能够从局部的个体状态或操作序列重建完整的系统状态。

​	CRDT最简单的例子是基于操作的可交换复制数据类型(Commutative Replicated DataType， CmRDT)。 CmRDT的操作需要满足：

+ 无副作用

  应用这些操作不会改变系统状态。

+ 可交换

  参数顺序无关紧要：x·y=y·x。换句话说，无论是x与y合并还是y与X合并不改变最终结果。

+ 按照因果关系排序

  操作成功传递所依赖的前提条件是：要确保系统已经达到操作可以应用的状态。

​	例如，我们可以实现仅增长计数器(grow-only counter)。每个服务器可以保存一个状态向量，该状态向量由包含所有其他参与者的最近已知计数器更新，初始值为零。每个服务器只允许修改向量中自己的值。当更新被传播时，函数merge(state1，state2)合并来自两个服务器的状态。

​	例如，我们有三台服务器，都有已被初始化而处于初始状态的向量：

```pseudocode
节点1:			节点2:		节点3:
[0,0,0]		[0,0,0]		[0,0,0]
```

​	如果我们更新第一个和第三个节点上的计数器，它们的状态会发生如下变化：

```pseudocode
节点1:			节点2:		节点3:
[1,0,0]		[0,0,0]		[0,0,1]
```

​	当更新传播时，我们使用一个合并函数，选取每个位置上的最大值来组合结果：

节点1(已传播节点3的状态向量):

```pseudocode
merge([1，0，0]，[0，0，1])=[1，0，1]
```

节点2(已传播节点1的状态向量)：

```pseudocode
merge([0，0，0]，[1，0，0])=[1，0，0]
```

节点2(已传播节点3的状态向量)：

```pseudocode
merge([1，0，0]，[0，0，1)=[1，0，1]
```

节点3(已传播节点1的状态向量)：

```pseudocode
merge([0，0，1]，[1，0，0])=[1，0，1]
```

​	为了确定当前向量状态，可以计算所有位置上的值的和：sum([1，0，1])=2。合并函数是可交换的。由于服务器只能更新自己的值，而且这些值是相互独立的，因此不需要额外的协调。

​	通过节点用于递增计数的向量P和节点用于递减计数的向量N，可以实现一个支持增减的正负计数器(PN-Counter)。在较大规模的系统中，为了避免传播巨大的向量，可以使用*超级对等节点(super-peers)*。超级对等节点复制计数器的状态，从而帮助避免持续的点对点交流[SHAPIRO11b]。

​	要保存和复制值，我们可以使用寄存器。最简单的寄存器是**最后写入胜出(Last-Write-Wins，LWW)寄存器**，它为每个值存储一个唯一的、全局排序的时间戳以解决冲突。当发生写冲突时，我们只保留具有较大时间戳的那个值。这里的合并操作(选择具有最大间戳的值)也是可交换的，因为它依赖于时间戳。如果不允许丢弃值，我们可以提供应用程序特定的合并逻辑，并使用多值寄存器，该寄存器存储所有写入的值，让应用程序来选择正确的值。

​	CRDT的另一个例子是无序仅增长集合(Grow-only Set，G-Set)。每个节点维护本地状态，并可以向其追加元素。添加元素将生成一个有效集合。合并两个集合也是一个可交换的操作。与计数器类似，我们可以使用两个集合来支持增加和删除。<u>在这种情况下我们必须遵循一个约束：只有在增加集合中的值才能被插入到删除集合中。要想重建集合的当前状态，可以从增加集合中移除被包含在删除集合中的所有元素[ SHAPIRO1b]</u>。

​	结合更复杂结构的另一个无冲突类型的例子是无冲突复制JSON数据类型，它允许对具有列表和映射表类型的深度嵌套JSON文档进行诸如插入、删除和赋值等修改。此算法在客户端执行合并操作，并且不要求操作以任何特定顺序传播[KLEPPMANN14]。

​	CRDT为我们提供了相当多的可能性，我们看到越来越多的数据存储利用它来提供强最终一致性(SEC)。这是一个强有力的概念，我们可以将其添加到构建容错分布式系统的工具库中。

## 11.11 本章小结

​	容错系统使用复制来提髙可用性：即使某些进程发生故障或没有响应，系统作为一个整体也可以继续正常运行。但是，保持多个副本同步需要额外的协调。

​	我们讨论了几个单操作一致性模型，从保证最多的到保证最少的依次是：

+ 可线性化

  操作看起来是瞬时应用的，并且保持操作实际的顺序。

+ 顺序一致性

  操作效果是以某种全序传播的，此顺序与它们被单个进程执行的顺序一致。

+ 因果一致性

  因果相关的操作的效果以相同的顺序对所有进程可见。

+ PRAM/FIFO 一致性

  操作变为可见的顺序与它们在各个进程执行的顺序相同。来自不同进程的写操作可能会被以不同的顺序观察到。

​	之后，我们讨论了多个会话模型：

+ 读自己写

  读操作反映先前的写操作。写操作经过系统传播一定能被之后来自同一客户端的读操作看到。

+ 单调读

  任何读操作不能读到比已经读过的值更早的值。

+ 单调写

  来自同一客户端的写入按此客户端执行的顺序传播到其他客户端。

+ 读后写

  同一个客户端执行的读操作如果观察到一些写操作，则后续的写操作将会排在这些写操作之后。

​	理解这些概念可以帮助你理解底层系统能提供何种保证，并将其用于应用程序开发。一致性模型描述了操作数据必须遵循的规则，但仅限于特定的系统。将具有较弱保证的系统构建在具有较强保证的系统之上，或者忽略底层系统的一致性含义，可能会导致不可恢复的不一致性和数据丢失。

​	我们还讨论了最终一致性和可调一致性的概念。基于Quorum的系统使用多数派来保证数据的一致性。见证者副本可用于降低存储成本。

# 12. 反熵和传播

​	到目前为止，我们讨论的通信模式都是点对点(对等的)或一对多的(协调者和副本)。为了在整个系统中可靠地传播数据记录，我们需要传播节点本身是可用的并能够访问到其他节点，但是即便如此，吞吐量也受单台机器限制。

​	快速而可靠的传播可能不太适用于大量数据记录，但是对集群范围的元数据却很重要，例如成员信息(节点加入或离开)、节点状态、故障情况、表结构变更等。传达这些信息的消息往往出现频率不高且数据量很小，但是必须被尽可能快且可靠地传播。

​	将此类的更新传播到集群中的所有节点通常有以下三大类方法[DEMERS87]：

+ a)通知从一个进程广播到其他所有进程。
+ b)定期的点对点交换信息，对等节点成对地连接并交换消息。
+ c)合作广播，消息接收者会成为广播者，帮助更快更可靠地传播消息。

​	当集群中的节点数较少时，将消息广播到其他所有进程是最简单直接的方法。但在大型集群中，由于节点数量庞大，广播会变得很昂贵。并且由于过度依赖单个进程，广播也很不可靠。各个进程可能并不总是知道网络中其他所有进程的存在。此外，广播的进程及其毎个接收者都必须在工作时间上有重叠，而这在某些情况下可能难以实现。

​	为了放宽这些限制，我们可以假设某些更新可能会传播失败。协调者尽力将消息传递给所有可用的参与者，而<u>一旦出现故障，**反熵(anti-entropy)机制**会让节点重新同步</u>。这样，传递消息的责任由系统中所有节点共同承担，并分成两个步骤：<u>主要传递</u>和<u>定期同步</u>。

​	熵(entropy)是一种衡量系统无序程度的属性。**在分布式系统中，熵表示节点之间的状态分歧程度**。我们希望将熵保持在最低的水平，有许多技术可以帮助减少熵。

​	<u>反熵机制通常用于在主要传递机制失败的情况下将节点恢复最新状态</u>。即使协调者在某个时刻发生故障，系统也可以继续正常运行，因为其他节点将继续传播信息。换句话说，<u>在最终一致的系统中，反熵被用来降低收敛的时间界限</u>。

​	为了保持各节点的同步，反熵会触发一个后台或前台进程，比较和调和丢失或冲突的记录。后台反熵进程会利用Merkle树之类的辅助数据结构，从更新日志中识别出分歧的数据。前台反熵进程会捎带(piggyback)地在读取或写入请求上附加额外逻辑，比如提示移交、读修复等。

​	如果多副本系统中的副本间出现分歧，为了恢复一致性和同步，我们必须通过成对比较副本状态来查找和修复丢失的记录。对于大型数据集这会非常耗时：我们必须从两个节点分别读出整个数据集，并将尚未传播的、包含最新状态的更改通知相关副本。为了降低成本，我们可以考虑副本过期的方式以及数据访问的模式。

## 12.1 读修复

​	读取时最容易检测出副本之间的差异，因为此时我们可以联系副本，从每个副本上查询状态，然后检查它们的响应是否一致。注意，这种情况下，我们不会查询每个副本上存储的完整数据集，而是将目标限制在那些客户端请求的数据。

​	<u>协调者执行分布式读取时会乐观地假设各个副本是同步的并且存有相同的信息。如果副本的响应不同，协调者会把缺失的变更发送给相应的副本</u>。

​	这种机制称为**读修复(read repair)，它常常被用于检测和消除数据不一致**。<u>读修复中，协调者节点向各个副本发出请求，等待副本响应并对其进行比较。如果某些副本因错过了最近的变更而导致它们的响应不同，那么协调者会检测出不一致，并将变更发送回副本[DECANDIA07]</u>。

​	有些Dynamo式的数据库选择不要求联系所有的副本，而改用可调的一致性级别。要返回一致的结果，我们不必联系并修复所有副本，而只需联系满足一致性级别的节点数。如果我们进行Quorum读取和写入，我们仍可以获得一致的结果，但是某些副本可能仍未包含所有写入。

​	读修复可以实现为阻塞或异步操作。阻塞读修复中，原始的客户端请求必须等到协调者“修复”副本才能返回。而异步读修复仅仅只是调度一个修复任务，读请求的结果可以立即返回给用户。

​	**对于Quorum读来说，阻塞读修复能确保读取的单调性**(参见11.6节)：一旦客户端读取到某个值之后，后续的读请求要么返回一样的值，要么返回更新的值，因为副本状态已经修复了。如果不使用 Quorum读，则会失去这一单调性保证，因为在后续读取时数据可能尚未传播到目标节点。与此同时，阻塞读修复会牺牲可用性，因为修复需要由目标副本确认，并且只有当它们响应后才能返回。

​	为了准确检测出各副本响应的记录之间的差异，一些数据库(例如 Apache Cassandra)使用了具有合并监听器(merge listener)的专用迭代器，它能重新构建出合并后的结果与各个输入之间的差异。之后，协调者将这些差异通知副本，告知它们缺失的数据。

​	读修复假定副本大多数情况下是同步的，我们不希望毎个请求都回退到阻塞修复。由于阻塞修复的读取单调性，只要中间没有发生写操作，我们可以期望后续请求返回相同且一致的结果。

## 12.2 摘要读

​	协调者也可以不向每个节点都发出完全读取请求，而是只发一个完全读取请求，而向其他副本仅仅请求数据的摘要(digest)。<u>摘要请求读取副本本地数据——不是完整的数据快照，而是计算出数据的哈希值</u>。<u>之后，协调者可以算出完整读取的哈希值，并将其与所有其他节点返回的摘要进行比较。如果所有摘要都匹配，则可以确信各个副本是同步的</u>。

​	如果摘要不匹配，协调者无法确定哪些副本是领先的、哪些副本是滞后的。为了使滞后的副本与其他节点恢复同步，协调者需要向那些摘要与其他节点不同的副本发出完全读取请求，比较它们的响应，找出差异之处，并将变更发送给滞后的副本。

> ​	摘要通常使用非加密哈希函数(例如MD5)来计算，因为只有快速地算出摘要才能让“快乐路径”性能更佳。哈希函数可能发生冲突，但是对于大多数实际系统而言，冲突可能性可以忽略不计。数据库通常有不止一种反熵机制，因此我们可以预期，即使在(不太可能发生的)哈希冲突的情况下，数据也将会被其他的子系统修复。

## 12.3 提示移交

​	另一个反熵方法称为提示移交(hinted handoff)[DECANDIA07]，这是一种**写侧修复机制**。如果目标节点未能确认写入，则写入协调者或某一副本会存储一条特殊的记录，称为一个提示(hint)，当目标节点恢复后，该记录会立即被重放过去。

​	在Apache Cassandra中，除非用的是ANY一致性级别[ELLIS11]，提示的写入不用满足复制因子(参见11.8节)，因为提示日志中的数据不会被用于读取，仅仅是帮助滞后的参与者追赶上来。

​	一些数据库(例如Riak)同时使用Sloppy Quorum和提示移交。在Sloppy Quorum中，当副本故障的情况下，写操作可以使用节点列表中的其他健康节点，这些节点可以不是该操作原来的目标副本。

​	例如，假设我们有一个包含节点{A，B，C，D，E}的五节点集群，其中{A，B，C}是写操作对应的副本，而节点B处于不可用状态。A作为查询的协调者，选择节点D以满足Sloppy Quorum并保持所需的可用性和持久性保证。<u>现在，数据被复制到{A，D，C}上。但是，D上的这条记录在元数据中带有一个提示，因为该写入最初是针对B的。一旦B恢复，D将会尝试向它转发提示。当提示在B上重放之后，就可以安全地删除这条记录，而不会减少副本的总数[DECANDIA07]</u>。

​	在类似的情况下，如果节点{B，C}由于网络分区而被短暂地与集群其他部分分开，并且对{A，D，E}进行了 Sloppy Quorum写入，那么，紧随其后的对{B，C}的读取将无法观察到最新的数据[DOWNEY12]。换句话说， **Sloppy Quorum提高了可用性但牺牲了一致性**。

## * 12.4 Merkle树

> [梅克尔树_百度百科 (baidu.com)](https://baike.baidu.com/item/梅克尔树/22456281?fr=aladdin)
>
> [区块链- Merkle树_朝歌-CSDN博客_merkle树](https://blog.csdn.net/qq_40452317/article/details/90482721)

​	由于读修复只能修复当前被查询到的数据的不一致性，我们需要另一套机制来寻找和修复未被查询到的数据的不一致性。

​	<u>正如我们之前所讨论的，要想精确找出副本之间哪些行存在不一致，需要成对地交换和比较数据记录。这是很昂贵且不切实际的。许多数据库使用*Merkle树*[MERKLE87]来降低数据比对的成本</u>。

​	**<u>Merkle树是一个对本地数据的紧凑的哈希表示，它是一棵由哈希值构成的树</u>**。哈希树的底层是通过扫描整个表的数据记录、计算记录范围的哈希值构成的。较高层级则来自对较低层级的哈希值再次进行哈希，从而建立一个层次结构表示形式，使得我们可以通过比较哈希值来快速检测出不一致性，或通过递归地遍历哈希树节点来缩小不一致的范围。这可以通过逐层交换和比较子树来完成，也可以交换和比较整棵树。

​	Merkle树的构成：最低层级包括数据记录范围的哈希值，而每个更高层级的哈希值是通过对下一层级的哈希值进行哈希得出的，递归地重复此过程直至树的根节点。

​	**<u>要确定两个副本间是否存在不一致，我们只需要比较其Merkle树中的根节点哈希值。通过自顶向下成对地比较哈希值，我们可以找到节点间存在差异的数据范围，并修复其中的数据记录</u>**。

​	**<u>由于Merkle树是自底向上递归计算的，因此一个数据的变化会触发整个子树的重新计算。在树的大小(决定了交换消息的大小)和它的精度(数据范围有多小，或者说有多精确)之间也存在一个权衡</u>**。

## * 12.5 位图版本向量

​	关于反熵的最新研究中引入了位图版本向量(bitmap version vector)[GONCALVES15]，它<u>基于最近更新情况来解决数据冲突</u>：**每个节点都会保存各个对等节点的操作日志，这些日志包含本地发生或从副本复制过来的事件**。**反熵时，我们会比较日志，并将丢失的数据复制到目标节点**。

​	<u>每次写入(由某一个节点协调)均表示为点(i，n)：由节点n协调的节点本地序列号为i的一个事件。序列号i从1开始，在每次节点执行写操作时递增</u>。

​	为了跟踪副本状态，我们使用<u>节点本地逻辑时钟</u>。每个时钟代表一组点的集合，表示该节点直接看到(由该节点本身协调的)或间接看到(由其他节点协调并复制过来的)的写入。

​	<u>在节点的逻辑时钟中，节点本身协调的事件之间没有间隙。如果某些写操作没有从其他节点复制过来，时钟则会包含间隙。为了让两个节点重新同步，可以让它们交换逻辑时钟，识别出缺失的点所表示的间隙，然后复制与之相关联的数据记录。为此，我们需要重新构建出毎个点引用的数据记录。这项信息存放在点因果容器(Dotted Causal Container，DCC)中，它将各点映射到给定键上的因果信息。这样一来，冲突解决过程就能够获取写操作之间的因果关系</u>。

​	该方法的**优势在于，它捕获了值写入之间的因果关系，并允许节点精确标识其他节点上缺少的数据点**。一个可能的**缺点是，如果节点长时间宕机，对等节点就一直无法截断日志，因为当滞后节点恢复时，还需要将数据复制给它**。

## 12.6 Gossip传播

​	为了纳入其他节点，并让更新的传播具有广播的范围和反熵的可靠性，我们可以用Gossip协议。

​	<u>Gossip协议是一种概率性的通信过程，它是基于谣言在人类社会中的传播方式或疾病在人群中的传播方式</u>。谣言和流行病十分形象地描述了该协议的工作方式：只要还有想听的人，谣言就会继续蔓延；只要人群中还有易感的成员，疾病就会继续传播。

​	Gossip协议的主要目标是使用协作式的传播将信息从一个进程传播到集群的其余部分就像病毒在整个人群中传播的过程，它从一个人传染给另一个人，每一步都可能扩大感染的范围， Gossip协议中信息也在系统中不断中继，使越来越多的进程参与进来。

​	<u>持有需要传播的记录的进程称为传染性的(infective)，而任何尚未收到更新的进程称为易感染的(susceptible)</u>。<u>传染性的进程经过一段时间的主动传播之后，不再传播新状态，这时我们称它为已删除的(removed)</u> [DEMERS87]。所有进程都以易感染状态开始。每当某个数据记录的更新到达时，接收到该更新的进程将变成传染性状态，并开始将更新分发给其他随机的相邻进程，从而感染它们。<u>一旦传染性进程确定更新已经传播，它们就会变成已删除状态</u>。

​	**<u>为了避免显式的协调和维护全局的接收者列表，以及避免要求单个协调者向系统中的每个参与者都广播消息，这类算法使用兴趣损失(loss of interest)函数建模完整性</u>**。协议的效率取决于：在将发送冗余消息的开销保持在最低的情况下，能多快地感染尽可能多的节点。

​	Gossip可用于同构(所有节点都是对等的)去中心系统中的异步消息传递，这类系统中，节点可能没有长期成员资格，也没有以任何拓扑进行组织。**由于Gossip协议通常不需要显式的协调，因此它们在具有灵活的成员关系(节点频繁加入和离开)或网状网络(mesh network)的系统中很有用**。

​	Gossip协议非常健壮，它能帮助我们在分布式系统存在固有故障的情况下实现高可靠性。<u>由于**消息是以随机方式中继**的，因此即使节点之间的某些通信组件发生故障，消息仍可以通过其他路径传递。可以说这样的系统天然能够适应故障</u>。

### 12.6.1 Gossip技术细节

​	<u>进程定期随机选择f个对等节点并与它们交换当前的“热”信息，其中f是可配置的参数，称为扇出(fanout)</u>。每当进程从其他对等节点获悉新信息时，它会尝试将其传递到更多节点。由于对等节点的选择是概率性的，因此总会有一些重叠，消息会被重复传递，并且可能会继续流传一段时间。消息冗余性是一个衡量重复传递开销的度量指标。**冗余性是一种重要的属性，它对于Gossip至关重要**。

​	**系统达到收敛所需的时间称为*延迟***。达到收敛(停止Gossip过程)和将消息传递给所有对等节点这两个概念存在细微的差异，因为消息可能会在很短时间内就通知到所有对等节点，但Gossip仍在继续。<u>扇岀和延迟取决于系统规模：在更大规模的系统中，我们要么增加扇出以保持延迟的稳定，要么允许更高的延迟</u>。

​	一段时间后，随着节点注意到它们一次又一次地接收到相同的信息，消息将开始失去重要性，节点最终将会停止中继。兴趣损失可以概率性地计算(每个进程的每一步都会计算传播停止的概率)，也可以使用一个阈值(对接收到重复消息的次数进行计数，当次数过高时停止传播)。两种方法都必须考虑集群的规模和扇出。对重复消息进行计数以衡量收敛性可以改善延迟并减少冗余[DEMERS87]。

​	<u>在一致性方面，Gossip协议提供*收敛*一致性[BIRMANO7]：节点对更早发生的事件具有一致视图的可能性更高</u>。

### 12.6.2 覆盖网络

​	尽管Gossip协议很重要也很有用，但它们通常只适用于有限范围的问题。非传染性的方法可以以非概率性的确定性、较少的冗余和往往更优的方式来分发消息[BIRMANO7]。<u>Gossip算法广受赞誉的主要是它的扩展性，以及它可以在logN个消息回合内分发一条消息(其中N是集群的大小) [KREMARRECO7]，但务必记住**Gossip回合中也会产生大量的冗余消息**。**为了达到可靠性，基于 Gossip的协议会产生一些重复的消息传递**</u>。

​	随机选择节点可以大大提高系统的健壮性：如果存在网络分区，只要尚且存在间接连接两个进程的链路，消息最终将被传递。<u>该方法的明显缺点是它不是消息最优的：为了保证健壮性，我们必须维护对等节点之间的冗余连接和发送冗余消息</u>。

​	两种方法的折中是在Gossip系统中<u>构建一个临时的固定拓扑</u>。为此，我们可以创建对等节点间的覆盖网络(overlay network)：节点可以对它的对等节点进行采样，并根据接近程度(通常由延迟来衡量)选择最佳的联系点。

​	系统中的节点可以构成生成树(spanning tree)：不含重边的无向无环图，并覆盖整个网络。有了这个图，消息就可以按照固定数量的步骤分发。

​	<u>这种方法的潜在缺点之一是，它可能会导致形成对等节点的“岛”——这些节点之间互相连接，并且彼此之间具有较强的偏好</u>。

​	为了保持较少的消息数量，同时在连接断开时能够快速恢复，我们可以在系统处于稳定状态时混合使用两种方法(固定拓扑和基于树的广播)，而当故障切换和系统恢复时回退到Gossip。

### 12.6.3 混合Gossip

​	*推送/惰性推送多播树(Plumtree)* [LEITAO07]在传染和基于树的广播之间做了权衡。**Plumtree的工作原理是构建出节点的生成树覆盖网络，从而以最小的开销主动分发消息**。正常情况下，节点将完整的消息发送给一个很小的对等节点子集，该子集由对等节点采样服务提供。

​	<u>毎个节点将完整的消息发送给很小的节点子集，而对于其余节点，它只是惰性地(lazily)转发消息ID。如果节点接收到它从未见过的消息标识符，它可以查询对等节点以获取这条消息。这个惰性推送(lazy-push)步骤能确保高可靠性，并提供一种快速修复广播树的方法。故障发生时，协议通过惰性推送步骤回退到Gossip，广播消息并修复覆盖网络</u>。

​	由于分布式系统的固有性质，任何节点或节点之间的链接都随时可能发生故障，从而使这一段无法访问，并导致无法遍历树。惰性Gossip网络帮助节点将看到的消息通知给对等节点，从而构造以及修复树。

​	<u>使用惰性推送机制进行树的构建和修复的一个优点是：在负载恒定的网络中，由于最先响应的节点会被加入广播树中，因此**该算法倾向于生成一棵最小化消息延迟的树**</u>。

### 12.6.4 局部视图

​	向所有已知的对等节点广播消息、维护集群的完整视图可能会很昂贵甚至不切实际，在流失率(衡量加入和离开系统的节点数量)较高时更是如此。为了避免这一代价，Gossip协议常常使用对等节点采样服务。该服务维护集群的局部视图，该视图定期用Gossip刷新。局部视图之间互相重叠，因为Gossip协议需要一定程度的冗余，但是过多的冗余也意味着更多的工作负担。

​	例如，**混合局部视图(Hybrid Partial View，HyParView)协议**[LEITAO07]维护集群的**较小的活跃视图(active view)**和**较大的被动视图(passive view)**。<u>活跃视图中的节点构成一个可用于传播消息的覆盖图。被动视图则用于维护节点列表，用于替换活跃视图中的故障节点</u>。

​	每隔一段时间，节点进行一次洗牌(shuffle)操作，在此期间它们互相交换活跃和被动视图。交换期间，节点将它从对等节点收到的被动或活跃视图中的成员都添加到自己的被动视图中，并滚动删除最旧的值以限制列表大小。

​	活跃视图的更新则是根据视图中节点的状态变更以及对等节点的请求。如果进程P1怀疑P2发生了故障(P2是P1活跃视图中的一个对等节点)，则P1会从它的活跃视图中删除P2，并尝试与被动视图中的替代进程P3建立连接。如果连接失败，则从P1的被动视图中删除P3。

​	根据P1活跃视图中的进程数，如果P3的活跃视图已满，它可以选择拒绝连接。如果P1的视图为空，则P3必须将其当前的活跃视图中的某一个对等节点替换成P1。<u>这能加快节点的启动或恢复过程，使其快速成为集群中的有效成员，但也可能会导致一些环状连接</u>。

​	该方法中，节点仅向活跃视图中的对等节点分发消息，因此减少了系统中的消息数量；同时，该方法使用被动视图实现恢复机制，从而保证了高可靠性。衡量算法性能和质量的一个途径是：当拓扑重新组织时，对等节点采样服务能以多快的速度收敛到一个稳定覆盖[JELASITY04]。HyParView在这项上的得分很高，这要归功于它的视图的维护方式，以及给启动过程赋予更高的优先级。

​	**<u>HyParView和Plumtree使用混合Gossip方法：使用一小部分对等节点来广播消息，但是在发生故障或网络分区时回退到更宽的对等网络形式</u>**。两种系统都不依赖所有节点的全局视图，这是很有用的特性——不仅因为系统中的节点数量很多(多数情况下并非这种情况)，也是因为<u>维护每个节点上的全局成员列表代价很高</u>。<u>局部视图允许节点仅与相邻节点的一个较小子集保持活动连接</u>。

## 12.7 本章小结

​	最终一致系统允许副本状态存在不一致。可调一致性使得我们可以牺牲一致性来换取可用性，或者反之。副本不一致可以通过下面反熵机制中的一种来解决：

+ 提示移交

  如果目标节点宕机，则将写入临时存储在临近的节点上，并在目标恢复后立即重放这些变更。

+ 读修复

  在读取期间比较各个响应，并检测缺失的记录，随后将其发给滞后副本，从而使被请求的数据范围恢复一致状态。

+ Merkle树

  通过计算和交换哈希树来检测需要修复的数据范围。

+ 位图版本向量

  维护有关最新写入信息的紧凑记录，利用它检测丢失的副本写入。

​	**上述反熵方法针对三个维度之一做了优化：缩小范围、最后更新时间或数据完整性**。

+ 为了减小反熵的范围，我们可以只同步正在被査询的数据(读修复)或个别缺失的写入(提示移交)。
+ 如果假设大多数故障都是暂时的并且参与者可以很快从中恢复，那么我们可以保存最近发生分歧的事件日志，从而准确地找出需要同步的数据(位图版本向量)。
+ 如果我们需要成对地比较多个节点上的完整数据集，并高效地找出它们之间的差异，则可以对数据进行哈希并比较哈希值(Merkle树)。

​	为了在大规模系统中可靠地分发信息，可以使用Gossip协议。混合Gossip协议减少了消息交换的数量，同时尽可能保持了容忍网络分区的能力。

​	**许多现代系统都使用Gossip来检测故障以及维护成员信息**[DECANDIA07]。 HyParView被用在Partisan中，这是一个高性能、高扩展性的分布式计算框架。Plumtree被用在Riak Core中，用于传递集群范围的信息。

# 13. 分布式事务

​	为了在分布式系统中维持秩序，我们至少要保证一定程度的一致性。在11.5节中，我们讨论了单个对象、单个操作的一致性模型，这些模型帮助我们论证单个操作的正确性。但是，在数据库中我们经常需要原子地执行多个操作。

​	原子操作可以用状态转移来解释：在启动某个事务之前，数据库处于状态A；当事务完成的时候，状态从A变成B。从操作的角度看这很容易理解，因为事务没有附带一个事先确定好的状态。相反，事务从某个时间点开始将操作应用于数据记录。这使我们在调度和执行方面具有一定的灵活性：事务可以被重新排序甚至重试。

​	事务处理主要关注的是决定一个合法的执行历史，以建模和表示可能的交错执行方案。在这种情况下，历史代表一个依赖关系图：哪些事务在当前事务之前执行。如果一个执行历史与事务的某一串行历史等效(即具有相同的依赖关系图)，则称它为可串行化的。你可以回顾5.3.1节执行历史的概念、历史的等效性、可串行化以及其他概念。总的来看，本章是第5章在分布式系统中的对应，第5章中我们讨论了节点本地的事务处理。

​	**单分区事务可以使用我们在第5章中讨论过的悲观(基于锁或跟踪)或乐观(尝试并验证)并发控制方案，但是这些方法无法解决多分区事务的问题，因为多分区事务需要实现不同服务器之间的协调、分布式提交和回滚协议**。

​	一般来说，从一个账户向另一个账户转账时，你希望同时完成扣减第一个账户余额和增加第二个账户余额这两个操作。但是，如果我们将事务分解为若干独立步骤，即便是扣减或增加余额操作乍一看也不是原子的：我们需要读取旧的余额，加上或减去所需的金额，然后保存该结果。这些子步骤中的每一步又涉及多个操作：节点收到请求、解析请求、在磁盘上找到数据、进行写操作、最终确认请求。即便如此也仍是一个相当高层的视角：哪怕执行一个简单的写入，我们也要做上百个小步骤。

​	这就意味着我们必须先执行事务，然后再让结果可见。在此之前，我们需要先定义什么是事务。事务是一组操作，一个原子的执行单元。事务的原子性意味着：要么它的全部结果都变得可见，要么全都不可见。例如，如果我们在一个事务内修改了几行甚至几张表，要么所有修改都被应用了，要么全都不被应用。

​	<u>为了保证原子性，事务必须是可恢复的。换句话说，如果事务无法完成、被中止或超时，则其结果必须完全回滚</u>。一个不可恢复的、部分执行的事务会让数据库处于不一致状态。总而言之，在事务执行失败的情况下，必须将数据库恢复到之前的状态，就像从未做过该事务一样。

​	**另一个重要的方面是网络分区和节点故障：系统中的节点独立地发生故障并恢复，但是它们的状态必须保持一致**。这意味着原子性要求不仅适用于本地操作，还适用于在其他节点上执行的操作：事务的修改必须要么持久化地传播到事务涉及的所有节点，要么个都不传播[LAMPSON79]。

## 13.1 多个操作的原子性

​	为了让多个操作看起来是原子的，尤其是当其中一些是远程操作时，我们需要使用一类称为**原子提交(atomic commitment)**的算法。**<u>原子提交不允许参与者之间出现分歧只要有一个参与者投票反对，事务就不能提交</u>**。同时，这意味着发生故障的进程也必须与其他参与者达成共识。它的另一个重要含义是，如果存在拜占庭故障，原子提交算法无法正常工作：这时进程可能会撒谎或是决定一个任意值，而这会破坏全体的共识[HADZILACOS05]。

​	原子提交试图解决的问题就是要对以下问题达成共识：是否要执行当前被提议的事务？事务参与者不能选择、影响或修改被提议的事务，更不能提出另一个事务，它们只能对自己是否愿意执行此事务进行投票[ROBINSON08]。

​	原子提交算法没有对准备(prepare)、提交(commit)和回滚(rollback)这些操作做出严格的语义限制，数据库开发者需要决定：

+ 何时可以认为数据已准备好提交，这时只要交换一个指针便可以让修改对外可见。
+ 如何执行提交本身，以让事务结果在最短的时间内变得可见。
+ 如果算法决定不提交，如何回滚事务所做的更改。

​	第5章中，我们讨论了节点本地上的事务实现。

​	许多分布式系统使用原子提交算法，例如MySQL(用于分布式事务)和Kafka(用于生产者和消费者的交互[MEHTA17])。

​	在数据库中，分布式事务是由一个通常叫作<u>事务管理器</u>的组件执行的。事务管理器是负责调度、协调、执行和跟踪事务的子系统。在分布式环境中，事务管理器负责确保节点本地的可见性保证与分布式原子操作规定的可见性相符合。换句话说，<u>事务提交发生在所有分区以及所有副本之中</u>。

​	我们将讨论两种原子提交算法：**两阶段提交(它解决了提交的问题，但不允许协调者发生故障)**和**三阶段提交[SKEEN83] (它解决了非阻塞原子提交问题，即使协调者发生故障时，参与者也可以继续提交) **[BABAOGLU93]。

## * 13.2 两阶段提交

​	让我们从最简单的分布式提交协议开始，该协议能够保证多分区原子更新(关于分区的更多信息请参考13.6节)。两阶段提交(2PC)是数据库事务中的一个重要概念。

​	2PC分为两个阶段：

+ 第一阶段中分发决定的值并收集投票；
+ 第二阶段，节点仅仅翻转开关，让第一阶段的结果变得可见。

​	2PC假定存在一个**领导者(leader)**或**协调者(coordinator)**负责保存状态、收集投票，并作为协商的主要参考依据。其余节点称为**参与者(cohort)**。

​	<u>通常情况下，每个参与者负责一个分区(即互不相交的数据集合)，在这些数据上执行事务。协调者和每个参与者都会为所有执行过的步骤保留本地操作日志。参与者投票接受或拒绝协调者提议的某个值</u>。通常来说，这个值就是要执行的分布式事务的标识符，但是2PC也可以用在其他场景下。

​	协调者可以是接收事务请求的节点，可以是用选主算法随机选出来的，可以是手动分配的，甚至可以是在系统的整个生命周期内都保持固定的。协议本身对协调者角色没有任何限制。出于可靠性或性能的原因，可以将该角色转移给其他参与者。

​	顾名思义，两阶段提交的执行分为两个步骤：

+ **准备(prepare)阶段**

  协调者通过发送Propose消息告诉参与者关于新事务的提议。参与者要决定它们是否可以提交自己那部分事务。如果参与者决定可以提交，就向协调者投赞成票。否则，它会要求协调者中止事务。**所有参与者的决定都保留在协调者的日志中，并且每个参与者也会在本地保留一份它的决定**。

+ **提交(commit)/中止(abort)阶段**

  事务中的操作可以修改多个分区上的状态(每个参与者代表一个分区)。<u>但凡有一个参与者投票中止事务，则协调者也会向所有的参与者发送Abort消息</u>。仅当所有参与者都投赞成票时，协调者才会向他们发送最终的Commit消息。

​	准备阶段中，协调者分发提议的值，并收集各个参与者的投票：这个提议的值是否应该被提交。参与者可以选择拒绝协调者的提案，例如，当另一个冲突的事务已经提交了个不同值的时候。

​	协调者收集投票之后，可以决定要提交事务还是中止事务。如果所有参与者均投了赞成票，它会决定提交并发送Commit消息通知它们。否则，协调者会向各个参与者发送Abort消息，事务将被回滚。换句话说，如果任意一个节点拒绝该提案，则整个流程将会中止。

​	**<u>每个步骤中，协调者和参与者都必须将各个操作的结果写入持久性存储中，以便能够在发生本地故障的情况下重建状态并恢复，并且可以将结果转发给其他参与者使其能够重放操作</u>**。

​	**在数据库的语境下，每次2PC通常负责一个事务**。在准备阶段，事务内容(操作、标识符和其他元数据)从协调者发送给参与者。参与者在本地执行事务，并停留在部分提交状态(也称为预提交状态)，以让协调者在下一阶段通过提交或中止来完成执行。事务提交时，其内容已持久地存储到所有其他节点上[BERNSTEIN09]。

### 13.2.1 2PC中的参与者故障

​	让我们考虑几种故障的场景。例如，如果其中一个参与者在提议阶段故障，则协调者无法继续进行提交，因为它要求所有投票都是赞成票。如果其中一个参与者不可用，则协调者将中止事务。该要求会影响到可用性：单个节点的故障就会阻止事务提交。一些系统，例如Spanner(参见13.5节)，在Paxos组而不是单个节点上执行2PC，从而改进可用性。

​	**2PC的核心思想在于参与者的承诺：一旦对提案做出了肯定的回应，它不能再反悔，因此只有协调者才能中止事务**。

​	<u>如果其中一个参与者在接受提案后发生故障，它必须先了解投票的实际结果，然后才能知道正确的值，因为协调者可能由于其他参与者的投票而中止了提交。当参与者节点恢复时，它必须跟上协调者的最终决定。为此，我们通常将决策日志保留在协调者端，并将决定的值复制到故障的参与者。在此之前，参与者不能处理请求，因为它还处于不一致状态</u>。

​	由于协议中存在多个阻塞操作，此时进程要等待其他参与者(当协调者收集投票时，或参与者正在等待提交/中止阶段时)，因此，链路故障可能导致消息丢失，进程会无限地等待下去。如果提议阶段中协调者未收到来自某个副本的响应，它不会因此阻塞，因为它可以触发超时并中止事务。

### 13.2.2 2PC中的协调者故障

​	第二阶段中，如果某个参与者没有收到协调者的提交或中止指令，则应当尝试找出协调者做出的决定。<u>协调者可能已经决定了该值，但未能传达给某个副本。在这种情况下，可以从其他参与者的事务日志或是备份协调者的日志中找到决策信息</u>。**从复制日志中确定提交决定是安全的，因为这个决定一定是已经达成一致的：2PC的最终目的就是在所有节点上提交或中止，在一个参与者中提交即意味着其他所有参与者都必须提交**。

​	第一阶段中，协调者收集投票，继而也就获得了参与者的承诺：它们将等待其明确的提交或中止指令。如果协调者在收集投票后、广播投票结果之前发生故障，则参与者最终将处于不确定状态。参与者不知道协调者的决定，也不知道它是否已向某些参与者(可能也不可达)通知了事务结果[BERNSTEIN87]。

​	协调者无法继续进行提交或中止操作，这使得集群处于**未决状态**。这意味着在发生协调者永久性故障的情况下，参与者将无法得知最终决定。因为这个特性，我们说**2PC是一种阻塞原子提交算法**。<u>如果协调者始终无法恢复，它的替代者只能再次为给定事务收集投票，并做出最终决定</u>。

​	**许多数据库使用2PC：MySQL、 PostgreSQL、 MongoDB等。因为其简单性(易于论证、实现和调试)和低开销(消息复杂度和协议往返次数低)，两阶段提交经常用于实现分布式事务**。实现正确的恢复机制并拥有备份协调者节点，以减少发生上述故障的可能性，这一点非常重要。

## * 13.3 三阶段提交

​	为了让原子提交协议在协调者故障下保持健壮并避免进入**未决状态**，三阶段提交(3PC)协议增加了一个额外的步骤，并且**双方**都具有<u>超时机制</u>，使得参与者在协调者发生故障时仍能继续提交或中止(取决于系统状态)。3PC假定同步网络模型，且不存在通信故障[BABAOGLU93]。

​	**3PC在提交/中止步骤之前添加了一个准备阶段，该阶段中协调者告知参与者在提议阶段收集的投票信息，即使协调者发生故障，协议也可以继续执行**。

​	3PC的所有其他性质都和2PC类似，包括**要求有一个协调者**。

​	3PC的另一个有用的补充是<u>参与者侧的超时，参与者根据进程当前正在执行的步骤，超时之时将强制进行提交或中止</u>。

​	三阶段提交包含以下三个步骤：

+ 提议(propose)阶段

  协调者发出提议值并收集投票。

+ 准备(prepare)阶段

  协调者将投票结果通知参与者。如果投票通过并且所有参与者都决定要提交，则协调者会发送一条Prepare消息，指示它们准备提交。否则，将发送Abort消息并退出流程。

+ 提交(commit)阶段

  协调者通知参与者提交事务。

​	在提议这一步中，类似于2PC，协调者分发提议值并收集参与者的投票。<u>如果协调者在此阶段崩溃导致操作超时，或者其中一个参与者投反对票，则事务中止</u>。

​	收集投票后，协调者做出决定。如果协调者决定继续进行事务，则发出Prepare指令。<u>协调者可能无法将 Prepare消息发送给所有参与者，或是可能没有收到参与者的确认这种情况下，参与者可能会在超时后中止事务，因为算法尚未完全进入到已准备状态</u>。

​	**一旦所有参与者成功进入已准备状态并且协调者收到了它们的准备确认，无论其中任何方发生故障，事务都将被提交**。<u>之所以可以这样做是因为此阶段中所有参与者看到的状态是相同的</u>。

​	<u>在提交阶段，协调者将准备阶段的结果传达给所有参与者，**重置它们的超时计数器并完成事务**</u>。

**3PC中的协调者故障**

​	所有的状态转换都要被协调，直到每个节点都完成上一个阶段之后，参与者才能继续进入下一个阶段：协调者必须等待副本才能继续。如果参与者在超时之前没有收到协调者的消息，并且没有走完准备阶段，它们可以中止事务。

​	正如我们之前讨论的，<u>2PC无法从协调者故障中恢复，并且参与者可能卡在未决状态，直到协调者恢复为止。3PC避免了在这种情况下阻塞流程，并允许参与者以一个确定性的决策继续执行</u>。

​	**3PC的最坏场景是网络分区**。<u>一些节点已成功进入准备状态，在超时后将会继续进行提交。有些节点无法与协调者通信，因此会在超时后中止。这导致了**脑裂**：根据协议，**一些节点会继续进行提交，另一些节点会中止，使得各个参与者处于不致且矛盾的状态**</u>。

​	<u>**虽然从理论上说3PC确实在某种程度上可以解决2PC的阻塞问题，但却带来了更大的消息开销与潜在的不一致性，并且在出现网络分区的情况下无法很好地工作**。这或许是3PC未被广泛使用的主要原因</u>。

## * 13.4 Calvin分布式事务

> [Calvin：一个为分区数据库设计的快速分布式事务框架 - 简书 (jianshu.com)](https://www.jianshu.com/p/43909447728f)

​	我们已经了解了同步的代价以及相关的几种事务方法。但是，还有其他方法可以减少争用并减少事务持有锁的总时间。<u>一种方法是让各副本在获取锁并继续执行之前，就执行顺序和事务边界达成一致。如果我们能够做到这一点，节点故障就不会导致事务中止，因为节点可以从并行执行同一事务的其他参与者中恢复状态</u>。

​	传统数据库使用<u>两阶段锁</u>或<u>乐观并发控制</u>来执行事务，并没有确定的事务顺序。这意味着必须协调各节点以保证事务顺序。而**确定性事务顺序消除了执行阶段的协调开销，因为所有<u>副本获得相同的输入</u>，所以也会得到等同的输出**。这种方法通常称为Calvin——一种快速的分布式事务协议[THOMSON12]。使用 Calvin实现分布式事务的一个著名例子是FaunaDB。

​	**为了获得确定性顺序， Calvin使用定序器(sequencer)，它是所有事务的入口点**。<u>定序器决定事务的执行顺序，并**建立全局事务输入序列**</u>。为了最大限度地减少竞争和批量决策， Calvin将时间线切分成*epoch*。定序器收集事务并将其分组到短时间窗口内(原论文使用10毫秒的窗口)，这些窗口也就成为复制单元，因此不需要为每个事务单独进行通信。

​	<u>当一个事务批被成功复制之后，**定序器**会将其转发给**调度器(scheduler)**，它负责编排事务的执行</u>。调度器使用确定性调度协议，可以并行执行部分事务，并同时保留定序器指定的串行执行顺序。将事务应用于特定状态只会产生由事务指定的更改，并且事务顺序是预先确定的，因此，各副本无须与定序器做更多的通信。

​	<u>Calvin中的每个事务具有**读取集**(事务执行的依赖，即执行事务所需的当前状态的数据记录集合)和**写入集**(事务执行的结果，也就是它的副作用)</u>。 Calvin本身不支持事务依赖于额外的读取来决定读取集和写入集。

​	Calvin的工作者线程(由调度器管理)的执行过程分为四个步骤：

1. 分析事务的**读取集**和**写入集**，用读取集确定节点本地的数据记录，并创建活跃参与者的列表(即包含写入集元素且将对这些数据进行修改的参与者)。
2. 收集执行事务所需的本地数据，换句话说，收集恰好位于该节点上的读取集记录。收集到的数据记录将被转发给相应的活跃参与者。
3. 如果当前工作线程正在一个活跃参与者节点上执行，它将接收到其他参与者发来的数据记录，对应于步骤2中执行的操作。
4. 最后，执行一批事务，**将结果持久化到本地存储中**。<u>**不用将执行结果转发到其他节点，因为它们接收相同的事务输入，可以自己在本地执行并持久化结果**</u>。

​	一个典型Calvin的实现将定序器、调度器、工作者和存储子系统放在一起。<u>为了确保各个定序器就当前epoch/批包含哪些事务达成共识， Calvin使用Paxos共识算法(参见14.3节)或异步复制(其中某个专用副本作为领导者)。尽管使用**领导者**可以改善延迟，但节点必须重建故障领导者的状态才能继续，因此也带来了更高的恢复成本</u>。

![img](https://upload-images.jianshu.io/upload_images/2224-18eb577c9db44e26.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp)

## * 13.5 Spanner分布式事务

> [Paxos算法详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/31780743)
>
> [简单解释Spanner的TrueTime在分布式事务中的作用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/44254954)
>
> [分布式系统中的时间和顺序——关于Spanner中的Linearizability_高可用架构的博客-CSDN博客](https://blog.csdn.net/weixin_45583158/article/details/100143234)

​	Calvin常常被拿来和另一个称为Spanner[CORBETT12]的分布式事务方法进行对比。

​	Spanner的实现(或派生)包括几个开源数据库，最著名的是CockroachDB和YugaByteDB。

+ **Calvin通过在定序器上达成共识来建立全局事务执行顺序。**

+ **Spanner则在每个分区的共识组(换句话说，每个分片)上使用两阶段提交。** 

​	Spanner的架构相当复杂，本书中我们仅介绍一些高层次上的细节。

​	<u>**为了实现一致性并建立事务的顺序关系， Spanner使用*True Time*：一种高精度的物理时钟API，同时也暴露时钟误差的范围，允许本地操作中引入人为的减速以等待时钟误差范围过去**</u>。

​	Spanner提供三种主要的事务类型：**读写事务**、**只读事务**和**快照读**。

+ **读写事务**<u>需要加锁</u>，使用<u>悲观的并发控制</u>，并且<u>存在**领导者(leader)**副本</u>。
+ **只读事务**是<u>无锁</u>的，<u>可以在任何副本上执行</u>。只有在读取最新时间戳时才需要领导者，领导者负责从Paxos组中获取最后提交的值。对特定时间戳的读取是一致的，因为数据有版本，而快照的内容一旦写入就无法更改。<u>每个数据记录都会被分配一个时间戳，记录了写入该值的事务的提交时间。这也意味着每条记录可能存在多个时间戳的版本</u>。

![0?wx_fmt=png](https://img-blog.csdnimg.cn/img_convert/9e67310d8f719838391ce0aacf417dd6.png)

​	<u>Spanner的架构：**每个spanserver(副本，向客户端提供数据的服务器实例)包含多个tablet，每个 tablet对应一个Paxos(参见14.3节)状态机**。副本被分组为副本集，称为 Paxos组，这是数据放置和复制的单元。每个 Paxos组都有一个长期的领导者(参见14.3.4节)。在处理跨分片的事务时，领导者会相互通信</u>。

​	<u>每个写入操作必须通过Paxos组的领导者，而读取可以直接在最新副本的tablet上进行</u>。领导者上有**锁表(lock table)**和**事务管理器**，锁表用于使用两阶段锁(参见5.3.8节)机制来实现并发控制，事务管理器负责跨分片的分布式事务。**需要同步的操作(例如事务内的写入和读取)必须先从锁表中获取锁，而其他操作(快照读)可以直接访问数据**。

​	<u>对于跨分片的事务，Paxos组领导者必须协调并执行两阶段提交以保证一致性，并使用**两阶段锁**保证隔离性</u>。2PC算法要求所有参与者都存活才能成功提交，因而可能会损害可用性。而<u>Spanner使用Paxos组代替单个节点作为参与者，解决了这一问题</u>。这意味着即使组内的某些成员宕机，2PC也可以继续运行。**在Paxos组内部，只有领导者节点会参与2PC**。

​	**Paxos组用于在多个节点之间一致地复制事务管理器的状态**。

+ <u>在执行事务时，**Paxos组的领导者首先获取写锁**，并选择一个写入时间戳——该时间戳必须比之前任何事务的时间戳要大，并通过Paxos记录一条2PC的prepare日志</u>。
+ 事务协调者收集时间戳，随后生成一个大于任何准备时间戳(prepare timestamp)的提交时间戳(commit timestamp)并通过Paxos记录一条commit日志。然后，**事务协调者需要等待直到提交时间戳过后，因为它必须保证客户端只能看到时间戳已经过去的事务结果**。之后，它将此时间戳发送给客户端和各个领导者，领导者将一条commit日志连同新的时间戳一同记录到其所在的Paxos组中，此时便可以释放锁。

​	<u>**单分片事务**无须与事务管理器通信(并且之后也不必进行跨分区的两阶段提交)，因为**查询Paxos组和锁表足以保证事务顺序和分片内部的一致性**</u>。

​	Spanner读写事务提供了称为**外部一致性(external consistency)**的序列化顺序：事务时间戳反映了序列化顺序，即使在分布式事务中也是如此。**外部一致性具有与可线性化(linearizability)等效的实时属性：如果事务T1在T2开始之前提交，则T1的时间戳小于T2的时间戳**。

​	**<u>总结一下， Spanner使用Paxos进行一致的事务日志复制，使用两阶段提交进行跨分片事务，使用TrueTime进行确定性事务排序</u>**。

​	<u>相比Calvin[ABADI17]， Spanner跨分区事务的成本更高，因为多出了一个两阶段提交的过程</u>。理解这两种方法都很重要，依靠它们，我们能够在分区的分布式数据存储中执行事务。

## * 13.6 数据库分区

> [一致性哈希算法原理 - lpfuture - 博客园 (cnblogs.com)](https://www.cnblogs.com/lpfuture/p/5796398.html)
>
> [聊聊一致性哈希 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/24440059)
>
> [B站面试挺难的，现场手写负载均衡—一致性哈希环算法_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1bP4y157pH)

​	讨论Spanner和Calvin时，我们经常用到分区(partitioning)这个词。现在我们来更详细地讨论一下它。对于大多数现代应用程序而言，将所有数据存储在单个节点上是不现实的，因此许多数据库都使用分区：逻辑上将数据分成较小的、易于管理的段。

​	**数据分区最直接的方法是将数据划分成多个范围，并允许每个副本集(replica set)只管理特定的范围(分区)**。执行査询时，客户端(或査询协调者)需要基于路由键将读写请求路由到正确的副本集。<u>这种分区方案通常称为分片(sharding)：每个副本集作为数据某个子集的单个来源</u>。

​	<u>**为了最有效地使用分区，必须考虑负载和值的分布并据此确定分区大小**。这意味着可以将读写负担较重的范围分裂成更小的分区，从而分散负载。同时，如果某些范围包含的值比其他范围更密集，最好也将它们分裂成更小的分区</u>。例如，假如我们选择邮编作为路由键，由于人口分布并不均匀，一些邮编范围可能分配到更多的数据(比如人或订单)。

​	**当集群添加或删除节点时，数据库必须重新分区数据以保持均衡**。<u>为了保证数据迁移过程的一致性，我们应当在更新集群元数据及开始将请求路由到新的位置目标之前先搬运数据</u>。一些数据库可以进行自动分片(auto-sharding)，使用算法来决定最佳分区方式，并重新放置数据。这些算法通常基于各分片的读取和写入负载以及数据量等信息来进行决策。

​	<u>为了从路由键找到目标节点，一些数据库计算路由键的哈希值，并通过某种方式将哈希值映射到节点ID</u>。使用哈希确定副本位置的一个优点是能够减少范围热点，因为哈希值与原始值的排列顺序不同。两个字典序接近的路由键原本会被放在同一副本集上，但如果用哈希值，则会被放在不同副本集上。

​	将哈希值映射到节点ID的最直接的方法是将哈希值除以集群大小再取其余数(取模)如果系统中有N个节点，则目标节点ID可以通过`hash(v) mod N`算出。该方法的主要问题是，每当添加或删除节点、集群大小从N更改为N'时，`hash(v) mod N'`算出的很多值都和原来不同。这意味着大部分数据都要被移动。

**一致性哈希**

​	为了缓解该问题，一些数据库(例如 Apache Cassandra和Riak)使用一种叫作**一致性哈希(consistent hashing)**的分区方案。如前所述，<u>路由键的值被输入哈希函数，返回的哈希值被映射到一个环上，以便在超过最大值之后回卷到最小值。每个节点在环上拥有自己的位置，负责前一个节点到当前节点之间的值范围</u>。

​	<u>使用一致性哈希能减少维持数据均衡所需的移动次数：**节点离开或加入只会影响到环上与该节点直接相邻的节点，而不影响整个集群**</u>。

​	<u>定义中的一致性表示：**当哈希表大小发生变化时，如果我们有K个可能的哈希键和n个节点，平均而言，我们仅需移动K/n个键**。也就是说，**一致性哈希函数的输出受函数范围变化的影响最小**</u>[KARGER97]。

## * 13.7 Percolator分布式事务

> [漫谈Google Percolator分布式事务 - 简书 (jianshu.com)](https://www.jianshu.com/p/c39e64d6c5da)

​	回到分布式事务上来，由于允许一些读取和写入异常，隔离级别可能很难论证。<u>如果应用程序不要求可串行化，避免写入异常的一种方法是使用SQL-92中描述的快照隔离(Snapshot Isolation，SI)事务模型</u>。

​	**快照隔离保证事务内的所有读取结果与数据库的某个快照一致**。快照中包含了在事务的开始时间戳之前提交的所有值。如果存在**写-写冲突**(即，两个并发执行的事务尝试写入同一单元格)，那么只有一个能够提交成功。这种策略通常称为**首个提交者胜利(first committer wins)**。

​	**快照隔离能避免读偏斜(read skew)——一种在读已提交隔离级别下允许的异常**。例如，x、y之和应该等于100。事务T1执行操作read(x)读到值70。T2更新两个值write(x，50)和 write(y，50)并提交。如果T1尝试运行read(y)，并根据T2新提交的y的值(50)继续执行事务，将会导致不一致。T1在T2提交之前读到的值x与新的y值彼此不一致。而快照隔离仅会让小于某个特定的时间戳的值对事务可见，新的y值对T1不可见[BERENSON95]。

​	快照隔离有几个实用的特性：

+ **它只允许对已提交数据的可重复读取**。
+ 值是一致的，因为它们是从某个时间戳的快照中读取的。

+ **<u>冲突的写入将被中止并重试，以防止产生不一致</u>**。

​	<u>尽管如此，快照隔离下的操作历史不是可串行化的。由于只有对相同单元格的冲突写入会被中止，因此仍可能发生**写偏斜(write skew)**(参见5.3.3节)</u>。写偏斜发生在两个事务修改的值集合不相交时，每个事务本身的写入都不违反约束。两个事务都能提交，但两个事务的写入合在一起就会违反这些约束。

​	<u>快照隔离提供的语义可以满足许多应用程序的需求，其主要优势在于读的效率较高，因为**不需要加锁——快照数据是无法修改的**</u>。

​	Percolator是一个在分布式数据库Bigtable(参见1.3.4节)上实现事务API的库。这是在现有系统之上构建事务API的一个很好的例子。Percolator用不同的列保存数据记录、已提交的数据点位置(写入元数据)和锁信息。为了避免竞争条件，它使用Bigtable提供的条件修改API，该API允许在单个远程调用中执行读-修改-写操作。

​	<u>每个事务必须和**授时节点(timestamp oracle)**通信两次(授时节点负责为整个集群提供单调递增的时间戳)：一次获取事务开始时间戳，另一次是在提交过程中。写入会先被缓存起来，最后由客户端驱动进行两阶段提交(请参阅13.2节)</u>。

+ **由于客户端在提交事务时可能会发生故障，因此我们需要确保进行到一半的事务能被提交或回滚**。
+ 如果之后的事务遇到一个不完整的状态，则应尝试释放主锁并提交事务。
+ 如果主锁已经释放，那么事务内容必须被提交。
+ **<u>同一时刻只可以有一个事务持有某个锁，而且所有状态转换都是原子的，因此不存在两个事务都在尝试修改内容的情况</u>**。

​	<u>快照隔离是一种很重要也很有用的抽象，经常用在事务处理中。它简化了语义，排除了一些异常情况，并为改善并发性和性能提供了机会，因此许多MVCC系统都提供这种隔离级别</u>。

​	一个基于Percolator模型的数据库的例子是TiDB(Ti代表钛元素)。TiDB是一个强致、高可用、可水平扩展的开源数据库，兼容MYSQL。

## * 13.8 协调避免

​	<u>讨论可串行化性的成本并尝试减少协调量，同时仍提供强大的一致性保证的另一个例子是协调避免</u>[BAILIS14b]。在保持数据完整性约束的前提下，只要操作满足约束融合性，协调是可以避免的。**约束融合性(Invariant confluence或*I*-Confluence)定义为这样一种属性：两个满足约束但存在分歧的数据库状态一定能够合并为一个有效的最终状态。这里说的约束对应ACID中的一致性**。

​	<u>由于可以将任何两个有效状态合并为一个有效状态，因此满足约束融合性的操作不需额外协调就可以直接执行，这显著地提高了性能和可扩展性</u>。

​	**为了保持约束，除了定义修改数据库的操作之外，我们还必须定义一个合并(merge)函数，它接受两个状态作为输入。当状态被独立地修改时，可以用合并函数将分歧的状态重新收敛**。

​	**<u>事务在本地的数据库版本(快照)上执行</u>**。<u>如果事务执行需要用到其他分区中的某些状态，就把该状态也放到本地。事务提交时，产生的本地快照的改动会被迁移并和其他节点的快照合并</u>。

​	允许协调避免的系统模型必须保证以下性质：

+ **全局有效性**

  无论对于合并后的状态，还是刚提交的存在分歧的状态，所需的约束条件始终是满足的，事务不会观察到无效的状态。

+ **可用性**

  <u>如果所有包含状态的节点对客户端都是可达的，那么事务必然会成功提交，除非提交将违反某个事务约束，这种情况下事务会中止</u>。

+ **收敛**

  节点可以独立维护其本地状态，但如果之后没有新的事务并且网络分区不会无限持续下去，它们一定能够达到相同的状态。

+ **不需协调**

  <u>本地事务的执行独立于为其他节点执行的对本地状态的操作</u>。

​	一个实现协调避免的例子是**读原子性多分区(Read-Atomic Multi Partition，RAMP)事务**[BAILIS14c]。<u>RAMP使用**多版本并发控制**和当前进行中操作的元数据来从其他节点获取缺失的状态更新，从而允许读取和写入操作能够并发进行</u>。

​	例如，如果读取操作与某些修改相同的条目的写入操作存在重叠，这种情况可以被检测出来，必要时，还可以通过一轮额外的通信，利用正在进行的写入操作的元数据获取所需的信息来对其进行修复。

​	在分布式环境下使用基于锁的方法可能不是个好主意，相反，RAMP提供以下两个性质：

+ **同步独立性**

  <u>一个客户端的事务不会阻塞、中止或是强迫另一个客户端的事务等待</u>。

+ **分区独立性**

  <u>客户端无须联系那些事务不涉及的分区</u>。

​	<u>RAMP引入了**读原子性(read atomic)隔离级别**：事务不会观察到来自进行中的、未提交的或中止事务的正在进行的状态变更</u>。换句话说，**事务更新要么全都对并发事务可见，要么全都不可见**。<u>根据该定义，读原子性隔离级别还排除了**断裂读(fracturedread)**：即事务仅观察到其他事务执行的写入的一个子集</u>。

​	<u>**RAMP提供原子的写可见性，而不需要互斥**——其他解决方案(例如分布式锁)往往将这两者耦合在一起</u>。

​	**这意味着事务不会相互阻塞RAMP分发事务元数据，这些元数据允许读取操作检测到并发进行的写入**。通过元数据，事务可以检测到较新的记录版本，找到并获取最新的记录版本，然后对它们进行操作。

​	<u>为了避免协调，所有本地的提交决定也必须在全局范围内生效。在RAMP中，这是通过以下要求解决的：**当某个分区中的写入变为可见时，此事务中所有其他分区的写入也必须可见**。</u>

​	<u>为了使得读写操作不阻塞其他并发的读写操作，同时在本地和整个系统范围内(事务修改过的所有其他分区中)都保持读原子性隔离级别，RAMP使用**两阶段提交**来处理写入操作</u>：

+ 准备阶段

  第一阶段准备写操作并将其放到相应的目标分区中，**但不使其对外可见**。

+ 提交/中止阶段

  第二阶段将事务的写操作进行的状态更改变得可见，**使其在所有分区上原子地变得可用，或者回滚更改**。

​	<u>**RAMP允许一个记录同时存在多个版本**：最新值、进行中的未提交更改，以及被后来的事务覆盖掉的过期版本。**之所以需要保留过期版本，仅仅是为了正在进行中的读取请求。只要所有并发的读操作结束，就可以丟弃过期的值**</u>。

​	为了防止、检测、避免并发操作之间的冲突，往往需要引入协调开销，因此，想让分布式事务高效且可扩展并不容易。系统越大，承载的事务越多，产生的协调开销也就越大。本节中描述的方法<u>尝试用约束条件来确定可以避免协调的地方，从而减少协调的量，只在绝对必要时才付出全部的代价</u>。

## 13.9 本章小结

​	本章中，我们讨论了实现分布式事务的几种方法。首先，我们讨论了两种**原子提交算法**：**两阶段提交**和三阶段提交。这两种算法的最大优点是易于理解和实现，但也有一些缺点。

+ 在2PC中，协调者(或至少是它的代替者)必须在整个提交过程中都存活，这会大大降低可用性。
+ 3PC在一些情况下取消了这个要求，但在网络分区的情况下可能发生**脑裂**。

​	现代数据库系统中的分布式事务通常使用共识算法来实现，我们将在第14章中进行讨论。例如，本章讨论的 Calvin和Spanner都使用Paxos。

​	<u>共识算法比原子提交算法更复杂，但是具有更好的容错性，并且**将决策结果与发起者分离开来，允许参与者决定一个值，而不是决定是否接受那个值**</u>[GRAY04]。

# 14. 共识

​	我们已经讨论了许多分布式系统的概念，从最基础的链路、进程、分布式计算的问题等基础知识开始，到故障模型、故障检测和领导者选举，最后讨论了一致性模型。我们终于准备好将所有这些知识融会贯通，探究分布式系统硏究的巔峰：分布式共识(consensus)。

​	分布式系统中的共识算法允许多个进程就某个值达成共识。**<u>FLP不可能定理(参见8.5节)表明，对于完全异步系统，不可能确保在有限时间达成共识</u>**。<u>即使消息传递是可靠的，进程也无法得知另一进程是崩溃了还是运行缓慢</u>。

​	第9章中，我们讨论了故障检测的准确性和速度之间的权衡。**共识算法采用异步模型并确保安全性，而外部故障检测器可以提供有关其他进程的信息，从而确保算法的活动性[CHANDRA96]**。

​	<u>由于故障检测并非完全准确，因此可能会出现这样的情况：共识算法等待进程故障被检测到，或者由于故障检测器错误地怀疑某个进程有故障而导致算法重启</u>。

​	<u>即使一些进程发生崩溃，进程也需要就某个参与者提出的某个值达成共识。如果一个进程没有崩溃并可以持续地执行算法步骤，我们说该进程是*正确*的</u>。**共识常被用于将事件按特定顺序排列，并确保参与者之间的一致性**。使用共识，我们可以构建出这样的系统：一组进程从一个值转移到下一个值，而客户端观察到的值仍是确定性的。

​	理论上说，共识算法具有三个性质：

+ **一致性**

  所有正确进程决定的值都相同。

+ **有效性**

  决定的值是由其中一个进程提议的。

+ **终止性**

  所有正确进程最终都会做出决定。

​	上述性质中的每一个都很重要。一致性根植于人们对共识的理解中。词典中对“共识”的定义就是“一致的认识”。这意味着**一旦达成共识，任何进程都不可以对结果有不同的意见**。你可以把它想成和朋友们约定在某时某地见面：你们所有人都想见面，只是需要对活动的具体细节达成一致。

​	有效性也是必不可少的。若没有这一条，共识可能变得没有意义。共识算法要求所有进程就某个值达成共识，如果进程对于任何提议的值，都使用某个预先决定的默认值作为决定输出，它们的确会达成一致，但是这样的算法输出是无效的，在实际中毫无用处。

​	**如果没有终止性，算法将会永远运行下去而不产生任何结论，或者将无限期地等待崩溃的进程恢复，这样的算法也没什么用**。<u>进程最终必须达成共识，并且，要想使算法具有实用性，共识必须相当快地完成</u>。

## 14.1 广播

​	广播(broadcast)是分布式系统中常用的通信抽象。广播算法用于在一组进程间传播信息。有许多种广播算法，它们基于不同的假设，并提供不同的保证。广播是一个重要的原语，它被用在包括共识算法在内的很多地方。我们已经讨论了广播的一种形式——Gossip(参见12.6节)。

​	<u>广播通常用于数据库复制：协调者节点将数据分发给所有其他参与者。</u>但是，想做到可靠的分发并不是件容易的事情：如果协调者在分发消息给一部分节点后就崩溃了，系统将会处于不一致的状态：有些节点观察到一条新消息，有些则没有。

​	广播消息最简单直接的方法是**尽力而为广播(best effort broadcast)**[ CACHIN1l]。这种情况下，由发送方负责确保将消息传递给所有目标。如果失败，其他参与者不会尝试重新广播该消息；如果协调者崩溃，这种广播将静默地失败。

​	为了确保广播的可靠性，即使发送方在传输过程中崩溃，也需要确保所有正确进程都收到相同的消息。

​	<u>我们可以通过故障检测和回退机制实现一个朴素版本的可靠广播。最简单的回退机制是允许每个接收消息的进程将消息转发给它知道的所有其他进程</u>。当源进程发生故障时其他进程将检测到故障并继续广播消息，用N<sup>2</sup>条消息洪泛(flooding)整个网络。即使发送方崩溃，消息仍可被其他节点接收和传递，从而提高了可靠性，并让所有接收者看到相同的消息[CACHIN11]。

​	该方法的一个缺点是它用到了N<sup>2</sup>条消息，其中N是剩余接收者的数量(因为每个广播进程都排除源进程及自己)。理想状态下，我们希望尽可能减少可靠广播所需的消息数量。

## 14.2 原子广播

​	刚刚描述的洪泛算法虽然能确保将消息送达，但不能保证送达的顺序：消息最终会在某个未知的时刻送达。<u>如果我们需要按顺序传递消息，则需要使用**原子广播(atomic broadcast)**，也称为**全序多播(total order multicast)**，它能保证**可靠传递**和**全序性**</u>。

​	<u>可靠广播能确保各个进程对传递的消息达成一致，而原子广播还可以确保它们对消息的顺序也达成一致(即对每个接收者来说，消息传递的顺序都相同)</u>。

​	总结一下，原子广播必须保证两个基本性质：

+ **原子性**

  各个进程必须就接收到的消息集合达成一致。所有无故障的进程要么全都收到了消息，要么全都没收到。

+ **有序性**

  所有无故障的进程均以相同的顺序收到消息。

​	因此，这里消息的传递是原子的：每条消息要么传递给了所有进程，要么不传递给任何进程；如果消息被传递了，那么其他消息要么位于其之前，要么位于其之后。

### 14.2.1 虚同步

​	<u>其中一种用广播进行群组通信的框架称为虛同步(virtual synchrony)</u>。

​	原子广播将完全有序的消息传递给一组静态的进程，而**虚同步则将完全有序的消息传递给一组动态的进程**。

+ **虛同步将进程分成组。只要组存在，消息就会以相同的顺序传递给组内所有成员**。
+ **在虚同步中，模型本身并未指定顺序，只要它提供的顺序对所有成员一致即可**，某些实现利用这一点来提高性能[BIRMAN10]。

+ 各进程具有相同的组视图，<u>每条消息都关联了一个组标识：只要是同一个组的进程，就能看到相同的消息</u>。

+ <u>当参与者加入、离开组或发生故障被迫退出组时，组视图就会改变。组视图的变更会被通知给所有成员</u>。每个消息都唯一地关联了其发送者所属的组。

​	**虚同步区分消息的接收(某一个组成员接收到消息时)和送达(所有组成员都接收到消息时)**。<u>如果消息是从某个视图发送的，则只能在同一视图中送达，这可以通过比较当前组与消息关联的组来确定</u>。**收到的消息在队列中保持等待状态，直到通知该进程消息已成功送达**。

​	由于每个消息都属于某个特定的组，除非该组中的所有进程都在视图变更之前接收到该消息，否则组成员不能认为此消息已送达。<u>这意味着所有消息都是在视图变更*之间*发送并送达的，这为我们提供了原子传递的保证</u>。这种情况下，组视图变更就像是消息广播无法通过的屏障。

​	**一些全序广播算法使用定序器(一个负责决定消息顺序的单独进程)对消息进行排序**。<u>这样的算法可能更易于实现，但依赖于对领导者故障的检测以保证活动性。使用定序器可以提高性能，因为我们无须为毎条消息在各进程之间建立共识，而是使用定序器本地的视图。这种方法也可以通过对请求进行分区来扩展</u>。

​	**尽管技术上很可靠，但虚同步并未被广泛采用，在商业系统中也很罕见**[BIRMAN06]。

### * 14.2.2 Zookeeper原子广播

> [ZAB协议 - 技术-刘腾飞 - 博客园 (cnblogs.com)](https://www.cnblogs.com/frankltf/p/10392151.html)
>
> [Apache ZooKeeper - Wikipedia](https://en.wikipedia.org/wiki/Apache_ZooKeeper)

​	原子广播最流行、最广为人知的一个实现是Apache Zookeeper[HUNT10， JUNQUEIRA11]使用的Zookeeper原子广播(ZAB)算法。 Zookeeper是一个分层的分布式键值存储，它使用ZAB确保事件的<u>完全有序</u>和<u>原子送达</u>，以保证副本状态之间的一致性。

​	ZAB中进程有**领导者**和**跟随者(follower)**两种角色。

​	**领导者是个临时的角色，负责驱动整个算法流程、广播消息给跟随者并建立事件的顺序**。<u>当写入新记录或读取最新值时，客户端连接到集群的某一节点，如果该节点恰好是领导者，它将处理该请求，否则请求将被转发给领导者</u>。

​	**为了保证领导者的唯一性，协议时间线被分成*epoch*，通过唯一且单调递增的序列号标识。<u>毎个epoch中只能有一个领导者</u>**。

​	选举过程从选择一个潜在领导者开始，这可以通过任意的选举算法来实现，只要选出的进程有较大概率是存活的即可。由于算法安全性由后面的步骤保证，因此确定潜在领导者更多是性能优化。前任领导者发生故障也可能导致潜在领导者的出现。

​	一旦确定了潜在领导者，该算法分为三个阶段执行协议：

+ 发现阶段

  潜在领导者了解其他所有进程已知的最新epoch并提出一个新的epoch，<u>该epoch大于任意跟随者的当前 epoch</u>。跟随者收到该提议后，回复前一个epoch中看到过的最新事务标识符。<u>这一步之后，进程将不再接受更早epoch的广播提议</u>。

+ 同步阶段

  此阶段用于从前任领导者的故障中恢复，并让滞后的追随者更快地追上进度。潜在领导者向各个追随者发送一条消息，告知自己是新epoch的领导者，并收集它们的确认。一旦收到确认后，领导者就确立了。这一步之后，追随者不会再接受任何其他进程成为epoch领导者的提议。<u>同步阶段中，新领导者会确保各个跟随者拥有相同的历史记录，并将先前epoch领导者已提交的提案转发给跟随者。**当这些提案传递完之后才会开始发送新epoch的提案**</u>。

+ 广播阶段

  跟随者恢复同步之后，就会开始活动消息的传递。**在本阶段中，领导者接收客户端的消息，确定消息顺序，并广播给跟随者：领导者发出一个新的提议，等待Quorum的跟随者确认，最后提交**。<u>该过程类似于一个没有中止的两阶段提交：**投票就是确认，参与者不能投票拒绝有效领导者的提议**</u>。但是，错误epoch的领导者的提议不会被确认。广播阶段一直持续到领导者崩溃、与跟随者间发生网络分区，或者由于消息延迟而被当作崩溃为止。

​	只要保证跟随者只接受已确立epoch的领导者的提议，就能保证协议的安全性。即便可能会有两个进程同时尝试竞选，但其中只有一个能获胜并成为epoch领导者。此外，算法还假定进程都诚实地执行规定的步骤并遵循协议。

​	<u>领导者和跟随者都依靠**心跳机制**来确定远程进程的活动性</u>。**如果领导者未从Quorum的跟随者那里接收到心跳，它将不再担任领导者并重新发起选举。同样地，如果某个跟随者认为领导者已经崩溃，它将发起新的选举**。

​	**<u>消息是完全有序的，在上一条消息确认之前，领导者不会尝试发送下一条消息</u>**。即使某些消息被跟随者收到不止一次，但只要遵循顺序传递，重复应用消息也不会产生其他副作用。ZAB能处理多个并发的状态更改，因为唯一的领导者将会接收写入请求、建立事件的顺序并广播更改。

​	<u>消息完全有序也提高了ZAB的恢复效率。**在同步阶段，跟随者回复序列号最高的已提交提案。领导者可以简单地选择具有最高提案的节点用于恢复，这可以是唯一的需要从中复制消息的节点**</u>。

​	ZAB的一个优点是效率高：广播过程仅需两轮消息，而且领导者故障只需从一个最新的进程中获取缺失消息就可以恢复。<u>拥有长期领导者对性能是有利的：我们不需要额外的共识回合来确立一致的事件历史，因为领导者可以根据其本地视图决定事件顺序</u>。

## 14.3 Paxos

​	<u>**在可能出现崩溃故障的异步系统中，原子广播等价于共识问题**[CHANDRA96]，因为参与者必须能得知消息顺序并对其达成一致</u>。你将会看到原子广播和共识算法在目标和实现方面存在许多相似之处。

​	最广为人知的共识算法可能是Paxos，该算法最初由Leslie Lamport在"The Part-TimeParliament"这篇论文[LAMPORT98]中提出。论文中，作者借助Paxos岛的立法及投票过程阐述了共识问题。2001年，作者又发表了一篇名为"Paxos Made Simple"的后续论文[LAMPORT01]，该文章引入了一些更简单的术语，现在我们通常用这些术语来解释该算法。

​	Paxos的参与者有**提议者**、**接受者**和**学习者**三种角色：

+ 提议者(proposer)

  从客户端接收值，创建提案，并尝试从接受者收集投票。

+ 接受者(acceptor)

  <u>投票**接受**或**拒绝**提议者提议的值</u>。为了容错，算法要求存在多个接受者，但是为了算法的活动性，只要 Quorum(大多数)的接受者投票即可接受提案。

+ 学习者(learner)

  扮演**副本**的角色，保存被接受提案的结果。

​	**<u>任何参与者都可以扮演任何角色，大多数实现将多个角色放在一起：一个进程可以同时作为提议者、接受者和学习者</u>**。

​	毎个提案包含一个由客户端提出的值和一个唯一且单调递增的提案编号。这个提案编号之后还会被用于确保操作的全序性。提案编号通常用(id， timestamp)实现，其中节点ID也是可比较的，可用于防止时间戳冲突。

### * 14.3.1 Paxos算法

​	Paxos算法大体上可以分为两个阶段：**投票(或提议)阶段**和**复制阶段**。

+ **投票阶段，提议者竞争领导权**；
+ **复制阶段，提议者将值分发给接受者**。

​	<u>提议者是客户端最初联系的节点，它接收需要被决定的值，并尝试从Quorum的接受者那里收集投票。这一步完成后，接受者将决定出的值分发给学习者，批准这一结果。**学习者增加已达成共识的值的复制因子**</u>。

​	只有一名提议者可以收集到多数票。**某些情况下，多个提议者可能会平分票数，因此这轮投票中没有人能获得多数票，只能重新投票**。我们将在14.3.3节中讨论这一情形以及其他提议者竞争的情况。

​	在提议阶段中，提议者发送一条Prepare(n)消息(其中n是提议编号)给多数派接受者，并尝试收集它们的投票。

​	接受者收到准备请求后需要按以下要求做出回应[LLAMPORT01]：

+ 如果该接受者尚未回应过编号更高的准备请求，则它**承诺不再接受任何编号更低的提案。**
+ **如果该接受者已经接受了(收到 Accept !(m， V<sub>accepted</sub>)消息)任何其他提案，它将回复一条Promise(m， V<sub>accepted</sub>)消息，通知提议者它已经接受了编号为m的提案**。
+ 如果该接受者已经回应过更高编号的准备请求，则通知提议者存在编号更高的提案。
+ 接受者可以回应多个准备请求，只要后来接收到的请求具有更高的编号。

​	<u>复制阶段中，当收集到多数票之后，提议者可以开始复制过程：提议者向接受者发送条Accept !(n，ν)消息以提交这个提案，其中v为值，n为提案编号。v是在接受方收到的回复中编号最高的提案中的值，如果所有回复均不包含旧的、已接受的提案，那么v可以是提议者自己提出的任何值</u>。

​	接受者接受编号为n提案，除非在提议阶段它已经回应了Prepare(m)，其中m大于n。**如果接受者拒绝该提案，它会通知提议者它所看过的编号最高的提案，帮助提议者追赶上进度[LAMPORT01]**。

​	一旦就某个值达成共识(换而言之，该值被至少一个接受者接受)，未来的提议者必须决定出相同的值以保证一致性。这就是为什么接受者要回应它们接受过的最新值。如果接受者之前没有接受过任何值，那么提议者可以自己选择一个值。

​	**学习者要等接收到大多数接受者的通知之后才能知道已决定的值。为了让学习者尽快知道新值，接受者可以在新值被接受后立即通知它**。<u>如果学习者不止一个，每个接受者都需要通知每个学习者。可以将一个或多个学习者标记为负责人(distinguished learner)，由它来将接受的值通知给其他学习者</u>。

​	总结一下，算法第一阶段的目标是为本回合选出一个领导者，并了解要接受的值，从而使领导者可以继续进行第二阶段：广播该值。<u>对于基本算法来说，每当我们要确定一个值时，都必须执行这两个阶段。实践中，我们希望减少算法步数，因此可以允许提议者提出多个值</u>。我们之后会在14.3.4节中讨论这种做法的更多细节。

### * 14.3.2 Paxos的Quorum

​	Quorum用于保证在某些参与者发生故障的情况下，我们仍能从存活的参与者那里收集投票，从而继续执行算法。Quorum表示执行操作所需的最少票数，通常为多数派(超过半数)的参与者。 **Quorum背后的主要思想是：即使参与者发生故障或恰好被网络分区隔开，也至少有一个参与者可以充当仲裁者，以确保协议正确性。**

​	一旦提案被足够多的参与者接受，提案中的值就一定会被协议接受，因为任何两个多数派至少有一个共同的参与者。

​	**<u>Paxos可以保证在无论多少节点发生故障时都具有安全性。任何配置都不会产生不正确或不一致的状态，不然就违背了共识的定义</u>**。

​	**为了在f个进程发生故障时仍可以保证活动性，协议总共需要2f+1个进程。这样来，当f个进程发生故障时，仍然有f+1个进程可以继续执行。由于使用了Quorum而非要求所有进程都存活， Paxos(以及其他共识算法)即使在f个进程发生故障时也能保证产生结果**。在14.3.7节中，我们将会用稍微不同的术语讨论Quorum，并描述如何构建一个仅在算法*步骤*之间需要 Quorum相交的协议。

> ​	**务必记住， Quorum仅描述了系统的阻塞属性**。为了保证安全性，每一步中我们都要等待至少Quorum个节点的响应。我们可以发送Propose和Accept!消息给更多节点，但不必等待它们的回应。一些系统使用推测执行(speculative execution)：发出冗余査询，帮助在部分节点发生故障时获得所需的响应数。但<u>为了保证活动性，我们可以在收到Quorum的响应后立即继续</u>。

### * 14.3.3 故障场景

​	在讨论故障时，分布式算法变得格外有趣。<u>一个故障场景是：在第二阶段，提议者在将值广播给全部接受者之前宕机(类似情况还可能发生在提议者还活着但响应很慢，或无法联系到部分接受者)。**这种情况下，新的提议者可以拾取并提交该值，然后将其分发给其他参与者**</u>。

+ 提议者P1已完成提案编号为1的选举阶段，但是在它发送值V1给接受者A1之后宕机。
+ 另一个提议者P2发起一个提案编号为2的新回合，收集Quorum个接受者的回复本例中为A1和A2)，并用P1提出的旧值V1继续执行。

​	<u>由于算法状态已被复制到多个节点，提议者故障不会导致无法达成共识。**但凡提议者宕机之前有一个参与者A1接受了提案的值，该提案都能被下一个提议者重新拾取**。这也意味着，所有这些都可以在原始提议者不知道的情况下发生</u>。

​	**<u>在客户端/服务器应用程序中，客户端仅连接到原始提议者，因此这可能会导致客户端不知道Paxos的执行结果</u>**。

​	除此以外还有其他可能的情况。例如：

+ 就像前一个例子，P1在将V1发送给A1之后宕机。
+ 下一个提议者P2发起一个提案编号为2的新回合，收集Quorum个接受者的回复，但是这次A2和A3首先做出了响应。收集到Quorum个回复后，P2提交了它自己的值，尽管理论上之前A3已经成功提交了一个不同的值。

​	这里还有另一种可能性：

+ 只有一个接受者A1接受值V1之后，提议者P1宕机。A1接受之后很快也宕机了，因此无法将值通知给下一个提议者。
+ 提议者P2发起新回合，它与A1的存活时间没有重叠，继续执行并提交了自己的值。
+ 回合结束后，任何与A1有重叠的提议者将会忽略A1的值并选择接受一个更新的提案。

​	**还有一个故障场景是：两个或更多提议者发生竞争，每个都试图通过提议阶段，但一直未能收集到多数派投票，因为其他提议者打败了它们**。

​	<u>尽管接受者承诺不接受任何编号更小的提案，但只要后来的提案编号更大，它们仍有可能回复多个准备请求</u>。**当提议者尝试提交该值时，它可能会发现接受者已经回复了编号更大的准备请求。这可能导致多个提议者不断重试并一直阻止彼此继续**。<u>我们通常引入随机退避机制来解决该冋题，最终，一个提议者将继续执行，而另一个提议者将等待</u>。

​	Paxos算法能容忍接受者故障，但剩余的接受者必须足以构成多数派。

### * 14.3.4 Multi-Paxos

​	到目前为止，我们讨论了经典的Paxos算法。在该算法中，我们选择任意一个提议者来尝试发起Paxos回合。这个方法存在的一个问题是，系统中的每轮复制都需要进行一轮提议。只有当选出提议者之后(即多数派的接受者向某一提议者回应Promise消息时)，才会进入复制阶段。<u>**为了避免重复进行提议阶段，使提议者可以重用已被认可的地位，我们可以使用Multi-Paxos**。 Multi-Paxos引入了领导者的概念——提议者的负责人(distinguished proposer)[LAMPORTO01]。这一补充十分关键，能显著提升算法的效率</u>。

​	有了一个已确立的领导者，我们就可以跳过提议阶段，直接进行复制：分发值并收集接受者的确认。

​	<u>经典Paxos算法中，读取可以通过执行一轮Paxos回合实现，它从不完整的回合(如果存在的话)中收集值。之所以必须这么做，是因为最后一个已知的提议者无法保证一定包含最新的数据，可能另一个提议者已经修改了状态，而当前提议者并不知道</u>。

​	<u>Multi-Paxos中也可能发生类似的情况：我们试图从已知的领导者进行读取，但此时另一位领导者已经当选，那么请求将返回过期的数据，这就违背了共识的可线性化(linearizability)保证。**为了避免这种情况并确保其他进程都无法成功地提交值，一些Multi-Paxos实现使用租约(lease)机制**</u>。

​	领导者定期与参与者通信，通知它们自己还活着，同时也延长了租约。<u>参与者需要做出回应，允许领导者继续在位，**并承诺在租约期内它们不会接受其他领导者的提案**</u>[CHANDRAO7]。

​	租约不是正确性保证，而是一种性能优化，它允许直接从当前领导者读取数据而不用收集Quorum个回应。为了确保安全性，**<u>租约依赖于参与者之间有界的时钟同步</u>**。<u>如果时钟偏移太大，可能会发生领导者认为其租约仍然有效，但其他参与者认为其租约已过期。这种情况下无法保证可线性化</u>。

​	<u>**Multi-Paxos有时被描述为应用在某数据结构上的操作的复制日志(replicated log)**</u>。算法本身对该数据结构并无感知，仅仅关注如何一致地复制要追加到日志上的值。<u>为了在进程崩溃时仍能保存状态，参与者将接收到的消息写入持久化日志中</u>。

​	为了防止日志无限变大，应将其内容应用到上述数据结构上。<u>当日志与主数据结构同步后，创建一个快照，之后就可以截断日志</u>。**日志和状态快照应当保持一致，<u>快照变更和日志截断这两个操作应当原子地完成</u>**[CHANDRAO7]。

​	<u>我们可以将单次的Paxos想象成一个单次写入寄存器(write-once register)：我们可以向格子中填入一个值，一旦写入值便无法再修改。在第一步中，提议者争夺寄存器的所有权，在第二阶段中，其中一个提议者写入值。同样地，可以将Multi-Paxos想象成仅追加日志，由一系列的值组成：我们每次可以写入一个值，所有值都严格有序，并且无法修改已写入的值[RYSTSOV16]</u>。也有一些共识算法提供读-改-写寄存器的集合，它们使用状态共享而不是复制状态机，例如Active Disk Paxos[CHOCKLER15]和CASPaxos[RYSTSOV18」。

### * 14.3.5 快速Paxos

​	<u>通过允许任何提议者**直接联系接受者而不是通过领导者**，我们可以在经典Paxos算法的基础上减少1次网络交互。为此，我们需要将Quorum从原来的f+1增加到2f+1(其中f是允许出现故障的进程数)，并将接受者的总数增加到3f+1 [JUNQUEIRA07]。这个优化被称为快速Paxos(Fast Paxos)</u> [LAMPORT06]。

+ 经典Paxos算法有一个条件：复制阶段中，<u>提议者可以选择在提议阶段收集到的任何值</u>。

+ 快速Paxos有两种类型的回合：经典回合的过程与经典Paxos相同，而<u>快速回合则允许接受者接受其他值</u>。

​	<u>描述这个算法时，我们把提议阶段收集到足够数量答复的提议者称为**协调者**，而用提议者来指代所有其他的提议者。有些快速Paxos的描述认为客户端可以直接联系接受者[ZHAO15]</u>。

​	<u>在快速回合中，如果允许协调者在复制阶段选择自己的值，那么它可以向接收者发出条特殊的Any消息</u>。这时，接受者可以接受任何提议者的值，就像经典回合中从协调者那里收到带有该值的消息一样。换句话说，各接受者独立地决定它们从不同提议者那里获得的值。

​	**如果两个或多个提议者都在尝试用快速步骤减少网络交互次数，接受者将收到多个不同的值，就会发生冲突(collision)。协调者必须进行干涉，发起新的回合来恢复**。

​	这意味着，当接受者在从不同的提议者收到值后，可能会决定出冲突的值。当协调器检测到值冲突时，它必须重新发起提议阶段，以让各个接受者收敛到单个值。

​	**<u>快速Paxos的一个缺点是：如果请求速率较高，冲突会导致网络交互次数更多、请求延迟更长</u>**。

​	文献[JUNQUEIRA07]表明，**由于副本数量的增加，以及随之而来的参与者之间交换的消息增加，尽管步骤减少了，但快速Paxos的延迟可能比经典Paxos更长**。

### * 14.3.6 平等Paxos

​	用提议者负责人作为领导者会让系统更容易发生故障：领导者一旦发生故障，需要选出个新的领导者，算法才能继续执行。另一个问题是，<u>领导者的工作负载比其他提议者更大，从而影响系统性能</u>。

> ​	**一种避免让领导者承担整个系统负载的方法是分区**。很多系统将可能值的范围划分为较小的段，让系统的一部分负责一个特定范围，而不用管其他部分。这举措有助于提高可用性(将故障隔离到单个分区，防止传播到系统的其他部分)、性能<u>(因为不同值所在的段互不重叠</u>)和可扩展性(<u>可以通过增加分区数量来扩展系统</u>)。**务必注意，涉及多个分区的操作需要原子地执行**。

​	**不同于Multi-Paxos使用领导者和提案编号来对命令进行排序，我们可以<u>为每个特定命令使用一个领导者，通过査找并建立依赖关系来对命令进行排序</u>。这种方法通常称为平等Paxos**(Egalitarian Paxos， EPaxos)[MORARU11]。

​	允许无冲突写入独立地提交到复制状态机的这个想法最早在文献[LAMPORT05]中引入，称为广义Paxos(Generalized Paxos)。 EPaxos是广义Paxos的第一个实现。

​	EPaxos试图结合经典Paxos算法和Multi-Paxos的优势。

+ 经典Paxos提供高可用性，因为每个回合都会选出一个领导者，但是消息复杂度更高。
+ Multi-Paxos提供高吞吐量，消息数量更少，但是领导者可能成为瓶颈。

​	EPaxos开始于一个**预接受(Pre-Accept)阶段**，<u>该阶段中，一个进程成为某个特定提案的领导者</u>。每个提案必须包括：

+ 依赖关系

  所有可能和当前提案有冲突的命令，但是不必包含已提交的那些。

+ 序列号

  序列号用于打破循环依赖。它被设为一个大于任何已知依赖项序列号的值。

​	**收集这些信息后，它发送一条Pre-Accept消息给*快速Quorum*的副本。快速Quorum包含[3f/4]个副本，其中f是可容忍的故障数**。

​	<u>副本检査其本地命令日志，在本地视图中寻找可能冲突的提案，并依此更新提案的依赖关系，然后将这些信息回复给领导者。如果领导者从快速Quorum的副本收到响应，并且发现各副本以及领导者的依赖项列表均一致，则可以提交这个命令</u>。

​	<u>如果领导者没有收到足够的响应，或者从副本接收到的依赖命令列表不同且包含有干扰的命令，它将用新的依赖项列表以及序列号更新当前提案。新的依赖项列表包含了所有副本回应的依赖项。新的序列号必须大于各副本中看到的最高序列号</u>。之后，领导者将新的命令发送给[f/2]+1个副本。最终，领导者可以提交该提案。

​	实际上，我们有两种可能的场景：

+ 快速路径

  当依赖项匹配时，领导者可以安全地继续进行提交阶段，且只需要快速Quorum的副本即可。

+ 慢速路径

  当副本间存在分歧时，必须先更新各副本的命令列表，然后领导者才能继续提交。

​	**由于在预接受阶段已经收集了依赖关系，因此在执行请求时，命令间的顺序已经确定命令之间不会突然冒出别的命令：新命令只会被追加到序列号最大的命令之后**。

​	执行命令时，副本将构建一张依赖关系图，以反向依赖关系的顺序执行所有命令。换句话说，在执行一条命令之前，必须先执行它所有的依赖项(包括间接依赖项)。<u>考虑到**只有互相干扰的命令才存在依赖**，因此这种情况对于大多数工作负载来说应该相对罕见</u>[MORARU13]。

​	<u>类似于Paxos，EPaxos也使用提案编号来防止传播过期的消息。序列号由epoch(当前集群配置的标识符，当有节点离开或加入时会变化)、单调递增的节点本地计数器和副本ID构成。如果副本收到的提案序列号小于已经见过的序列号，它将拒绝该提案，并回复已知的最大序列号以及更新过的命令列表</u>。

### * 14.3.7 柔性Paxos

​	Quorum通常被定义为多数派的进程。根据定义，无论我们如何选择节点，两个Quorum间必然存在交集：总是至少存在一个节点将它们联系在一起。

​	我们必须回答两个重要的问题：

+ 是否有必要在每个执行步骤中联系多数派节点？
+ 所有的Quorum都要相交吗？换句话说，用来选出提议者负责人(第一阶段)的Quorum、用于决定一个值的 Quorum(第二阶段)和每个执行实例(例如，如果第二步的多个实例被并发地执行)都必须有共同的节点吗？

​	我们仍在讨论共识，因此我们不能更改安全性的定义：算法必须保证达成一致。

​	Multi-Paxos很少会执行领导者选举阶段，提议者负责人可以提交多个值而无须重新进行选举，它可能会长期处于领导地位。在11.8节中，我们讨论了一些公式，帮助我们找到能使节点集合之间存在交集的配置。一个例子是仅等待一个节点确认写入(允许对其余节点的请求异步完成)，然后从全部的节点读取。换句话说，**只要我们保证R+W>N，读写集之间就至少有一个共同节点**。

​	<u>我们能否使用类似的逻辑来解决共识问题？事实证明是可以的：**在 Paxos中，我们只需要求第一阶段的节点(选举领导者的节点)与第二阶段的节点(参与接受提议的节点)存在重叠**</u>。

​	换句话说， **<u>Quorum并不一定是多数派，只要是非空的节点集合即可</u>**。

​	如果我们定义参与者的总数为N，提议阶段成功所需的节点数为Q1，接受阶段成功所需的节点数为Q2，那么只要保证Q1+Q2>N即可。<u>**由于第二阶段通常比第一阶段更常见，要想让Q2只包含N/2个接受者，只要将Q1调得更大(Q1=N-Q2+1)。这一发现对于理解共识问题至关重要，相应的算法被称为柔性Paxos(Flexible paxos)**</u>[HOWARD16]。

​	举个例子，假设共有5个节点，如果我们要求收集4个节点的选票才能当选，那么在复制阶段中，我们就可以允许领导者只等待2个节点的回应。此外，由于任意2个接受者的子集都与领导者选举的Quorum之间存在重叠，因此我们可以向不相交的接受者集合提交提案。直觉上说，之所以能这样，是因为毎当新的领导者被选出而当前领导者不知情时，必定至少有一个接受者知道新领导者的存在。

​	**柔性Paxos允许我们牺牲部分可用性换取更低的延迟**：<u>为了减少参与第二阶段的节点数量，必须收集更多的投票，即要求领导者选举阶段有更多的参与者可用。好消息是，**只要当前的领导者稳定(不需要重新选举)，就可以持续进行复制阶段并容忍最多N-Q2个节点故障**</u>。

​	另一个用到相交Quorum思想的Paxos变体是垂直Paxos(Vertical Paxos)。垂直Paxos区分读取和写入 Quorum，二者必须相交。领导者需要为一个或多个编号较小的提案收集较小的读Quorum，并为自己的提案收集较大的写入Quorum[LAMPORT09]。文献[LAMPSON01]还区分了外Quorum和决策Quorum，它们可转换为准备和接受阶段，并给出了类似于柔性Paxos中的Quorum定义。

### * 14.3.8 共识的推广解法

​	Paxos解释起来可能不大容易：它包含多个角色和多个步骤，还有各式各样的变体，常常令人难以跟上。但其实我们可以用更简单的方式思考它。之前我们将参与者分成各种角色，还划分了决策回合，现在，我们可以只用一组简单的概念和规则来实现与单回合Paxos相同的保证。我们对该方法仅做简短的讨论，因为这还是一个相对较新的进展[HOWARD19]——了解它很重要，虽然我们尚未看到它的实现和实际应用。

​	假设有一个客户端和一组服务器。每个服务器上有多个寄存器(register)，每个寄存器用个下标来标识。寄存器只能被写入一次，它可能处于以下三种状态之一：未写入、包含一个值或包含nil(一个特殊的空值)。

​	不同服务器中下标相同的寄存器构成一个寄存器集。每个寄存器集可以包含一个或多个Quorum。根据其中的寄存器的状态， <u>Quorum可能处于**未决定(Any和 Maybe v)**或**已决定(None和Decided v)**两种状态之一</u>:

+ Any

  取决于将来的操作，该Quorum可以决定为任何值。

+ Maybe v

  如果该Quorum做出一个决定，那个决定只能是v。

+ None

  该Quorum无法决定出值。

+ Decided v

  该Quorum已经决定出值v。

​	**客户端与服务器交换消息并维护一个状态表，用于跟踪值和寄存器，并可以从中推断出Quorum所做出的决定**。

​	<u>为了保持正确性，我们必须限制客户端与服务器的交互方式，以及客户端可以写入哪些值、不能写入哪些值。对于读取来说，仅当客户端从**同一寄存器集中**的Quorum节点中读到**已决定的值**之后，才能输出决定的值</u>。

​	为了保证算法的安全性，我们必须保留几个约束条件，因此写入规则要稍微复杂些。首先，我们要确保客户端不会自己提出新的值：<u>写入寄存器的值要么来自输入，要么是从寄存器中读到的</u>。**客户端写入的值不能让同一个寄存器的不同Quorum决定不同的值**。最后，<u>客户端不能覆盖前边的寄存器集之前已决定的值(下标1到r-1的寄存器集的决定必须为None、 Maybe v或 Decided v)</u>。

**广义Paxos算法**

​	结合所有这些规则，我们可以实现一个广义Paxos算法，它使用单次写入寄存器[HOWARD19]就单个值达成共识。假设我们有三个服务器[S0，S1，S2]、寄存器[R0，R1，…]以及客户端[C0，C1，…]，客户端只能写入分配的寄存器子集。我们对所有寄存器使用简单的多数派Quorum({S0，S1}，{S0，S2}，{S1，S2})。

​	算法决策过程包括两个阶段。第一阶段确保写入的安全性，第二阶段将值写入寄存器：

+ 第一阶段

  客户端向服务器发送P1<sub>A</sub>(register)命令(register表示寄存器下标)，检查要写入的寄存器是否是未写入状态。如果该寄存器是未写入的，则将下标1到register-1的所有空寄存器都置为nil，以阻止客户端写入前边的寄存器。服务器回复到目前为止所有已写入的寄存器。如果从多数派的服务器收到回应，则客户端选择所在寄存器下标最大的非空值；若回应中不包含任何值，则可以选择它自己的值。如果没有收到足够多的回应，则重启第一阶段。

+ 第二阶段

  客户端向所有服务器发送P2<sub>A</sub>(register， value)消息(value表示一个值)，告知它们第一阶段选择的值。如果多数派服务器做出了回应，则客户端可以输出决定出的值。否则将从第一阶段重新开始。

​	客户端C0尝试提交值V。在第一步中，其状态表为空，服务器S0和S1回复空的寄存器集，说明到目前为止没有寄存器被写入过。在第二步中，客户端可以提交值V，因为没有写入过其他的值。

​	此时，任何其他客户端都可以查询服务器以了解当前状态。对于R0来说， Quorum{S0，S1}已经到达 Decided V状态， Quorums{S0，S2}和{S1，S2}已达到 Maybe V状态，所以C1选择值Ⅴ。此时，客户端不能决定出除Ⅴ以外的值。

​	该方法能帮助我们理解Paxos的语义。不同于之前从远程参与者间的交互来思考状态(比如，提议者询问接受者是否已经接受过另一个提议)，我们可以从最后的已知状态来思考，从而使决策过程变得简单并消除可能的歧义。不可变状态和消息传递也更容易正确地实现。

​	<u>我们可以将其与经典Paxos进行比较。例如，当客户端发现前边的某个寄存器集处于Maybe V状态时，它会尝试再次提交V，这个过程类似于Paxos中提议者可以在上个提议者发生故障之后继续提议该值，只要故障发生之前有至少一个接受者收到了该提案。类似地，Paxos中如果发生领导者冲突，可以用较高的提案编号重新投票来解决，而在广义Paxos中，所有未写入的下标更低的寄存器都被置为nil</u>。

## * 14.4 Raft

​	在诞生后的十多年间， Paxos几乎是共识算法的代名词，但在分布式系统社区中，它一直以难以理解著称。2013年，一种称为Raft的新算法出现了，设计它的研究者希望创造一种易于理解和实现的算法。Raft最初在论文"In Search of an Understandable Consensus Algorithm"(寻找可理解的共识算法) [ONGARO14]中提出。

​	分布式系统本身就够复杂的了，人们也希望能有个更简单的算法。作者随论文发布了个名为LogCabin的参考实现，以消除可能的歧义并帮助将来的实现者更好地理解算法。

​	各个参与者在本地保存一份日志，记录了状态机执行的命令序列。由于各进程接收的输入是相同的，日志中记录的命令及其顺序也是相同的，因此，将这些命令应用到状态机上一定会得到相同的输出。<u>Raft将领导者的概念变成头等公民，以此来简化共识问题。领导者负责协调状态机的操作和复制</u>。**Raft和原子广播算法以及Multi-Paxos有许多相似之处：从各副本中选出一个领导者，由它进行原子决策并建立消息顺序**。

​	Raft中的每个参与者都可以扮演以下三个角色之一：

+ **候选者(candidate)**

  <u>领导者的位置是暂时的，任何参与者都可以担任此角色。要成为领导者，节点必须先变成候选者，并尝试收集多数派的投票</u>。如果候选者既没有赢得选举也没有输掉选举(票数被多个候选者平分，谁也没有得到多数派的投票)，那么将开启新的任期并重新选举。

+ 领导者(leader)

  当前的**临时**集群领导者，负责处理客户端请求并与复制状态机进行交互。领导者当选的时间段称为任期(term)。每个任期用一个单调递增的数宇来标识，它可以持续任意长的时间段。<u>如果当前的领导者崩溃、无响应或其他进程怀疑它发生了故障(可能是由于网络分区或消息延迟导致的)，则会选举新的领导者</u>。

+ 跟随者(follower)

  **被动参与者，负责保存日志条目以及响应领导者和候选者的请求。Raft的跟随者角色类似于Paxos中的接受者加学习者。所有进程一开始都是跟随者**。

​	<u>为了在不依赖时钟同步的情况下保证全局偏序关系，Raft将时间分为任期(也称为epoch)，**毎个任期内领导者是唯一且稳定的**</u>。任期用单调递增的数字编号，每个命令用任期编号和任期内的消息编号唯一标识[HOWARDI4]。

​	有时候，不同参与者眼中的当前任期可能不一致，因为它们可能在不同的时刻得知新任期，或者可能错过了一个或多个任期的领导者选举。每条消息都包含一个任期标识符，如果参与者发现自己的任期已过期，则将其更新为编号较大的那个[ONGARO14]。这意味着，**在任一时刻可能存在多个任期，但当发生冲突时，编号较高的任期获胜**。<u>只有发起新的选举或发现当前任期已过期时，节点才会更新它的任期</u>。

​	当节点启动时，或当跟随者没有收到领导者的消息并怀疑其已经崩溃时，都会发起领导者选举。参与者首先转变成候选者状态，然后尝试从多数派节点收集投票。

+ 领导者选举

  <u>候选者P1向其他进程发送**RequestVote**(请求投票)消息，其中包括候选者任期、它所知道的最后任期和它观察到的最后一条日志的ID</u>。获得多数派投票之后，该候选者成功当选为本任期的领导者。**每个进程最多投票给一位候选者**。

+ **周期性心跳协议**

  使用心跳(heartbeat)机制来保证参与者的活动性。领导者会定期向所有跟随者发送心跳以维持其任期。<u>如果跟随者在选举超时时间内未收到心跳，则会假定领导者发生了故障并发起新的选举</u>。

+ 日志复制/广播

  领导者可以不断地通过**AppendEntries**(追加条目)消息将新的值追加到复制日志中。该消息中包含领导者的任期、前一条日志的下标和任期以及一个或多个要保存的日志条目。

### * 14.4.1 Raft中的领导者角色

​	<u>**领导者只能从拥有全部已提交条目的节点中选举出来**：如果在选举期间，跟随者的日志信息比候选者更新(换句话说，任期ID更大，或任期一样但日志序列更长)，则会拒绝投票给这个候选者</u>。

​	为了赢得选举，候选者必须获得多数票。日志条目总是按顺序复制的，因此只要比较最新条目的ID就能知道一个参与者的状态是否是最新的。

​	一旦当选，领导者需要接受客户端请求(也可能是从其他节点转发过来的)并将其复制给跟随者。复制的方式是向日志中追加新的条目，并且并行地分发给所有跟随者。

​	<u>跟随者收到AppendEntries消息时，将消息中的条目追加到本地日志中，并对消息进行确认，告知领导者消息已被持久化。当收到足够多副本的确认消息后，该条目即被视为已提交，领导者日志中也会相应地做上标记</u>。

​	由于只有最新的候选者才能成为领导者，跟随者从来不用将领导者带到最新的状态，日志条目只会从领导者流向跟随者，而不会反过来。

+ a) 一条新的命令x=8被追加到领导者的日志中。
+ b) **提交值之前需要先将其复制给多数派的参与者**。
+ c) **领导者完成复制后立即在本地提交值**。
+ d) 将提交决定复制到跟随者。

​	一个共识回合的例子，其中P1是领导者，它拥有最新的事件视图。领导者将日志条目复制给跟随者，并在收集到确认消息后提交这些命令。<u>提交一个条目时也会提交它以前的所有条目</u>。**只有领导者才能决定是否可以提交该条目**。每个日志条目上都标有任期ID(日志条目方框右上角的数字)和日志下标，以标识其在日志中的位置。已提交的条目一定已被复制给Quorum的参与者，并且可以安全地按照日志中的顺序应用到状态机。

### * 14.4.2 故障场景

​	当多个跟随者决定成为候选者时，可能会没有任何一个候选者收集到多数派的投票，这种情况被称为平票(split vote)。**Raft用随机定时器来降低再次平票的概率**。其中一个候选者可以略早发起下一轮选举，从而收集到足够的票数，而其他人则等待并做出让步。这种方法可以在不引入其他协调的情况下加快选举速度。

​	跟随者可能会宕机或反应迟缓，领导者必须尽最大努力保证消息传递。如果未能在预期时间内收到确认，它可以尝试再次发送消息。作为一种性能优化的方式，它也可以并行发送多个消息。

​	**由于领导者复制的每个条目都是唯一标识的，因此重复发送消息一定不会破坏日志顺序。跟随者用序列ID对消息去重，确保重复传递不会产生意外的副作用**。

​	序列ID也用来保证日志顺序。<u>如果领导者发来一个编号较大的日志条目，但(消息中附带的)前一个条目的ID和任期与本地不匹配，跟随者将会拒绝该条目</u>。如果两个不同副本上的日志条目具有相同的任期和下标，则它们一定包含相同的命令，并且在此之前的所有条目也都相同。

​	**<u>Raft保证永远不会把未提交的消息显示为已提交，但是由于网络延迟或副本运行缓慢，已提交的消息可能仍然显示为进行中(In progress)</u>。这个性质没什么害处，可以通过客户端重试来解决**[HOWARD14]。

​	为了进行故障检测，领导者需要向跟随者发送心跳，通过这一方式维持任期。当其中一个节点注意到当前领导者宕机时，它将尝试发起选举。**新当选的领导者必须将集群状态恢复到最新的日志条目**。<u>为此，它先找到共同基础点(common ground，领导者和跟随者都认同的最后日志条目)，并命令跟随者丢弃在此之后的所有(未提交的)条目</u>。然后，它将自己日志中的最近条目发送给跟随者，覆盖其历史记录。领导者自己的日志记录永远不会被删除或覆盖，只会将条目追加上去。

​	总结一下，Raft算法提供以下保证：

+ **任一给定任期内只会选举出一个领导者，同一任期内不可能有两个领导者都处于活动状态**。
+ 领导者不会删除日志条目或改变其顺序，只会向日志中追加新消息。
+ **已提交的日志条目一定也存在于后续领导者的日志中，不可能被撤回，因为在提交该条目之前已经完成复制**。
+ <u>所有消息均由消息ID和任期ID唯一标识，无论是当前的还是后续的领导者，都不能将同一标识符重用于不同条目</u>。

​	自诞生以来，Raft已变得非常流行，如今它被用在许多数据库及其他分布式系统中，包括CockroachDB、Etcd和Consul。这可以归功于它的简洁直观，也意味着Raft实现了它的承诺——成为一个可靠的一致性算法。

## 14.5 拜占庭共识

​	到目前为止，我们讨论的所有共识算法都假定不存在拜占庭故障(参见8.7.3节)。换句话说，节点都“诚实”地执行算法，而不是尝试利用算法或伪造结果。

​	我们将看到，这一假设允许以更少的可用参与者达成共识，并且一次提交所需的网络交互次数更少。但是，有时分布式系统也会部署在有潜在对抗性的环境中，其中各节点并非由同一实体控制，我们需要用算法来保证：即使在某些节点行为失常甚至带有恶意的情况下，系统仍然能够正常运行。除了人为的恶意之外，拜占庭式故障还可能由bug错误配置、硬件问题或数据损坏引起。

​	<u>大多数拜占庭共识算法需要用N<sup>2</sup>条消息完成一个算法步骤，其中N表示Quorum的大小，因为Quorun中的每个节点都必须与其他所有节点通信。每个步骤都需要用其他节点做交又验证，这么做是必需的，因为节点不能彼此信赖，也不能依赖领导者，必须将返回的结果与多数响应进行比较才能验证其他节点的行为</u>。

​	我们在这里只讨论一种拜占庭共识算法：**实用拜占庭容错(Practical Byzantine FaultTolerance，PBFT)** [CASTRO99]。PBFT假定节点故障相互独立(即，故障可能会相互配合，但不能立即或至少不会以相同的方式接管整个系统)。系统具有弱同步假设，就像你期望中的正常网络那样：故障可能会发生，但不会无限持续下去，最终一定会恢复。

​	<u>节点间的所有通信都经过加密，以防止伪造消息或网络攻击。副本知道彼此的公钥通过它来验证身份并加密消息。缺陷节点可能会泄漏系统内部的信息，因为即使用了加密，每个节点也需要解密消息内容并做出回应。这不会危害到算法，因为它有不同的用途</u>。

### 14.5.1 PBFT算法

> [PBFT共识算法详解 - 无_忧 - 博客园 (cnblogs.com)](https://www.cnblogs.com/zmk-c/p/14535734.html)

​	**为了保证PBFT算法的安全性和活动性，缺陷节点(也叫拜占庭节点)不能超过(n-1)/3个(其中n是参与者总量)。系统若要容忍f个缺陷节点，则需要至少n=3f+1个节点**。

​	之所以如此，是因为需要多数派节点同意一个值：可能存在f个缺陷副本，还可能有f个副本没有响应但并非因为缺陷(例如，由于网络分区、电源故障或维护所致)。算法必须能够从无缺陷的副本中收集到足够的响应，以保证仍能超过缺陷副本的响应数量。

​	<u>PBFT中共识的性质与其他共识算法相似：尽管可能发生故障，但所有无缺陷的副本必须就接收到的值的集合及顺序达成一致</u>。

​	为了区分集群的配置，PBFT使用视图。每个视图中有一个副本是主副本(节点)，其余副本被视为备份副本(节点)。所有节点都被连续编号，主节点的序号是v mod N，其中v是视图ID，N是当前配置下的节点数。当主节点发生故障时，视图也会发生变化。客户端通过主节点执行操作，主节点将请求广播到备份节点，备份节点执行请求并将响应发回客户端。<u>客户端等待f+1个副本回复相同的结果，才能确认执行成功</u>。

​	主副本收到客户端请求后，协议执行分为三个阶段：

+ 预准备(pre-prepare)

  主节点**广播**一条消息，其中包含视图ID、一个唯一的单调递增标识符、载荷(客户端请求)和载荷的摘要。摘要使用强抗冲突性的哈希函数计算得到，并由发送方签名。如果备份节点的视图与主节点的视图匹配，且请求未被篡改(计算出的载荷摘要与消息中的一致)，那么备份节点会接受这条消息。

+ 准备(prepare)

  如果备份节点接受了预准备消息，它将进入准备阶段，并开始向其他所有副本(包括主副本)**广播**Prepare消息，其中包含视图ID、消息ID和载荷的摘要，但不包含载荷本身。<u>只有当接收到**2f个**来自不同备份节点的 Prepare消息并且这些消息的视图、ID和摘要均相同时，节点才能进入下个阶段</u>。

+ 提交(commit)

  之后，备份节点进入提交阶段，将Commit消息**广播**到其他所有副本，并等待收集到**2f+1条**匹配的Commit消息(可能包括自己)。

​	算法中，摘要用于减少准备阶段中的消息大小，为了验证消息，没必要重新广播整个载荷，而摘要就是对载荷的一个概括。加密哈希函数具有抗冲突性：很难产生两个不同但摘要相同的值，更不用说两个内容不同、摘要相同并且同时都有意义的值。此外，还要对摘要进行签名以确保摘要本身来自受信任的来源。

​	**2f这个数字很重要，因为算法需要至少f+1个无缺陷副本对客户端做出响应**。

​	正常情况下PBFT算法的时序图（客户端，节点P1～P4）：客户端向P1发送请求，为了从一个阶段移动到下一个阶段，节点需要从*运作正常*的对等节点收集足够多的匹配响应。P4可能发生了故障或响应了不匹配的消息，因此它的响应会被忽略不计。

​	在准备阶段和提交阶段中，节点向其他所有节点发送消息，并等待从相应数量的其他节点收到消息，以检査消息是否匹配并确保不广播错误的消息。各节点对所有消息进行交叉验证，从而保证只有无缺陷的节点才能成功提交消息。如果无法收集足够多的匹配消息，节点不会进入下一步。

​	当副本收集到足够多的提交消息时，它们会通知客户端并完成当前回合。只有当收到**f+1个**匹配的响应后，客户端才能确定执行已正确完成。

​	<u>当副本注意到主节点处于非活动状态且怀疑它可能发生了故障时，就会发生视图变更。检测到主节点故障的节点将不再回应消息(除了检查点和视图变更相关消息)，广播一个视图变更通知并等待确认。当新视图的主节点收到2f个视图变更事件时，它将发起个新视图</u>。

​	为了减少协议中的消息数量，客户端可以先<u>试探性执行请求</u>(比如，当它们收集到足够多匹配的Prepared消息之后)并收集**2f+1个**匹配的响应。如果客户端无法收集到足够多匹配的试探性响应，它将重试并等待f+1的非试探性响应，就像之前描述的那样。

​	<u>PBFT中的只读操作仅需一轮网络交互即可完成</u>。客户端向所有副本发送读请求。各副本等到所有进行中的修改该值的操作完成提交，然后在其试探性状态(tentative state)上执行请求并回复客户端。当从不同副本收集到值相同的**2f+1个**响应后，操作就完成。

### * 14.5.2 恢复和检查点

​	副本将接受的消息保存在一个稳定的日志中。每条消息至少要保留到**f+1个**节点执行过为止。当发生网络分区时，这些日志能帮助其他副本更快地恢复，但是恢复过程还需要某些方法来验证这些状态的正确性，不然恢复机制可能会被用作攻击手段。

​	为了证明这些状态是正确的，节点计算出到某个给定序列号为止的消息的状态。节点可以比较摘要、验证状态的完整性，以及确保恢复过程中收到的消息加在一起达到的最终状态是正确的。这一过程太昂贵，无法为每个请求执行。

​	<u>每隔N个请求(N是可配置的常量)，主节点会创建一个稳定检查点(stable checkpoint)：它广播一个最新序列号(标识执行结果已被反映在状态中的最后一个请求)以及相应状态的摘要。然后，它等待2f+1个副本响应，这些响应构成了该检查点的证明，保证各副本可以安全地丢弃给定序列号之前的所有预准备、准备、提交和检查点消息</u>。

​	理解拜占庭容错性很重要，它被用在有潜在对抗性的网络中的存储系统上。**大多数时候，对节点间通信进行身份验证和加密就足够了，但是当系统各部分之间没有信任时，必须采用类似PBFT的算法**。

​	容忍拜占庭故障的算法会带来显著的消息交换开销，因此，了解这些算法的使用场景格外重要。文献[BAUDET19]和文献[ BUCHMAN18]中还提到了一些其他协议，它们尝试为包含大量参与者的系统优化PBFT算法。

## 14.6 本章小结

​	共识算法是分布式系统中最有趣的也是最复杂的主题之一。过去的几年中，涌现出了许多新算法以及现有算法的多种实现，这证明了共识算法的重要性和流行程度日渐增长。

​	在本章中，我们讨论了经典Paxos算法以及Paxos的几个变体，每个变体都对不同的特性进行了改进：

+ Multi-Paxos

  允许提议者保留这一角色，提议多个值而不是一个。

+ 快速Paxos

  允许我们用快速回合减少消息数量，快速回合中，接受者可以处理当前领导者以外的提议者发来的消息。

+ EPaxos

  通过分析提交的消息之间的依赖关系建立事件的顺序。

+ 柔性Paxos

  放宽对Quorum的要求，只要求第一阶段(投票)的Quorum和第二阶段(复制)的Quorum存在交集。

​	Raft用更简单的方式描述共识，让领导者角色成为算法中的头等公民。Raft将日志复制领导者选举和安全性三个方面分开。

​	为了保证对抗性环境中的共识安全性，应当使用拜占庭容错算法，例如PBFT。PBFT中，参与者交叉验证彼此的响应，仅当有足够多的节点遵守算法规则时，才会继续执行步骤。

# 第二部分总结

​	性能和扩展性是数据库系统的重要属性。存储引擎和节点本地的读写路径可能会对系统性能产生较大影响：决定在本地能够多快地处理请求。同时，负责集群内通信的子系统通常对数据库的扩展性产生较大的影响：决定集群大小和容量的上限。但是，如果一个存储引擎缺乏扩展性，其性能随着数据集的增长而下降，那它也只能用于有限的场景。若把很慢的原子提交协议构建在最快的存储引擎上，也不会产生好的结果。

​	分布式的、集群范围的以及本地节点上的进程是相互联系的，必须整体地考虑。设计数据库系统时，你必须考虑不同子系统如何配合协作。

​	第二部分首先讨论了分布式系统与单节点应用程序的不同，以及在分布式环境下会遇到哪些困难。

​	我们讨论了分布式系统的基本构成、各种一致性模型以及几个重要类别的分布式算法，其中一些可用于实现这些一致性模型：

+ 故障检测

  准确且高效地识别远程进程的故障。

+ 领导者选举

  快速且可靠地选择一个进程作为临时协调者。

+ 消息传播

  使用点到点通信可靠地分发信息。

+ 反熵

  识别并修复节点间的状态差异。

+ 分布式事务

  在多个分区上原子地执行一系列操作。

+ 共识

  在远程参与者之间达成一致，并且能容忍一定程度的进程故障。

​	这些算法被用在许多数据库系统、消息队列、调度器等重要的基础架构软件中。利用本书中的知识，你能更好地了解它们的工作方式，从而帮助你更好地决定应当使用哪个软件，并识别出潜在的问题。
