# 数据库系统内幕-学习笔记-01

# 前言

你将深入了解如下内容：

+ 存储引擎：学习存储的种类、分类依据，理解基于B树和不可变日志存储结构的存储引擎。
+ 存储构建块：理解数据库文件如何使用诸如页缓存、缓冲池等辅助数据结构来组织构建高效的存储。
+ 分布式系统：逐步学习节点和进程间如何连接并构建复杂的通信模式。
+ 数据库集群：深入探究现在数据库中常用的一致性模型，并了解分布式存储系统是如何实现一致性的。

# 第一部分 存储引擎

​	数据库是模块化的系统，由多个部分组成：

+ 接受请求的传输层
+ 决定以最高效方式运行查询的查询处理器
+ 执行操作的执行引擎
+ **存储引擎**

​	存储引擎(或数据库引擎)是数据库的一个软件组件，它负责在**内存和磁盘**上<u>存储、检索和管理数据</u>，而设计它的目的是长久保存每个节点的数据[REED78]。

​	数据库可以响应复杂的査询，存储引擎则会更细粒度地看待数据并提供一组简单的数据操作API，允许用户创建、更新、删除和检索数据记录。从某个角度来看，**数据库是构建在存储引擎之上的应用程序**，<u>它提供了表结构( schema)、查询语言、索引、事务和许多其他有用的特性</u>。

​	<u>为了获得灵活性，键和值都可以是没有预设格式的任意字节序列</u>。它们的排序和表示语义是在更高级别的子系统中定义的。例如，你可以在一个表中使用int32(32位整数)作为键，而在另一个表中使用 ascii(ASCⅡ字符串)；从存储引擎的角度来看，这两个键都只是序列化的条目。

​	BerkeleyDB、 LevelDB(及其后代 RocksDB)、LMDB(及其后代 libmdbx、 Sophia和HaloDB)等存储引擎的开发都**与它们现在所嵌入的数据库彼此独立**。

​	使用**可插拔的存储引擎**使数据库开发人员能够使用现有存储引擎来构建数据库系统，并将精力集中在其他子系统上。

​	同时，**数据库系统组件之间清晰的解耦为切换不同引擎提供了机会，这些引擎可能分别适用于特定的用例**。

​	例如：流行的数据库 MySQL有几个存储引擎，包括 InnoDB、MyISAM和RocksDB（在 MyRocks发行版中），而MongoDB则允许在WiredTiger、内存以及(现已弃用的)MMAPv1存储引擎之间进行切换。

---

+ 数据库的比较

  ​	模拟现实世界中的工作负载不仅能帮助你了解数据库的运行方式，还能帮助你学习如何操作与调试数据库，并了解其社区的友好程度和能提供帮助的程度。数据库的选择总是这些因素的组合，而性能通常并不是最重要的方面：**使用保存数据缓慢的数据库通常比使用会快速丢失数据的数据库要好得多**。

  ​	要比较数据库，非常详细地理解用例并定义当前和预期的变量是有帮助的，例如：

  + 表结构和记录大小
  + 客户端数量
  + 查询类型和访问模式
  + 读写查询速率
  + 任何这些变量中的预期变化

  ​	明确这些变量可以帮助回答以下问题：

  + 数据库支持所需的查询吗？
  + 数据库能够处理我们计划存储的数据量吗？
  + 单个节点可以处理的读写操作有多少？
  + 一个系统计划要有多少个节点？
  + 鉴于预期的增长率，我们如何扩展集群？
  + 维护过程是什么？

  ​	在回答了这些问题之后，你可以构建一个测试集群并模拟你的工作负载。大多数数据库已经有了压测工具，可以用来重现特定的用例。如果没有标准的压测工具用来在数据库生态系统中生成现实中的随机工作负载，那么这可能是一个危险的信号。如果有什么东西让你无法使用数据库自带的工具，那么你可以尝试一个现有的通用工具，或者从零开始实现一个。

  ​	有些人以为，可以将数据库当作黑匣子而无须了解其中的内容是件好事。但实践往往表明，这样做迟早会碰到bug、服务中断、性能倒退或其他问题。你最好为这些问题做好准备，如果你了解并且理解数据库的内部结构，就可以减少业务风险且更有可能快速地恢复。

---

+ TPC-C基准

  ​	事务处理性能委员会(Transaction Processing Performance Council，TPC)提供了一组数据库厂商用来比较和宣传其产品性能的基准。TPC-C是一个**联机事务处理(OLTP)**基准，它是只读事务和更新事务的混合，用于模拟常见的应用程序工作负载。

  ​	该基准关注的是**执行的并发事务的性能和正确性**。主要性能指标是**吞吐量**：**数据库系统每分钟能够处理的事务数**。其需要执行事务具备**ACID属性**并符合基准本身定义的属性集。

  ​	此基准不专注于任何特定的业务部门，但提供了对大多数适用OLTP数据库的应用都很重要的抽象操作集。它包括几个表(tables)和实体(entities)，如仓库(warehouses)、库存(stock/inventory)、客户(customers)和订单(orders)，并指定了表布局(table layouts)、可以对表执行的事务的细节、表的最小行数和数据持久性约束(data durability constraints)。

# 1. 简介和概述

​	数据库可以用于不同的目的：一些主要用于临时热数据，一些用作长期冷数据的存储，一些允许复杂的分析查询，一些只允许通过键访问值，一些专门对存储时间序列数据进行了优化，一些则可高效地存储二进制大对象(Binary Large Object，BLOB)。为了理解其差异和边界，我们将从简短的分类和概述出发，这有助于我们理解要讨论的范围。

​	一些人将数据库分为三大类。

+ 联机事务处理(OLTP)数据库

  它处理大量面向用户的请求和事务。查询通常是预定义的，并且运行时间都很短。

+ 联机分析处理(OLAP)数据库

  它处理复杂的聚合。OLAP数据库通常用于分析和数据仓库，能够处理复杂的、长时间运行的ad-hoc查询。

+ 混合事务和分析处理(HTAP)数据库

  它结合了OLTP和OLAP存储的属性。	

​	还有许多其他术语和分类：键值存储、关系数据库、面向文档的存储和图数据库。我们假定读者对它们的功能具有髙层次的认知和理解，因此在这里没有对这些概念进行定义。由于我们在这里讨论的概念是广泛适用的，并且对前面提到的大多数数据存储在某种程度上也都是适用的，所以完整的分类方式对于进一步的讨论既不必要也不重要。

## 1.1 数据库架构

![img](https://s6.51cto.com/oss/202103/24/f63b1924e54f62b5a1dadf1cff2dd639.jpg)

​	一些描述数据库架构的信息来源(例如，[HELLERSTEINO7]、[WEIKUM01]、[ELMASRI11]和[MOLINA08]以不同的方式定义了组件及其间的关系。图1-1所示的架构展示了这些表示中的一些常见主题。

​	数据库使用客户端/服务器模型，其中数据库系统实例(节点)扮演服务器的角色，而应用程序实例则扮演客户端的角色。

​	客户端请求通过传输子系统到达数据库。而请求则以査询的形式出现，通常以某种查询语言表示。传输子系统还负责与数据库集群中的其他节点进行通信。

​	在接收到査询后，传输子系统将査询移交给查询处理器，由査询处理器对该査询进行语法解析、解释和验证。稍后，数据库将执行访问控制检査，因为只有在解释査询之后才能完全执行这些检查。

​	<u>解析后的査询被传递给查询优化器，后者首先消除査询中不可能执行的部分与冗余的部分，然后根据内部统计信息(索引基数、近似交集大小等)和数据分布(数据存储在集群中的哪些节点以及传输所需的成本)，尝试找到执行査询的最高效方法。优化器既处理査询解析所需的关系操作(通常表示为依赖关系树)，也处理査询优化(如索引排序、基数估计和访问方法的选择)</u>。

​	**査询通常以执行计划(或查询计划)的形式呈现**。<u>执行计划是为得到完整结果而必须执行的一系列操作。由于执行同一个查询的不同执行计划存在效率上的差异，所以优化器会挑选出最优的计划</u>。

​	执行计划由执行引擎处理，后者收集本地和远程操作的执行结果。远程执行可能涉及向集群中的其他节点写入数据、从其他节点读取数据以及数据复制。

​	本地査询(直接来自客户端或其他节点的査询)由存储引擎执行。存储引擎拥有如下几个具有专门职责的组件。

+ **事务管理器**

  事务管理器调度事务，并确保它们不会使数据库处于逻辑不一致的状态。

+ **锁管理器**

  锁管理器为正在运行的事务锁定数据库对象，确保并发操作不会破坏物理数据的完整性。

+ **访问方法(存储结构)**

  **访问方法(存储结构)管理磁盘上的数据访问并负责组织磁盘上的数据**。访问方法包括堆文件和存储结构，例如B树(参见2.3节)和LSM树(参见7.1节)。

+ **缓冲区管理器**

  缓冲区管理器将数据页缓存在内存中(参见5.1节)。

+ **恢复管理器**

  恢复管理器维护操作日志并在出现故障时还原系统状态(参见5.2节)。

​	<u>**事务管理器和锁管理器共同负责并发控制(参见5.3节)**：它们在保证数据的逻辑和物理完整性的同时，确保尽可能高效地执行并发操作</u>。

## * 1.2 内存数据库与磁盘数据库

+ 小结
  + 内存数据库
    + 特征：内存存储主要数据，磁盘进行日志记录和数据备份。
    + 优势：
      1. 磁盘提供内存管理，资源管理相对磁盘更为简单
      2. 硬件层面内存访问比磁盘快
      3. NVM存储减少甚至完全消除某些场景的读写延迟不对称，提高读写性能，允许字节可寻址访问
      4. **无需像磁盘文件系统一样设计复杂的宽树、矮树数据结构**，可以直接将简单数据结构存储到内存中
    + 劣势：
      1. 相对磁盘昂贵
      2. 天然具有**易失性**，需要接近硬件层面的支持才能尽量弥补该缺陷
      3. 
  + 磁盘数据库
    + 特征：内存作为临时缓存，磁盘存储主要数据。
    + 优势：
      1. 价格便宜，通常用于支持大量数据存储
      2. 数据存储具有持久性，断电数据不丢失
    + 劣势：
      1. 随机读写性能远差于内存，即便是顺序读写也是有性能差距
      2. 需要配合文件系统进行开发，需设计复杂数据结构存储数据

---

​	数据库系统将数据存储在**内存**和**磁盘**上。

+ 内存数据库(有时称为主存数据库( main memoryDBMS)主要将数据储存在内存中，并<u>使用磁盘进行**数据恢复**和**日志记录**</u>。
+ 磁盘数据库则将大部分数据保存在磁盘上，并<u>使用内存来**缓存**磁盘内容或作为**临时存储**</u>。

​	这两种类型的系统在一定程度上都使用磁盘，但主存数据库几乎只将其内容存储在内存中。

​	数据库使用内存作为主要数据存储的主要原因是性能、相对较低的访问成本以及访问粒度。基于內存的编程也比基于磁盘的编程简单得多。<u>操作系统抽象了内存管理，允许我们从分配和释放任意大小的内存块的角度进行思考。而在磁盘上，我们必须手动管理数据引用、序列化格式、释放的空间和碎片</u>。

​	内存数据库增长的主要限制因素是内存的**易失性**(换言之，缺乏数据持久性)以及成本。由于内存中的内容不是持久化的，所以软件缺陷、崩溃、硬件故障和断电都可能导致数据丢失。<u>有一些方法可以确保系统持久运行，例如不间断电源和由电池支持的内存，但它们需要额外的硬件资源和专门的运维知识</u>。在实践中，这些都可以归结为以下事实：**磁盘更容易维护，价格也低得多**。

​	随着**非易失性存储器(NVM)**[ARULRAJ17]技术的可用性和普及程度的提高，内存数据库发展受限的情况可能发生改变。NVM存储减少甚至完全消除了(取决于某个确切的技术)读和写的延迟之间的不对称，进一步提高了读写性能，并**允许字节可寻址(byte addressable)访问**。

---

+ 基于内存存储的持久性

  ​	**数据库在认定操作完成之前，必须先将其结果写入一个顺序日志文件**。我们将在5.2节中更详细地讨论预写日志。
  
  ​	为了避免在启动过程中或崩溃后重放完整的日志内容，内存数据库维护了一个**备份副本**，该备份副本使用一个基于磁盘且已排序的数据结构，并且对该结构的修改通常是**异步(与客户端请求解耦)**且分批处理的，这样可以减少IO操作的数量。
  
  ​	<u>在恢复过程中，数据库可以从备份和日志中还原数据库内容</u>。
  
  ​	**日志数据通常用于批量备份**。在处理该批日志数据之后，备份将持有截止到这一特定时间点的数据库快照，因此可以丢弃之前的日志内容。这个过程称为**生成检查点(checkpointing)**。它会更新磁盘上的数据库快照使其更接近最新的日志条目，从而缩短恢复时间，且<u>备份过程不会阻塞客户端请求</u>。
  
  > 一些说法认为内存数据库相当于一个具有巨大页缓存的磁盘数据库，这是不公平的(见5.1节)。即使页缓存在内存里，序列化格式和数据布局也会使磁盘数据库产生额外的开销，而不会达到与内存数据库相同的优化程度。
  
  ​	磁盘数据库使用专门的存储结构，针对磁盘访问进行了优化。在内存中，我们可以比较快地跟踪指针，并且**随机内存访问比随机磁盘访问要快得多**。
  
  ​	**<u>基于磁盘的存储结构通常具有宽树和矮树的形式</u>**(参见2.1.2节)，而基于内存的实现可以从更大范围的数据结构中进行选择，并实现不可能或难以在磁盘上实现的优化[ MOLINA92]。
  
  ​	同样，处理磁盘上的可变大小数据需要特别注意，而在内存中，这通常是一个用指针来引用值的问题。

## 1.3 面向列和面向行的数据库

​	大多数数据库系统存储一组数据记录，这些记录由表中的列和行组成。**字段是列和行的交集**：某种类型的单个值。属于同一列的字段通常具有相同的数据类型。

​	对数据库进行分类的方法之一是按数据在磁盘上的存储方式进行分类：按行或按列进行分类。表可以水平分区(将属于同一行的值存储在一起)，也可以垂直分区(将属于同列的值存储在一起)。

​	下图描述了这种区别：a)显示了按列分区的值，b)显示了按行分区的值。

![img](http://p6.itc.cn/images01/20200616/e6c67ccfe3c24536a849b0808a34f5fb.jpeg)

> 面向行的数据库的例子很多：MySQL、 PostgreSQL和大多数传统的关系数据库。而两个开源的、面向列数据存储的先驱则是MonetDB和C-Store（C- Store是Vertica的开源前身）。

### 1.3.1 面向行的数据布局

+ 小结
  + 需要按行读取数据的情况下，面向行的数据布局可以提高空间局部性。	
  + 但是只需要一行中某几列时，面向行的读取比面向列的读取开销更大（其他大字段无用列被连带读取）

---

​	面向行的数据库按记录或行来存储数据。它的布局非常接近表格的数据表示方法，即其中每一行都具有相同的字段集合。

​	在需要按行访问数据的情况下，面向行的存储最有用，将整行存储在一起可以提高空间局部性[DENNING68]。

> **空间局部性原则**是局部性原则之一。该原则指岀，如果访冋一处存储，则其附近的其他存储区域也会在不久的将来被访问。

​	因为**诸如磁盘之类的持久性介质上的数据通常是按块访问的**(换句话说，磁盘访问的最小单位是块)，所以单个块可能将包含某行中所有列的数据。这对于我们希望访问整个用户记录的情况非常有用，但这样的存储布局会使访问多个用户记录某个字段的查询(例如，只获取电话号码的查询)开销更大，因为其他字段的数据在这个过程中也会被读入。

### 1.3.2 面向列的数据布局

+ 小结
  + 数据按照列连续存储在磁盘，可按列读写
  + 适合计算聚合的分析型工作负载
  + 为满足连接、筛选和多行聚合等场景，常用一些隐式标识符(虚拟ID)映射到字段（ps：字段即行和列的交集）

---

​	面向列的数据库垂直地将数据进行分区(即通过列进行分区)，而不是将其按行存储。

​	在这种数据存储布局中，**同一列的值被连续地存储在磁盘上**(而不是像前面的示例那样将行连续地存储)。

​	*例如，如果我们要存储股票市场的历史价格，那么股票价格这一列的数据便会被存储在一起。将不同列的值存储在不同的文件或文件段中，可以按列进行有效的查询，因为它们可以一次性地被读取出来，而不是先对整行进行读取后再丢弃掉不需要的列。*

​	**面向列的存储非常适合计算聚合的分析型工作负载**，例如査找趋势、计算平均值等。如果逻辑记录具有多个字段，但是其中某些字段(在本例中为股票价格)具有不同的重要性并且该字段所存储的数据经常被一起使用，那么我们一般使用复杂聚合来处理这样的情况。

​	为了重建数据元组(这对于连接、筛选和多行聚合可能很有用)，我们需要在列级别上保留一些元数据，以标识与它关联的其他列中的数据点是哪些。如果你显式地执行此操作，则需要毎个值都必须持有一个键，这将导致数据重复并增加存储的数据量。针对这种需求，<u>一些列存储使用**隐式标识符(虛拟ID)**，并使用该值的位置(换句话说，其偏移量)将其映射回相关值</u>[ABADI13]。

> 在过去几年中，可能由于对不断增长的数据集运行复杂分析査询的需求不断增长，我们看到了许多新的面向列的文件格式，如 Apache Parquet、 Apache ORC、 RCFile，以及面向列的存储，如 Apache Kudu、 ClickHouse，以及许多其他列式数据存储组件[ROY12]。

### 1.3.3 区别与优化

1. 在一次读取中，从同一列中读取多个值可以显著提高缓存利用率和计算效率。在现代CPU上，向量化指令可以使单条CPU指令一次处理多个数据点[DREPPER07]
2. 另外，将具有相同数据类型的值存储在一起(例如，数字与数字在一起，字符串与字符串在一起)可以提高压缩率。我们可以根据不同的数据类型使用不同的压缩算法，并为每种情况选择最有效的压缩方法。

​	要决定是使用面向列还是面向行的存储，你需要了解访问模式。

+ 如果所读取的记录中的大多数或所有列都是需要的，并且工作负载主要由单条记录查询和范围扫描组成，则面向行的存储布局可能产生更好的结果。
+ 如果扫描跨越多行，或者在列的子集上进行计算聚合，则值得考虑使用面向列的存储布局。

### 1.3.4 宽列式存储

​	**面向列的数据库不应与宽列式存储(如 Big Table或 Hbase)相混淆**。在这些数据库中数据表示为多维映射，列被分组为**列族**(通常存储相同类型的数据)，并且<u>在每个列族中，数据被逐行存储</u>。<u>此布局最适合存储由一个键或一组键来检索的数据</u>。

​	理解宽列式存储的概念表示是有用的，而它们的物理布局也有所不同。列族的数据布局示意图如图1-4所示：列族被单独存储，但在每个列族中，属于同一键的数据被存储在一起。

[![img](https://s3.51cto.com/oss/202103/24/cddb8555ef06ed9e12ad330bc9f6dd86.jpg)](https://s3.51cto.com/oss/202103/24/cddb8555ef06ed9e12ad330bc9f6dd86.jpg)

## 1.4 数据文件和索引文件

​	数据库系统通常将数据文件和索引文件分开：数据文件存储数据记录，而索引文件存储元数据并使用它来定位数据文件中的记录。索引文件的大小通常比数据文件小。文件被划分成页(page)，每个页通常具有单个或多个磁盘块的大小。页可以被组织成记录的序列或分槽页(slotted page)(参见3.5节)。

​	新增记录（插入）和对现有记录的更新使用键/值对来表示。**大多数现代存储系统不显式地删除页上的数据。相反，它们使用删除标记（deletion marker，也称为墓碑（tombstone）），其中包含此删除动作的元数据，如键和时间戳**。在垃圾收集过程中，这些被更新或被删除标记遮盖（shadowed）过的记录所占用的空间会被数据库回收。该过程会读取页，然后将活动（即未被遮盖）的记录写入新位置，并丟弃被遮盖的记录。

### 1.4.1 数据文件

​	数据文件(有时称为主文件(primary file))，通常可以用**索引组织表**(Index-OrganizedTable，IOT)、**堆组织表**heap-organized table，即堆文件)或**哈希组织表**(hash-organized table，即哈希文件)来实现。

+ 在堆文件中的记录不需要遵循任何特定的顺序，并且大多数情况下它们都是按写顺序放置的。这样，在追加新的页时，数据库便不需要额外的工作或文件重组。<u>堆文件需要额外的索引结构来指向存储数据记录的位置，以使其能够被检索到</u>。
+ 在哈希文件中，记录存储在桶中，并且键的哈希值确定记录属于哪个桶。存储在桶中的记录可以按追加顺序存储，也可以按键排序存储以提高査找速度。
+ <u>索引组织表将数据记录存储在索引自身</u>。由于记录是按键的顺序存储的，所以索引组织表中的范围扫描可以通过顺序扫描其内容来实现。

​	**将数据记录存储在索引中使我们能够将磁盘查找的次数至少减少一次，因为在遍历索引并找到搜索到的键之后，我们不必寻址另外的文件来查找相关联的数据记录**。

​	当记录存储在单独的文件中时，索引文件保存着**数据条目**(data entry)，该条目唯一地标识了数据记录，并包含足够的信息使得数据库可以在数据文件中找到它们。例如，我们可以存储文件偏移量(有时称为行定位符)、数据文件中数据记录的位置或哈希文件中的桶ID。**在索引组织表中，数据条目包含实际的数据记录**。

### 1.4.2 索引文件

​	索引是一种为了高效检索数据而对磁盘上的数据记录进行组织的结构。索引文件被组织成专门的结构，将键映射到数据文件里的记录。这些记录由对应的键(在堆文件的情况下)或主键(在索引组织表的情况下)所标识。

​	主(数据)文件上的索引称为主索引。但是，在大多数情况下，我们还可以假设主索引是在主键或作为主键的一组键之上构建的。所有其他索引都称为二级索引(secondary index)。

​	**二级索引可以直接指向数据记录，也可以简单地存储它的主键**。指向数据记录的指针可以保存堆文件或索引组织表中的偏移量。多个二级索引可以指向同一记录，从而允许单个数据记录能够由不同的字段来标识并且可以被不同的索引来定位。虽然主索引文件中毎个搜索键都有一个唯一的条目，但对于每个搜索键，二级索引可能会针对每个搜索键保存多个条目[MOLINA08]。

+ **如果数据记录的顺序遵循搜索键顺序，则这种索引称为聚簇索引(clustered/clustering index)**。聚簇索引中的数据记录通常与索引存储于同一文件，有时也存放在单独的聚簇文件中，而这些文件均保留了键的顺序。
+ **如果数据存储在单独的文件中，且其顺序不遵循键顺序，则索引称为非聚簇索引( nonclustered/ unclustered index)。**

a）一个索引组织表，其数据记录直接储存在索引文件内部。

b）索引文件仅保存偏移量，而用另外的文件保存数据记录。

[![img](https://s4.51cto.com/oss/202103/25/59f11a06b397af52ad8929a332fc7df6.jpg)](https://s4.51cto.com/oss/202103/25/59f11a06b397af52ad8929a332fc7df6.jpg)

> 索引组织表以索引的顺序保存数据，因此按定义一定是聚簇的。主索引通常是聚簇的，而根据定义，二级索引一定不是聚簇的，因为它们是用于加速主键以外的键的访问的。聚簇索引既可以是索引组织的，也可以具有单独的索引和数据文件。

​	**许多数据库系统都有一个固有的、显式的主键，即唯一标识数据库记录的一组列。在未指定主键的情况下，存储引擎可以创建一个隐式主键（例如，MySQL InnoDB引擎会自动添加新的自增列并填充该列的值）**。

​	该术语可以用于不同类型的数据库系统中：关系型数据库系统（如MySQL和PostgreSQL）、基于Dynamo的NoSQL存储（如Apache Cassandra和Riak），以及文档型存储（如MongoDB）。有时候它可能以专属于某个项目的特定名词的形式出现，但大多数情况下我们都可以将那些特定名词与这些术语进行清晰的映射。

### 1.4.3 间接的主索引

​	在数据库社区中，对于是直接通过文件偏移量引用数据记录还是通过主键索引引用数据记录存在不同的意见。两者各有利弊。

1. 两个索引从二级索引文件直接指向数据条目。
2. 二级索引通过主索引间接地定位数据条目。

![img](https://s6.51cto.com/oss/202103/25/9ff39e72fa20f2888aec4e2af17195b7.jpg)

​	我们还可以使用混合方法将数据文件偏移量和主键存储在一起。首先，你要检査数据偏移量是否仍然有效，如果它已经发生变化了，则需要额外对主键索引进行遍历，在找到新的偏移量后更新索引文件。

## * 1.5 缓冲、不可变性和有序性

​	存储引擎基于某些数据结构。但是，<u>这些结构并不描述缓存、恢复、事务性以及存储引擎在它们之上添加的其他内容的语义</u>。

​	在接下来的章节中，我们将从B树开始讨论(参见2.3节)，并试图解释为什么有这么多的B树变体，以及为什么新的数据库存储结构持续不断地出现。

​	存储结构有三个常见变量：是否使用**缓冲**、**使用不可变的还是可变的文件**，以及**是否按顺序存储**值(有序性)。本书中讨论的存储结构之间的大多数区别和优化都与这三个概念中的一个相关。

+ **缓冲**

  ​	缓冲定义了存储结构在将数据放入磁盘之前是否选择在内存中保留一定数量的数据。当然，**<u>毎个基于磁盘的数据结构都必须在某种程度上使用缓冲，因为读写磁盘的最小数据传输单元是块，并且我们希望写入完整的块</u>**。

  ​	在这里，我们讨论的是可避免的缓冲，这是存储引擎的实现者有意为之的事情。我们在本书中首先讨论的优化之一便是向B树节点添加内存缓冲区，以分摊IO成本(参见6.3节)。然而，这并不是我们应用缓冲的唯一方法。例如，尽管双组件LSM树(two-component LSM Tree，参见7.1.1节)与B树相似，但前者以完全不同的方式使用缓冲，并将缓冲与不可变性结合起来。

+ **可变性（或不可变性）**

  + 可变性定义了存储结构是否可以在文件的同一位置中读取文件的某些部分、更新它们并将更新的结果写入文件。
  + **<u>不可变结构是只可追加的：写入后不修改文件内容，而是将修改附加到文件的末尾</u>**。

  ​	除此之外，还有其他方法来实现不可变性，其中之一便是写时复制(copy-on-write，参见6.1节)，其中持有更新记录的(即被修改的)页会被写入文件中的新位置而非原位置。

  ​	<u>通常，LSM树和B树之间的区别便是数据是不可变的还是原地更新的</u>，但是也存在受B树启发但不可变的数据结构(例如，Bw树，见6.5节)。

+ **有序性**

  ​	**有序性定义为数据记录是否按键顺序存储在磁盘上的页中**。换句话说，紧密排序的键存储在磁盘上的连续段中。

  + <u>有序性常常决定了我们能否有效地扫描记录的范围，而不仅仅是定位单个数据记录</u>。
  + **无序存储数据的方式(通常按插入顺序)对于某些写入时的优化提供了可能性**。例如， Bitcask(参见7.4.1节)和 Wisckey(参见7.4.2节)直接在只可追加的文件中存储数据记录。

## 1.6 本章小结

​	在本章中，我们讨论了数据库系统的架构，并介绍了它的主要组件。

​	为了突出基于磁盘的结构的重要性及其与基于内存的结构的区别，我们讨论了基于内存和基于磁盘的存储。我们得出的结论是，基于磁盘的结构对于两种类型的存储都很重要，但其被用于不同的目的。

​	为了解释访问模式如何影响数据库系统的设计，我们讨论了面向行和面向列的数据库以及区分两者的主要因素。为了讨论如何存储数据，我们介绍了数据文件和索引文件。

​	最后，我们介绍了三个核心概念：**缓冲、不可变性和有序性**。我们将在整本书中强调这几个存储引擎的属性。

# 2. B树基础知识

​	在上一章中，我们将存储结构分为两类：可变存储结构和不可变存储结构，并确定不可变性为影响数据库设计和实现的核心概念之一。**大多数可变存储结构使用原地更新的机制**。在插入、删除或更新操作期间，数据记录直接在目标文件中原本的位置被更新。

​	存储引擎通常允许同一数据记录在数据库中存在多个版本，例如：当使用多版本并发控制（multi-version concurrency control，参见5.3.6）或分槽页结构（参见3.5节）时。为了简单起见，现在我们假设每个键只与一条数据记录相关联，且该数据记录具有唯的位置。

​	**最流行的存储结构之一是B树**。许多开源数据库系统都基于B树，多年来，这些数据库已经被证明可以覆盖大多数的用例。

​	B树并不是最近才发明的：它是由Rudolph Bayer和 Edward M.McCreight于1971年便引入的概念，此后其越来越流行。到了1979年，B树已经有了相当多的变体， Douglas Comer收集并系统化地整理了其中一些变体[COMER79]。

​	在深入研究B树之前，让我们先讨论一下为什么应该考虑替代传统搜索树(例如，二分搜索树、2-3树和AL树[KNUTH98])。为此，我们先回顾一下什么是二分搜索树。

## 2.1 二分查找树

​	二分搜索树(BST)是一种有序的内存数据结构，可以用来高效地进行键值查找。二分搜索树由多个节点组成，每个树节点由一个键、一个与该键关联的值以及两个子节点指针(因此称为二分)组成。

### 2.1.1 树的平衡

​	插入操作并不会遵循任何特定模式，元素插入可能导致树不平衡的情况(即它的个分支比另一个分支长)。

​	平衡树指的是高度为log<sub>2</sub>N的树(其中N是树中数据项的总数)，并且两个子树之间的高度差不大于1 [KNUTH98]。如果不进行平衡，我们将失去二分搜索树结构的性能优势，使得树的最终形状由插入和删除的顺序来确定。

​	为了防止在一个分支保持为空的时候还在另一个分支上增加新的元素使之变得更长(如图2-3b所示)，我们可以在毎次操作之后对树进行平衡。树的平衡是通过以最小化树高并将每一边的节点数保持在界限内的方式重新组织节点来完成的。

​	保持树的平衡的方法之一是在添加或删除节点后执行旋转：如果插入操作使分支不平衡(分支中的两个连续节点只有一个子节点)，则可以围绕中间节点旋转树。

### * 2.1.2 基于磁盘存储的树

​	如前所述，不平衡树的最差情况的时间复杂度为0(N)。平衡树的平均时间复杂度是0(1ogN)。同时，由于**扇出较低**(扇出是指每个节点允许拥有的最大子节点数)，我们必须相当频繁地执行平衡操作、重新定位节点并更新指针。**维护成本的增加使得二分搜索树作为存储在磁盘上的数据结构变得不切实际**[NIEVERGELT74]。	

​	如果我们想在磁盘上维护二分搜索树，则将面临如下几个问题。

+ 一个问题是**局部性**：由于元素是以随机顺序添加的，所以不能保证新创建的节点是在其父节点附近写入的，这意味着<u>节点子指针可能跨越多个磁盘页</u>。通过修改树的布局和使用**分页二分树**，我们可以在一定程度上改善这种情况(参见2.2.3节)。
+ 另一个与游历子指针的开销密切相关的问题是**树高**。由于二分树的扇出为2，所以树的高度是树中元素个数的以2为底的对数。<u>我们必须执行O(log<sub>2</sub>N)次查找以定位要搜索的元素，这就要求执行相同数量的磁盘传输</u>。2-3树和其他低扇出树具有类似的限制：**<u>虽然它们作为内存数据结构是有用的，但是较小的节点大小使得它们在外部存储上并不实用</u>**[COMER79]。

​	**<u>一个在磁盘上存储二分搜索树的简单方法所需的磁盘寻道次数与比较次数一样多，因为这样的结构原生不具备数据局部性</u>**。这使得我们踏上一条寻找某个可以提供数据局部性的数据结构的道路。

​	考虑到这些因素，更适合磁盘实现的树必须具有以下属性：

+ **高扇出，以改善邻近键的数据局部性。**
+ **低高度，以减少遍历期间的寻道次数。**

> 扇出与高度呈负相关：
>
> + 扇出越高，高度便越低。
> + 如果扇出高，则每个节点可以容纳更多子节点，这降低了节点数量，进而降低了高度

## 2.2 基于磁盘的结构

​	我们已经讨论了基于内存和基于磁盘的存储(参见1.2节)。对于特定的数据结构，我们可以得出相同的区别：有些更适用于磁盘，有些则在内存中工作得更好。

​	正如我们已经讨论过的，**<u>并不是毎个满足空间和时间复杂度要求的数据结构都能高效地用于磁盘存储</u>**。

​	**<u>数据库中使用的数据结构必须加以调整，以适应持久性介质存在的限制</u>**。

​	<u>当数据量大到在内存中保存整个数据集是不可能的或不可行时，通常就需要使用磁盘上的数据结构了。在任何时侯，只有一小部分数据可以缓存在内存中，而其余数据必须以某种允许高效访问的方式被存储在磁盘上</u>。

### * 2.2.1 机械硬盘

​	在大多数传统算法被开发出来的时候，旋转型磁盘是最广为使用的持久性存储介质，而这种硬盘对这些算法的设计产生了很大的影响。后来，存储介质的新发展(如闪存驱动器)激发了新的算法以及对现有算法的改进，以利用新硬件的能力。如今，新型数据结构仍在不断涌现，它们被优化以应用于非易失性的字节可寻址存储(例如，[XIA17]和KANNAN18])。

​	**在旋转型磁盘上，寻道增加了随机读取的成本，因为其需要磁盘旋转和机械磁头运动来将读/写磁头定位到期望的位置。然而，一旦完成了这些高成本的部分，读取或写入连续字节(即顺序操作)的成本就相对较低了**。

​	**<u>旋转型驱动器的最小传送单元是扇区</u>**，因此当执行某些操作时，至少可以读取或写入整个扇区。扇区大小通常从512字节到4KB不等。

​	**<u>磁头定位是机械硬盘(HDD)操作中成本最高的部分</u>**。这就是我们经常听到的顺序IO可以带来正面效果的原因之一：顺序IO将会从磁盘读取和写入连续的存储段。

### * 2.2.2 固态硬盘

​	固态硬盘(SSD)没有可移动的部件：既没有需要旋转的磁盘，也没有为读取而必须移动的磁头。典型的SSD由记忆单元构成，这些单元连接成串(每个串通常为32到64个单元)，串被组合成阵列，阵列被组合成页，页被组合成块[LARRIVEE15]。 

​	根据所使用的某种具体技术，一个单元可以保存一位或多位数据。不同设备的页大小不同，但通常在2KB到16KB之间。块通常包含64到512个页。块被组织成平面(plane)，最后，平面被放置在晶圆核心(die)上。SSD可以具有一个或多个晶圆核心。图2-5展示了这个层次结构。

![img](https://s6.51cto.com/oss/202103/25/fbee98d488b477b3d63c22a276efab95.jpg)

​	**可写(可编程)或可读的最小单元是页**。但是，我们只能对空的记忆单元进行更改(即对写入之前已擦除的单元进行更改)。**<u>最小的擦除实体不是页，而是保存多个页的块，这就是为什么它通常被称为擦除块。空块中的页必须按顺序写入。</u>**

​	**闪存控制器**有一个组件叫作闪存转换层(Flash Translation Layer，FTL)(有关FTL的更多信息，参见7.6.1节)，它负责将页ID映射到对应的物理位置，并跟踪空的、被写过的和被丢弃的页。另外，它还负责垃圾收集，在此期间FTL会查找可以被安全擦除的块。有些块可能仍有活页，在这种情况下，它会将活页从这些块迁移到新位置，并将页ID重新映射到那里。在这之后，它擦除现在未使用的块，使它们变为可写状态。

​	**<u>因为在这两种设备类型(HDD和SSD)中，我们都是面向一定量的数据(例如，逐块访问数据)而不是单个字节来操作的，所以大多数操作系统都具有块设备这个抽象</u>**[CESATIO5]。它隐藏内部磁盘结构并在操作系统内部缓冲I/O操作，**<u>因此毎当我们从块设备读取单个字时，包含它的整个块将会被读取</u>**。这是我们不能忽视的一个限制，在处理基于磁盘存储的数据结构时应该始终考虑到这一点。

​	<u>在SSD中，我们并不像在HDD中那样非常强调随机I/O和顺序I/O的区别，因为随机读取和顺序读取之间的延迟差异并不是很大</u>。不过由于预取、读取连续的页和内部并行性的缘故，二者仍然存在一些差异[GOOSSAERT14]。

​	<u>尽管垃圾收集通常是一个后台操作，但它可能会对写性能产生负面影响，特别是在随机和未对齐的写工作负载的情况下</u>。

​	<u>只写完整的块并将后续写操作组合到同一个块中，可以帮助减少所需的IO操作的数量</u>。在后面的章中，针对实现这一目标的方法，我们将讨论**缓冲**和**不变性**。

### * 2.2.3 磁盘存储结构

​	**<u>除了磁盘访问本身的成本之外，磁盘操作的最小单元是块这一事实是构建有效的磁盘存储结枃的主要限制和设计条件</u>**。要跟踪指向块内特定位置的指针，我们必须获取整个块。既然不得不这样做，那么我们可以通过更改数据结构的布局来利用这个条件。

​	在本章中，我们已经多次提到指针，但是这个词对于不同磁盘存储结构的语义稍有不同。在磁盘上，大部分时间我们都手动管理数据布局(除非我们使用内存映射文件)。虽然这类似于常规指针的操作，但我们必须计算目标指针的地址并显式地追踪该指针。

​	在大多数情况下，磁盘上的偏移量是预先计算出来的(即指针在它所指向的那部分内容被存储之前被写入磁盘)，或者缓存在内存中直到其被刷写到磁盘上。<u>在磁盘结构中创建长依赖链会极大地增加代码和结构的复杂性，因此最好将指针的数量及其跨度保持最小</u>。

​	总之，磁盘存储结构的设计要考虑到其目标存储介质的特性，并且通常要为实现更少的磁盘访问进行优化。我们可以通过**提高局部性**、**优化结构的内部表示**以及**减少页外指针的数量**来实现这一点。

​	在2.1节中，我们得岀结论，即高扇出和低高度是实现最佳磁盘数据结构所需的特性。我们还讨论了来自指针的额外空间开销，以及由于平衡而重新映射这些指针所带来的维护开销。
​	**<u>B树结合了这些思想：增加节点扇出、减少树高和节点指针的数量、降低平衡操作的频率</u>**。

---

+ 分页二分树

  ​	通过将节点分组到页来设计二分树的布局(如图2-6所示)改善了数据局部性。要找到下一个节点，只需在已经获取的页面中追踪指针即可。但是，其间的节点和指针仍然会产生一些开销。在磁盘上布局结构并进行进一步维护是一项不容易的工作，特别是在键和值不是预先排好序而是以随机形式添加的时候。**平衡需要页重组，这又会导致指针更新**。

  ![img](https://s6.51cto.com/oss/202103/25/eba2710e14a6a28b6767d8b3f3630563.jpg)



## 2.3 无处不在的B树

​	可以将B树看作图书馆里一个巨大的目录室：你首先必须选择正确的柜子，然后在那个柜子里选择正确的架子，接下来在架子上选择正确的抽屉，最后浏览抽屉里的卡片，找到你正在寻找的那一个。类似地，B树构建了一个帮助快速导航和定位搜索项的层次结构。

​	正如我们在2.1节中所讨论的，B树是建立在平衡搜索树的基础上的，不同之处在于前者具有更高的扇出(即具有更多的子节点)和更低的高度。

​	在大多数文献中，二分树节点被绘制为圆形。由于每个节点只负责一个键，并将存储范围划分为两部分，所以这种层次的细节表现是高效和直观的。而B树节点则通常被绘制为矩形，并且指针块也被显式地表现出来，以突出子节点和分隔符键之间的关系。图2-7并排显示了二分树节点、2-3树节点和B树节点，这有助于你理解它们之间的异同。

![img](https://s5.51cto.com/oss/202103/25/cd77c1f71f951a995fece01c3d8020c5.jpg)

​	没有东西阻止我们以同样的方式描绘二分树。这两种结构具有相似的指针追踪语义，而在如何维持平衡方面表现出差异。图2-8展示并提示了二分搜索树和B树之间的相似性：在这两种情况下，键将树分成子树，并用于遍历这棵树和查找要搜索的键。你可以将其与图2-1进行比较。

![img](https://s2.51cto.com/oss/202103/25/179a0c5c3105fa373109db4999e4642b.jpg)

​	**B树是有序的：B树节点内的键按顺序存储**。因此，我们可以使用像二分搜索这样的算法来定位搜索到的键。这也意味着B树中的查找具有对数复杂度。例如，在40亿（4×10<sup>9</sup>）个项中找到搜索的关键字需要大约32次比较（有关此主题的更多信息，参见2.3.3节）。

​	**<u>如果每一次比较都进行一次磁盘搜索，则搜索速度将大大降低，但是由于B树节点存储数十甚至数百个数据项，所以我们只需要在每个层跳转时进行一次磁盘搜索</u>**。我们将在后面更详细地讨论查找算法。 

​	使用B树，我们可以有效地执行单点查询和范围查询。在大多数查询语言中，我们通过谓词相等（=）表示单点查询来定位单个项，而通过谓词比较（<、>、≤和≥）表示范围查询来按顺序查询多个数据项。

### 2.3.1 B树的层次结构

​	B树由多个节点组成。每个节点最多可容纳N个键和N+1个指向子节点的指针。这些节点在逻辑上分为三类。 

+ 根节点

  根节点没有父节点，是树的顶端。

+ 叶节点

  叶节点是没有子节点的底层节点。

+ 内部节点

  连接根节点和叶节点的其他节点，B树通常包含多层的内部节点。

该层次结构如图2-9所示。

![img](https://s6.51cto.com/oss/202103/25/ba4b95867baf3aa9b9c340480cabc356.jpg)

​	<u>由于B树是一种页组织技术（即用于组织和导航固定大小的页的技术），所以节点和页这两个术语在描述中可相互替换</u>。

​	<u>节点容量与其实际持有的键的个数之间的关系称为占用率</u>。

​	**B树的特征在于其扇出（fanout）**：存储在每个节点中的键的个数。

+ <u>**为保持树的平衡需要做出一些结构上的更改，而更高的扇出则有助于均摊这些更改的所带来的开销。**</u>
+ <u>**同时，通过在单个块或多个连续块中存储指向子节点的键和指针，可以减少寻道的次数。平衡操作（即分裂和合并）会在节点已满或几乎为空时被触发**。</u>

---

+ B+树 

  ​	我们使用术语“B树”作为一类共享所有或大部分上述属性的数据结构的统称。其实我们所描述的数据结构的更精确的名称是B+树。[KNUTH98]将具有高扇出的树称为多向树（multiway tree）。 

  ​	**<u>B树允许在根节点、内部节点和叶节点当中的任意层上储存值。而B+树则仅在叶节点中存储值，其内部节点仅存储分隔键，用于指引搜索算法去找到叶节点上的关联值</u>**。 

  ​	<u>由于B+树中的值仅存储在叶节点这一层上，所以所有操作（插入、更新、删除和检索数据记录）仅影响叶节点，并且这些操作仅在分裂和合并期间才会传播到更高层</u>。

  ​	B+树广为人知，因此我们像其他文献中一样称之为B树。例如，在[GRAEFE11]中，B+树被指定为默认设计，MySQL InnoDB也将其B+树的实现称为B树。

### * 2.3.2 分隔键

​	存储在B树节点中的键称为索引条目(index entry)、分隔键(separator key)或分隔符单元格(divider cel)。它将树分割成子树(也称为分支或子范围)，其持有包含对应键的范围。**键存储时已经排好序，以便使用二分搜索**。査找算法通过定位一个键并跟随相应的指针从较高的层次移动到较低的层次来找到一个子树。

​	节点中的第一个指针指向小于第一个键的数据所在的子树，节点中的最后一个指针指向大于或等于最后一个键的数据所在的子树。其他指针指向两个键之间的子树K<sub>i-1</sub>≤K<sub>s</sub><K<sub>i</sub>,其中K是一组键，K<sub>s</sub>是属于子树的键。图2-10展示了这些不变式。

![img](https://s5.51cto.com/oss/202103/25/1fd7418cde3e1a79a1dc943fa0d6a127.jpg)

​	<u>一些B树变体还具有**同级节点指针**，它们通常位于**叶子层**上，以简化范围扫描</u>。这些指针有助于避免在查找下一个同级节点时还要返回到父级节点的情况。一些实现在两个方向上都有指针，在叶子层上形成一个**双链表**，<u>这使得逆向迭代成为可能</u>。

​	**使B树与众不同的是，它不是自上而下构建的(像二分搜索树那样)，而是采用相反的构建方式——自下而上。随着叶节点数量的增加，内部节点的数量和树的高度也将增加。**

​	<u>由于B树在节点内部为将来的插入和更新保留了额外的空间，所以树的存储占用率可以低至50%，但通常这个数值要高得多。较高的占用率不会对B树的性能产生负面影响</u>。

### 2.3.3 B树查找复杂度

​	可以从两个角度来讨论B树査找的复杂度：**块传输的数量**和**查找期间完成的比较的次数**。

​	就传输次数而言，对数基为N(毎个节点的键数)。从根节点每往下走一层，节点个数就多K倍，并且跟随一个子指针可以将搜索空间减少至N分之一。在查找期间，最多寻址log<sub>K</sub>M(其中M是B树中的项的总数)个页来査找一个搜索键。在从根到叶的通路上必须经过的子指针的数量也等于层数，换句话说，其等于树的高度h。

​	从比较次数的角度来看，对数基是2，因为在每个节点内搜索一个键是使用二分搜索完成的。每次比较都将搜索空间减半，因此复杂度为log<sub>2</sub>M。

​	了解寻道次数和比较次数之间的区别有助于我们从这两个角度直观地认识搜索是如何执行的，并理解其有着怎样的查找复杂度。

### * 2.3.4 B树查找算法

​	既然我们已经介绍了B树的结构和内部组织，我们就可以定义查找、插入和删除的算法了。<u>要在B树中找到一个项，我们必须执行从根节点到叶节点的单向遍历</u>。这种搜索的目的是査找被搜索的键或其前驱。

+ **査找精确匹配用于单点査询、更新和删除；**
+ **查找其前驱则对于范围扫描和插入非常有用。**

​	该算法从根节点上开始执行二分搜索算法，将要搜索的键与存储在根节点中的键进行比较，直到找到大于要搜索的键的第一个分隔键。这样便定位了一个要搜索的子树。正如前面所讨论的，索引键将树分割成多个子树，子树的边界位于两个相邻的键之间。<u>一旦找到子树，我们就顺着相应的指针继续相同的搜索过程(定位分隔键然后顺着指针往下找)，直到我们到达目标叶节点，在那里我们要么找到了搜索的键，要么通过定位它的前驱节点而得出它不存在的结论</u>。

​	每到一层，我们将得到一个更详细的树的视图：我们从最粗粒度的层(树的根节点)开始，然后下降到下一层，在那里，键可以表示更精确、更详细的范围，直到我们最终到达数据记录所在的叶节点。

+ **在进行单点査询时，在找到或找不到所搜索的键之后搜索便结束了**。
+ **而在进行范围扫描时，迭代从找到的最近的键值对开始，并顺着同级指针继续移动，直到到达范围的末尾或用尽范围谓词为止**。

### 2.3.5 键的数目

​	纵观文献，你可以找到描述键和子节点偏移量数目的不同方法。[BAYER72]提到了一个表示最佳页大小的、依赖于设备的自然数k。在这种情况下，页可以保存k到2k个键，但是可以被部分填充并保存最少k+1、最多2k+1个指向子节点的指针。根页可以容纳1到2k个键。之后，一个参数l被引入，并且任何非叶页都可以具有l+1个键。

​	其他文献，例如[GRAEFE11]，则描述了可以容纳多达N个分隔键和N+1个指针的节点，这些节点具有类似的语义和不变式。

​	这两种方法给我们带来了相同的结果，而其间的差异仅用于强调每篇文献的内容有所不同。在本书中，为了清楚起见，我们坚持使用N作为键的数目(对于叶节点则是键值对的数目)。

### 2.3.6 B树的节点分裂

​	要将一个值插入B树，我们首先必须定位目标叶节点并找到插入点。为此，我们使用前面小节中描述的算法先进行查找操作。在定位叶节点之后，键和值被追加到叶节点之上。在B树中，更新操作则通过使用查找算法定位目标叶节点并将新值与现有键相关联来完成。

​	如果目标节点没有足够的可用空间，我们就说该**节点溢出(overflow)**了[NICHOLS66]，此时必须将其分裂为两部分才能放入新的数据。更准确地说，如果以下条件成立，则需要分裂节点：

+ 对于叶节点：如果节点最多可以容纳N个键值对，且再插入一个键值对将使其超过其最大容量N。
+ 对于非叶节点：如果节点最多可以容纳N+1个指针，且再插入一个指针将使其超过其最大容量N+1。

​	分裂是通过分配新节点、将一半元素从原分裂节点传输给它并添加它的第一个键和指向父节点的指针来完成的。在这种情况下，我们说这个键被提升( promote)了。执行分裂处的数组下标称为分裂点(也称为中点)。分裂点之后的所有元素(在非叶节点分裂的情况下，包括分裂点)都被传输到新创建的兄弟节点，其余元素保留在分裂节点中。

​	如果父节点已满，即没有容纳被提升的键和指向新创建节点的指针的空间时，也必须分裂父节点。此操作可能会一直递归传播到根节点。

​	一旦树达到其容量(例如，分裂一直传播到根节点)，我们就必须分裂根节点。当根节点被分裂时，将分配一个新的根，该根具有分裂点的键。旧根节点(现在只拥有条目的一半)与新创建的同级节点一起被降级到下一层，从而使树高增加1。

​	**在分裂根节点或合并两个节点以形成新根时，树高会发生变化。在叶节点和内部节点所在的层上，树只水平生长**。

​	图2-11展示了向一个空间已满的叶节点插入新元素11的过程。我们在整个节点的中间画一条线，将一半的元素留在节点中，并将其余的元素移到新的节点中。分裂点的值被放置在父节点中，变为分隔键。

![img](https://s3.51cto.com/oss/202103/25/6aae704c023e59cb5718bc36cc150b1a.jpg)

​	图2-12展示了向一个空间已满的非叶(即，根或内部)节点插入新元素11时的分裂过程。为了执行分裂，我们首先创建一个新节点，并将元素从下标N/2+1的元素处移动到该节点，随后分裂点的键被提升到父一级。

![img](https://s5.51cto.com/oss/202103/25/c41e5a35448f148da7a861bf119725fb.jpg)

​	由于非叶节点分裂总是表现为从下一层传播上来的分裂，所以我们有一个额外的指针(指向下一层新创建的节点)。如果父一级没有足够的空间，则也必须被分裂。

​	是叶节点还是非叶节点(即，该节点是拥有键和值还是仅拥有键)被分裂并不重要。在叶节点分裂的情况下，键连同其相关值一起被移动。

​	分裂完成后有两个节点，我们必须选择正确的节点才能完成插入。为此，我们可以使用分隔键不变量。如果插入的键小于要提升的键，则最后插入到原分裂节点。否则，我们将要插入的键放入新创建的节点。

​	总之，节点分裂分为四个步骤：

1. 分配一个新节点。
2. 将一半元素从分裂节点复制到新节点。
3. 将新元素放入相应节点。
4. 在分裂节点的父节点处，添加一个分隔键和指向新节点的指针。

### 2.3.7 B树的节点合并

​	为了进行删除，首先要定位目标叶节点。当叶节点被定位后，键和与其相关的值就被删除。

​	如果相邻节点所拥有的值太少(即，其占用率低于阈值)，则需要合并同一层的节点。这种情况称为**下溢(underflow)**。[BAYER72]描述了两种下溢场景：如果两个相邻节点具有公共父节点，并且它们的内容能够放入单个节点，则它们的内容应该合并(连接起来)；如果它们的内容无法放入单个节点，则应在它们之间重新分配键以恢复平衡(参见4.4节)。更准确地说，如果满足以下条件，则合并两个节点：

+ 对于叶节点：如果一个节点可以容纳最多N个键值对，并且两个相邻节点中的键值对的数目加起来小于或等于N。
+ 对于非叶节点：如果一个节点可以容纳最多N+1个指针，并且两个相邻节点中指针的数量加起来小于或等于N+1。

​	图2-13展示了删除元素16时的合并过程。在这个过程中，我们将元素从一个兄弟节点转移到另一个兄弟节点。一般来说，将来自同一层右边节点的元素移到同层左边的节点。当然，只要保留键的顺序，则把左边的节点移动到右边也可以完成合并。

​	图2-14则展示了在删除元素10时必须合并的两个同级非叶节点。如果将它们的元素组合在一起后，其可被放入一个节点，那么我们只需一个节点即可。在合并非叶节点时，我们必须从父节点中下拉相应的分隔键(即降级)。父节点的指针数量减少了1，因为合并是从下层传播指针删除的结果，<u>指针的删除则是由删除页引起的。就像分裂一样，合并可以一直传播到根节点这一层</u>。

![img](https://s2.51cto.com/oss/202103/25/43eda27d94589441dba02a525a1e05fa.jpg)

​	总之，假设元素已经被删除，节点合并分为三步：

1. 从右节点复制所有元素到左节点。
2. 从父节点删除右节点指针(如果是非叶子节点合并，则将此指针进行降级)。
3. 删除右节点。

​	**为了减少分裂和合并的次数，B树经常采用的技术之一是再平衡**，我们将在44节中讨论它。

## 2.4 本章小结

​	在本章中，我们首先介绍了创建磁盘存储专用结构的动机。我们指出了二分搜索树可能具有相似的复杂度特性，但因为其扇出低，且平衡会引起的大量数据移动与指针更新，所以不是特别适用于磁盘。而B树通过增加毎个节点中存储的数据项的数目(高扇出)和减少平衡操作的频率来解决这两个问题。

​	随后，我们讨论了B树结构的内部原理以及其査找、插入和删除操作的算法概要。分裂和合并操作有助于重新构造树，以便在添加和删除元素时保持树的平衡。我们将树的深度保持在最小值，并在节点中仍有一些可用空间时向现有节点中添加数据项。

​	我们可以利用这些知识在内存中构造一个B树。为了实现一个基于磁盘的B树，我们需要深入了解如何在磁盘上布局B树的节点，以及如何使用数据编码格式来构建磁盘上的布局。

# 3. 文件格式

​	我们已经介绍了B树的基本语义，现在，我们可以探索B树以及其他数据结构究竟是如何在磁盘上实现的。访问磁盘和访问主存有所不同：<u>从应用开发者的视角看，主存访问几乎是透明的。由于虚拟内存机制[BHATTACHARJEE17]的存在，我们不用手动管理偏移量。而磁盘则是通过系统调用来访问的(参见https://databass.dev/links/54)，我们通常要指定目标文件内的偏移量，然后把数据从磁盘上的形式解析成适合主存的形式</u>。

​	这意味着，要想设计一个高效的磁盘数据结构，必须牢记这一区别。为此，我们必须想出一种易于构造、修改和解析的文件格式。在本章中，我们将会讨论通用的原理和实践，它们可以帮助我们设计各种磁盘结构，而不仅仅是B树。

​	B树有很多种可能的实现，在这里我们会讨论几种有用的技术。不同实现的细节可能有所不同，但是通用的原理是不变的。理解B树的基本原理(比如分裂和合并)是很有必要的，但这对于实际的实现来说还远远不够。要想做出一个有用的成品，还要掌握很多知识。

​	磁盘数据结构中指针管理的语义与内存中的有所不同，你可以将磁盘上的B树看作一种页管理机制：算法需要组合页并在页中移动。需要计算页和指向它们的指针并将它们放置在相应的位置。

​	由于B树最复杂的地方在于它的可变性，所以我们将会讨论页布局、分裂、迁移等可变数据结构中的概念。之后，当讨论LSM树(参见7.1节)时，我们会专注于排序和维护，因为那是LSM树最复杂的地方。

## 3.1 动机

​	非托管内存模型的语言允许我们在需要时(在合理范围内)分配更多的内存，而不必考虑诸如是否有连续的内存段、这些内存是分段还是连续的、内存释放之后会怎样等问题。而**在磁盘上，我们必须自己处理垃圾收集和碎片问题**。

​	磁盘上数据布局的重要性也远大于内存中其重要性。要想设计一个高效的磁盘数据结构，我们需要仔细设计布局以确保高速访问，还要考虑持久性存储介质的特性、设计」进制数据格式，并找出一种高效的序列化和反序列化的方法。

​	只要用过C这样的低级语言(不含其他库)，我们都体会过这一限制:结构体有预定义的大小，并且需要被显式地分配和释放。手动实现内存分配和跟踪则更有挑战性，因为我们只能操作预定义大小的内存段，而且必须跟踪哪些段已经被释放了、哪些段还在使用中。

​	<u>当数据存在主存中时，大部分内存布局的问题都不存在、很容易解决或是能用第三方库来解决</u>。例如，处理变长字段和超大数据就要简单得多：只要分配内存并用指针指向它扣可，无须以任何特殊的方式对其进行布局。在某些情况下，开发者确实会设计特殊的内存数据结构以利用CPU缓存行、预取或其他硬件相关的特性，但这样做主要是出于优化的目的[FOWLER11]。

## 3.2 二进制编码

​	为了将数据高效地保存在磁盘中，我们需要将它编码成一种紧凑的、方便进行序列化与反序列化的格式。在讨论二进制格式时，你会经常听到布局(layout)一词。由于我们不能用malloc和free等操作，只能使用read和write，所以需要用不同的思路重新思考数据访问的方式，并且据此来准备数据。

​	在本节中，我们将讨论构建高效页布局的主要原理，这些原理适用于任何二进制格式你可以利用类似的指导原则来构建文件、序列化格式或通信协议。

​	在将数据记录组织成页之前，我们首先要了解这些问题：如何以二进制形式表示键和数据记录、如何将多个值组合成更复杂的结构，以及如何实现可变长度的数据类型和数组。

### 3.2.1 原始类型

​	键和值具有某种类型，比如 integer(整数)、date(日期)或string(字符串)等并且可以用二进制形式表示(序列化为二进制形式和从二进制形式反序列化)。

​	大多数的数值类型都用固定大小的值来表示。当处理多字节数值时，务必在编解码时使用相同的字节序(byte-order或endianness)，字节序决定了一组字节的先后顺序。

+ 大端

  从最高有效字节(MSB)开始，从高位到低位依次排列。换句话说，最高有效字节具有最低的地址。

+ 小端

  从最低有效字节(LSB)开始，从低位到高位依次排列。

​	图3-1解释了两者的差异。对于十六进制32位整数OxAABBCCDD，其中AA是最高有效字节，以下是分别用大端和小端字节序编码的结果。

![img](https://s4.51cto.com/oss/202103/25/2cbc69098d8c7386b98618d5e571fc82.jpg)

​	例如，要重建具有对应字节序的64位整数，RocksDB拥有一个平台相关的定义以帮助识别目标平台字节序<sup>注1</sup>。如果目标平台的字节序与值的字节序不一致（函数EncodeFixed64WithEndian查找kLittleEndian值，将其与值的字节序作比较），它会调用EndianTransform来反转字节：倒着读出值中的各个字节，然后再拼接到结果上。

​	数据记录由数值、字符串、布尔值之类的原始类型以及它们的组合构成。但是，**当通过网络传输数据或是将其存储在磁盘上时，我们只能使用字节序列**。这意味着，当我们发送或写入一条记录时必须先将其序列化（转换成一段可解释的字节序列），当我们接收或读取一条记录时必须先将其反序列化（把字节序列解析成原来的记录）。

​	在二进制数据格式中，我们总是从原始类型开始，因为它们是构建更复杂数据结构的基石。不同的数值类型可能大小不同，一个字节（byte）是8个比特位，短整型（short）是2字节（16位），整型（int）是4字节（32位），长整型（long）是8字节（64位）。

​	浮点数（比如单精度浮点数（float）和双精度浮点数（double））由符号、小数和指数这三个部分构成。使用最广泛的浮点数表示是IEEE二进制浮点算术标准（IEEE 754）。32位浮点数表示一个单精度值，例如，浮点数0.15652的二进制表示如图3-2所示。低位的23位表示小数值，随后的8位表示指数值，最后的1位代表符号位（是否为负数）。

![img](https://s3.51cto.com/oss/202103/25/e785f9fa5a34bf93251bf2a0639e466d.jpg)

​	由于浮点数是通过分数计算出来的，所以它表示的数仅是近似值。完整的转换算法不在本书的讨论范围之内，我们只介绍其基本表示形式。

​	double表示双精度浮点值[SAVARD05]。大多数编程语言都在标准库中内置了编码和解码浮点数的方法。

### 3.2.2 字符串和变长数据

​	所有原始数值类型都有固定的大小。构造更复杂的值非常类似于C语言的struct<sup>注2</sup>，你可以将原始值组合到结构体中，并使用固定长度的数组或用指针指向其他内存区域。

​	字符串和其他变长数据类型（比如定长类型的数组）可以序列化为一个表示长度的数值字段size再加上size个字节，后面的这些字节是实际的数据。对于字符串来说，这样的表示形式时常称为*UCSD*字符串或*Pascal*字符串（以Pascal编程语言的流行实现而命名）。其伪代码表示如下：

```pseudocode
String {     
	size uint_16     
	data byte[size] 
} 
```

​	Pascal字符串的一种替代是以空（null）结尾的字符串，这时，读取方逐个字节地读出字符串，直到遇到结束符为止。相比之下，Pascal字符串表示有以下几个优点：可在常数时间内获取字符串的长度而无须遍历整个字符串；在某些特定语言的实现中，可以通过直接使用内存中size个字节的切片并将字节数组传入字符串构造函数的方式来构造字符串。

### 3.2.3 按位打包的数据：布尔值、枚举值和标志

​	布尔值可以用单个字节来表示，也可以将true和false分别编码为1和0。由于布尔值只有两个取值，用一个完整的字节太浪费了，所以开发者常常会将每8个布尔值合成一批，每个布尔值只占一位。我们称1所在的比特位为已置位的，0所在的比特位为未置位的或空位。

​	枚举值（enum）可以被表示为整数，常被用于二进制格式和通信协议。枚举值用于表示重复多、基数少的值。例如，我们可以用枚举值表示B树节点类型：

```pseudocode
enum NodeType { 
   ROOT,     // 0x00h 
   INTERNAL, // 0x01h 
   LEAF      // 0x02h 
}; 
```

​	另一个密切相关的概念是标志，它是打包的布尔值和枚举值的组合。标志可以表示多个非互斥的布尔值参数，例如，可以用它表示页是否包含值单元、值是定长的还是变长的、是否存在与当前节点相关联的溢出页。由于每个比特位都代表一个标志值，所以我们只能将2的幂用作掩码（这是因为二进制下2的幂总是只有一位为1，例如2<sup>3</sup> \== 8 \== 1000b、2<sup>4\</sup> \== 16 \== 0001 0000b等）。

```pseudocode
int IS_LEAF_MASK         = 0x01h; // bit #1 
int VARIABLE_SIZE_VALUES = 0x02h; // bit #2 
int HAS_OVERFLOW_PAGES   = 0x04h; // bit #3 
```

​	像布尔值一样，我们可以利用位掩码和位运算符从打包的值中读写各个标志位。例如，要想将其中某个标志置为1，可以将它和相应的位掩码按位求或（|），也可以用移位运算（<<）和位的下标代替位掩码。要想将某一位置为0，可以利用按位与（&）和按位取反运算符（~）。为了测试第n位是否被置位，可以将按位与的结果和0进行比较。

```pseudocode
// Set the bit 
flags |= HAS_OVERFLOW_PAGES; 
flags |= (1 << 2); 
 
// Unset the bit 
flags &= ~HAS_OVERFLOW_PAGES; 
flags &= ~(1 << 2); 
 
// Test whether or not the bit is set 
is_set = (flags & HAS_OVERFLOW_PAGES) != 0; 
is_set = (flags & (1 << 2)) != 0; 
```

## * 3.3 通用原理

​	**通常，在设计一种文件格式时，<u>首先要确定寻址方式</u>：是否要将文件拆分为相同大小的页、哪些页由单个块或多个连续块所组成**。

​	<u>大多数原地更新的存储结构都使用相同大小的页，从而大大简化了读取和写入访问</u>。仅追加(append-only)的存储结构通常也按页写入数据：记录被一条一条地追加上去，一旦内存中该页写满了，就将其刷写到磁盘上。

​	文件通常以定长的头部(header)开始，可能以一个定长的尾部(trailer)结束。尾部包含需要被快速访问的辅助信息或解析文件其余部分所必要的信息。文件的其余部分被分成多个页，图3-3展示了文件的大致组织方式。

![img](https://s4.51cto.com/oss/202103/25/52f6fecfbe57b3f21d88f4af98bdba5c.jpg)

​	许多数据库的表结构（schema）是固定的，其指定了表中字段的数量、顺序和类型。**固定的表结构有助于减少磁盘上存储的数据量：只需使用位置标识符而不用让每条记录都带上字段名**。

​	假如我们要为公司名册设计一个格式，以保存每个员工的姓名、生日、税号和性别，则有几种方案可以选用。我们可以将定长字段放在结构体头部，接着是变长字段：

```pseudocode
定长字段： 
| (4字节) employee_id                | 
| (4字节) tax_number                 | 
| (3字节) date                       | 
| (1字节)  gender                    | 
| (2字节) first_name_length          | 
| (2字节) last_name_length           | 
 
变长字段: 
| (first_name_length字节) first_name | 
| (last_name_length字节) last_name   | 
```

​	现在，若要访问first_name，则我们可以在定长区域之后切片出first_name_length个字节。若要访问last_name，则我们首先要算出在此之前的所有变长字段的总长度，以找到last_name的起始位置。为了避免涉及多个字段的计算，我们可以将偏移量和长度都编入定长区域，这样就可以独立地定位任何一个变长字段。

​	构建更复杂的结构常常会涉及层次结构：原始类型构成字段，一组字段构成单元，多个单元组成页，页组成段，段组成区域，等等。没有必须遵守的严格规则，一切都取决于你要为怎样的数据创建格式。

​	**数据库文件通常由多个部分组成，并且在该文件的头部、尾部或另一个单独的文件中包含一个查找表，记录了这些部分的起始偏移量**。

## 3.4 页的结构

​	<u>数据库将数据记录存储在数据文件和索引文件中。这些文件被划分为固定大小的单元，称为页。页大小通常是文件系统块的整数倍，一般是4～16KB</u>。

​	让我们看一个磁盘上B树节点的例子。从数据结构的角度来看，在B树中，我们区分了叶节点（包含键和数据记录对）与非叶节点（包含键和指向其他节点的指针）。每个B树节点占据一个页或多个链接在一起的页，因此在讨论B树时，“节点”和“页”（甚至“块”）这几个术语经常可以互换使用。

​	原始的B树论文[BAYER72]描述了一种简单的、用于定长数据记录的页组织方式，每个页仅是一连串的三元组，如图3-4所示：k表示键，v表示相关联的值，p表示指向子页的指针。

![img](https://s4.51cto.com/oss/202103/25/edb87cc80fcbaf782ca12fcfeea7295c.jpg)

​	这个方案很容易实现，但是有一些缺点：

+ 除非是在最右侧，否则向其他位置插入一个键需要移动已有的元素。

+ 无法有效地管理或访问变长记录，只适用于定长的数据。

## 3.5 分槽页

​	**当存储变长记录时，主要的问题是如何管理可用空间**：已删除记录所占用的空间需要被回收。如果我们把大小为n的记录放进大小为m的记录先前占用的空间里，那么除非m == n或再找到一个大小恰好为m – n的记录，否则仍然会有空余的空间。类似地，一个大小为m的段无法存放大小为k（k大于m）的记录，因此这条记录会被插入其他位置，而不会回收这些未使用的空间。

​	为了简化变长记录的空间管理问题，我们可以将页分成固定大小的段。但这样做最终也会浪费一些空间。例如，如果我们使用64字节的段，那么除非记录长度恰好是64的倍数，否则我们将浪费64 - (n mod 64)个字节，这里n表示记录长度。换句话说，除非记录长度是64的整数倍，否则总有一个块仅被部分填充。

​	<u>空间回收可以通过简单地重写页并移动记录来完成，但是需要保证记录的偏移量不变，因为页外的指针会用到这些偏移量</u>。在做到这一点的同时，我们希望尽可能减少空间浪费。

总之，我们需要一种页格式，它允许我们：

+ 以最小的开销存储变长记录

+ 回收已删除记录所占用的空间

+ 引用页中的记录，无论这些记录具体在什么位置

​	为了高效地存储变长记录，例如字符串、二进制大对象（BLOB）等，我们可以使用一种称为分槽页（slotted page）的技术，即分成很多槽（slot）的页[SILBERSCHATZ10]，或槽目录（slot directory）技术[RAMAKRISHNAN03]。这些技术被用在许多数据库中，比如PostgreSQL。

​	我们将页组织成一个槽或单元格（cell）的集合，并将指针和单元格分别存放在页两侧的独立内存区域中。若想保持记录原来的顺序，我们只需要重新组织指向单元格的指针；若要删除一条记录，我们只需将记录的指针置为空或删除指针即可。

​	分槽页具有一个固定大小的头部，其中包含关于页和单元格的重要信息（参见4.1节）。单元格的大小可能各不相同，并且可以容纳任意数据：键、指针、数据记录等。图3-5展示了分槽页的页面组织方式，其中每个页都有一个维护区域（头部）、单元格和指向它们的指针。

![img](https://s3.51cto.com/oss/202103/25/562c7af0313ec36f0fba55e623431115.jpg)

​	让我们来看看上述方法是如何解决本节开头提到的问题的。

+ 最小开销：分槽页唯一的额外开销是一个指针数组，用于保存记录实际所在位置的偏移量。

+ 空间回收：通过对页进行碎片整理和重写，就可以回收空间。

+ **动态布局：从页外部，只能通过槽ID来引用槽，而确切的位置是由页内部决定的**。

## 3.6 单元格布局

​	有了标志位、枚举值和原始类型，我们就可以开始设计单元格布局了。之后将单元格组合成页，再将页组合成树。单元格分为键单元格和键值单元格两种。

+ 键单元格包含一个分隔键和一个指针，该指针指向两个相邻键之间的页。
+ 键值单元格包含键和相关联的数据记录。

​	我们假定单个页内所有单元格是统一的（例如，要么全是键单元格，要么全是键值单元格；类似地，要么全都包含定长数据，要么全都包含变长数据，但不能是二者的混合）。这样一来，单元格的元数据只要在每个页上保存一份即可，而不用让每个单元格都保存一份。

​	构成一个键单元格需要以下信息：

+ 单元格类型（可以从页的元数据推断出来）

+ 键的长度

+ 该单元格指向的子页的ID

+ 键的数据（以字节表示）

​	一个变长键的单元格布局可能看起来像这样（对于定长键，不用在每个单元格上保存键的长度）：

```pseudocode
0                4               8 
+----------------+---------------+-------------+ 
| [int] key_size | [int] page_id | [bytes] key | 
+----------------+---------------+-------------+ 
```

​	我们将定长的字段放在一起，之后是key_size个字节。严格来说这不是必需的，但这可以简化偏移量的计算，因为所有定长字段都可以通过静态的、预先计算好的偏移量来访问，我们只需要为变长数据计算偏移量。

​	键值单元格存放数据记录而非子页ID。除此以外，它和键单元格的结构类似：

+ 单元格类型（可以从页的元数据推断出来）

+ 键的长度

+ 值的长度

+ 键的数据（以字节表示）

+ 数据记录的数据（以字节表示）

```pseudocode
0              1                5 ... 
+--------------+----------------+ 
| [byte] flags | [int] key_size | 
+--------------+----------------+ 
 
5                  9                    .. + key_size 
+------------------+--------------------+----------------------+ 
| [int] value_size |     [bytes] key    | [bytes] data_record  | 
+------------------+--------------------+----------------------+ 
```

​	你可能已经注意到了偏移量和页ID之间的区别。由于页大小是固定的，并且页由页缓存来管理（参见5.1节），我们只需要存储页ID即可，使用时再通过查找表将其转换成真正的文件偏移量。单元格偏移量是页局部的概念，它是相对于页起始位置的偏移量：因此，我们可以使用比较小的整数基数，从而使格式更紧凑。

---

+ 变长数据 

  ​	单元格中的键和值不一定是定长的，它们都可以是变长的，其位置可以通过偏移量从定长的单元格头部计算得到。 

  ​	要找到一个键，我们需要跳过单元格的头部，读取key_size个字节。类似地，要找到一个值，我们可以跳过单元格的头部再加上key_size个字节，然后读取value_size个字节。

  ​	有多种方法可以做到这一点，例如：先保存总长度，再用减法算出值的长度。这些方法都可以归结为一点—要有足够的信息将单元格切分成子部分，并从中重建出编码的数据。

## 3.7 将单元格放进分槽页

​	要把单元组织成页，我们可以使用在3.4节中讨论过的分槽页技术。我们将单元格追加到页的右侧（从尾向头），并将单元格偏移量/指针放在页的左侧，如图3-6所示。

![img](https://s6.51cto.com/oss/202103/25/e7d2b32794f292fd113b2744a2990c10.jpg)

​	键可以不按顺序插入，其逻辑上的顺序是通过维护偏移量指针的顺序来实现的。这种设计使得向页中追加单元格只需要最低限度的工作量，因为无论是在插入、更新还是删除操作中，始终都不需要移动单元格的位置。

​	假设有一个页用于保存名字。Tom和Leslie这两个名字先后被加入页中，从图3-7可以看出，这两个名字的逻辑顺序（在本例中是字典顺序）与插入顺序（追加到页中的顺序）不同。单元格按插入顺序排列，但偏移量被重新排序了，便于使用二分查找。

![img](https://s2.51cto.com/oss/202103/25/d68d773150d714b2d008f51e4abb2a74.jpg)

​	现在，我们想要在该页上再加一个名字：Ron。新数据被追加到页可用空间的末端，但是偏移量指针必须保持字典顺序：Leslie、Ron、Tom。为此，我们必须将偏移量重新排序：将插入点之后的指针向右移动，从而为指向Ron单元格的新指针腾出空间，如图3-8所示。

![img](https://s6.51cto.com/oss/202103/25/a3df09d1b8edb2b390ea60275198cd09.jpg)

## * 3.8 管理变长数据

​	从页删除一条记录不用删除实际的单元格，也不用移动其他单元格以重用这些释放的空间。相反，可以将这个单元格标记为已删除，并根据被释放内存的大小以及指针更新内存中的可用列表。可用列表保存了可用段的偏移量及其大小。每当插入新单元格时，我们首先检查可用性列表，看看是否有能放得下的段。图3-9展示了一个带有可用空间碎片的页的例子。

​	**SQLite把未被占用的段称为空闲块（freeblock），并将指向第一个空闲块的指针保存在页头部。此外，页上还保存了可用字节总数，用来快速检查新元素能否在碎片整理后被放入该页中**。

![cde1a9c3de72ba41690357262a8e793c.jpg (518×190)](https://s2.51cto.com/oss/202103/25/cde1a9c3de72ba41690357262a8e793c.jpg)

​	至于具体要使用哪一个空闲块，是通过以下策略计算的：

+ 首次适配优先

  这种方法可能会造成较大的额外开销，因为当我们把数据填进第一个合适的段之后，剩余的空间可能不够放下其他的单元格，因而被浪费掉了。

+ 最佳适配优先

  在最佳适配中，我们尝试寻找一个段，使得插入之后段内剩余的空间最小。

​	如果找不到足够长的连续字节来存放新的单元格，但有足够多的碎片字节可用，我们会读出所有存活的单元格再重新写入，即对页进行碎片整理，以回收空间来留给新的写入。如果在碎片整理之后依然没有足够的可用空间，我们就要创建一个溢出页（参见4.1.5节）。

 	**为了提高局部性（尤其是当键比较短的时候），某些实现会在叶节点上将键和值分开保存。**

​	**<u>将键放在一起可以改善搜索过程中的局部性</u>**。当定位到要查找的键之后，用对应的下标可以找到它的值单元格。如果是变长的键，则我们还要计算并保存一个额外的值单元格指针。

 	总结一下，为了简化B树的布局，我们假设每个节点都占据单个页。页由定长的头部、单元格指针块和单元格构成。单元格包含键和指向子节点或关联的数据记录的指针。B树使用简单的指针层级结构：页标识符用于在树文件中定位子节点，而单元格偏移量用于在页内定位单元格。

## 3.9 版本

​	随着数据库系统的不断演进，开发人员不断加入新功能并修复bug和性能问题。其后果是，二进制文件的格式也会发生变化。大多数时候，任一版本的存储引擎都要支持不止一种序列化格式（比如，当前的格式加上一种或多种旧的格式，以实现向后兼容）。为此，我们必须能够确定当前文件是什么版本。

​	有几种方式都能做到这一点。例如，Apache Cassandra中的文件名带有版本前缀，这样不用打开文件就可以知道文件的版本。从4.0版本开始，数据文件名带有“na”前缀，例如na-1-big-Data.db。旧版本文件则带有不同的前缀：3.0版本的文件带有“ma”前缀。

​	或者，可以将版本存储在单独的文件中。比如PostgreSQL将版本存储在PG_VERSION文件中。

​	版本也可以直接存储在索引文件的头部。在这种情况下，头部的一部分（或整个头部）必须用一种不随版本变化的格式进行编码。在找出文件编码的版本之后，我们可以创建出特定版本的读取器来解析文件内容。某些文件格式使用魔数（magic number）标识版本，我们将在4.1.1节中详细讨论。

## * 3.10 校验和

​	磁盘上的文件可能会由于软件错误或硬件故障而损坏。为了尽早识别这些问题，避免将损坏的数据传播到其他子系统甚至节点，我们可以使用校验和（checksum）以及循环冗余校验（CRC）。

​	一些资料中并不区分加密与非加密哈希函数，以及CRC与校验和。它们都是将一大块数据归纳为一个数字，但它们的使用场合、目的和提供的保证是不同的。

​	校验和是最弱的保证形式，它不能检测多个比特位的损坏。通常，校验和是用XOR结合奇偶校验或求和来计算的[KOOPMAN15]。

​	CRC可以帮助检测突发错误（比如多个连续比特位的损坏），其通常使用查找表和多项式除法来实现[STONE98]。多位错误的检测至关重要，因为大部分在通信网络和存储设备中发生的故障都是以这种方式呈现的。 

​	非加密哈希和CRC不应当用于验证数据是否已被篡改。对于这类场景，请务必使用专为安全性设计的强加密哈希。<u>CRC的主要目标是确保数据没有非人为的、意外的变化，而非用于抵御攻击或人为的修改</u>。

​	在将数据写入磁盘之前，我们计算其校验和并将它与数据一同写入。当读取数据时，我们重新计算校验和并把它与之前写入的校验和进行比较。如果校验和不匹配，我们就知道发生了损坏，因而不应当再使用这些数据。 

​	**由于计算整个文件的校验和通常是不切实际的，而且不太可能每次访问文件都读取全部内容，所以校验和通常是针对每个页计算的，并保存在页头部**。<u>这样一来，校验和可以更健壮（因为仅针对一小部分数据），而且就算单个页发生损坏，我们也不用丢弃整个文件</u>。

> [CRC（循环冗余校验）_百度百科 (baidu.com)](https://baike.baidu.com/item/CRC/1453359?fr=aladdin)

## 3.11 本章小结

​	在本章中，我们介绍了二进制数据的组织方式：如何序列化原始数据类型、将它们组合成单元格、用单元格构建出分槽页，以及如何读写这些数据结构。

​	我们介绍了如何处理变长数据类型（例如字符串、字节序列和数组），以及构造为它们特殊设计的包含值长度的单元格。

​	我们讨论了分槽页的格式，它允许我们通过单元格ID从页外部引用某个单元格。在分槽页中，我们可以按插入的次序存储记录，并通过对单元格偏移量进行排序来维持键的顺序。

​	这些原理可用于各种二进制格式，包括构建磁盘数据结构和网络协议。

# 4. B树的实现

​	在上一章中，我们讨论了二进制格式的一般原理，并学习了如何创建单元格、构建层次结构以及如何使用指针将它们与页进行连接。这些概念对原地更新和仅追加存储结构都适用。在本章中，我们将讨论一些B树特有的概念。

​	本章各节在逻辑上分为三组。首先，我们将讨论数据组织:如何建立键和指针之间的关系，以及如何实现页头和页之间的链接。

​	接下来，我们将讨论从根节点到叶子节点的整个下降过程：如何执行二分搜索，以及如何收集相关内容并跟踪父节点，以备后续发生节点的分裂或合并时使用。

​	最后，我们将讨论优化技术(再平衡、仅在右边追加和批量加载)、维护过程和垃圾收集。

## 4.1 页头

​	页头保存有关可用于页定位、维护和优化的信息。它通常包含描述页内容和布局的标志位、页中单元格的数量、标记空闲空间的上界与下界偏移量(用于追加单元格偏移量和数据)以及其他有用的元数据。

​	例如， PostgreSQL会在头部储存页大小和布局版本。在 MySQL InnoDB中，页头保存该页中的记录总数、层数和其他一些与实现相关的值。在 SQLite中，页头存储单元格的数量和最右指针。

### 4.1.1 魔数

​	文件或页头中经常放置的一个值是魔数。通常，它是一个多字节块，包含一个常数值表示该块代表一个页，并指定页的类型或标识其版本。

​	魔数通常用于校验和完整性检査[GIAMPAOLO98]。随机偏移量下的字节序列与魔数完全匹配的概率很小。如果匹配，则偏移量很可能是正确的。例如，为了验证页加载和对齐是否正确，在写入期间，我们可以将魔数`50 41 47 45`(“PAGE”(页)的十六进制表示)放入头部。在读取过程中，我们通过把从头部读取的四个字节与预期的字节序列进行比较来校验页。

### 4.1.2 同级指针

​	一些实现存储了前序和后继指针，指向左、右同级页。这些指针有助于定位相邻节点，而不必返回父节点。这种方法为分裂和合并操作增加了一些复杂度，因为同级偏移量也必须同时更新。例如，当一个非最右边的节点被拆分时，其同级右节点的后向指针(之前指向被拆分的节点)必须被重新绑定以指向新创建的节点。

​	要定位一个同级节点，除非有指针指向同级节点，否则我们必须通过父节点才能定位。这个操作可能一直上升到根节点，因为直接父节点只能帮助找到它自己的子一级节点。如果在页头中存储直接连接同级节点的指针，我们便可以方便地通过这些指针来定位同一层上的前序或后继节点。

​	存储同级指针的缺点之一是在分裂和合并期间必须更新它们。由于更新必须在同级节点中进行，而不是在分裂/合并节点中进行，所以可能需要额外的锁。我们将在5.3.8节中讨论同级指针在并发B树的实现中是如何发挥作用的。

### 4.1.3 最右指针

​	B树分隔键有严格的不变式：它用于将树拆分为子树并对这些子树进行遍历，因此指向子页的指针总是比指向键的指针多一个。这就是2.3.5节中提到的+1的来源。

​	在2.3.2节中，我们描述了分隔键不变式。在许多实现中，每个分隔键都有一个子指针，而最后一个指针则单独存储，因为它并没有与任何键相匹配。

​	这个额外的指针可以存储在头部，例如，其在 SQLite中的具体实现便是如此。

​	如果最右侧的子指针被拆分，并且新的单元格被追加到其父节点上，则必须重新分配最右子指针。拆分后追加到父节点的单元格(灰色)包含被提升的键，并指向拆分节点。此时分配了一个指向新节点的指针来替代原先的最右指针。 SQLite描述并实现了类似的方法。

### 4.1.4 节点的高键

​	我们可以采用一种稍微不同的方法，将单元格中的最右指针与节点的高键(high key)存储在一起。节点的高键表示当前节点下的子树中可能存在的最高的键。 PostgreSQL使用了这种方法，称为 Blink树(有关此方法隐含的并发含义，请参见5.3.8节)。

### * 4.1.5 溢出页

​	节点大小和树扇出是固定且不会动态改变的。我们也很难找到一个普遍最优的值：如果树中存在变长值，并且它们足够大，那么页中只能放下少数几个值；如果值很小，我们最终会浪费保留的空间。

​	B树算法规定每个节点持有特定数量的元素。由于某些值具有不同的大小，所以根据B树算法，可能会出现这样一种情况：节点尚未满，但保存该节点固定大小的页上已经没有更多的可用空间了。调整页大小需要将已经写入的数据复制到新区域，这通常是不切实际的。但我们仍然需要找到一种方法来增加或扩展页大小。

​	**为了实现变长节点而无须将数据复制到新的连续区域，我们可以从多个链接起来的页中构建节点**。

​	例如，默认页大小为4K，而在插入几个值之后，其数据大小增长到4K以上。此时我们并不使用任意大小的页，而是允许节点以4K为增量进行增长，因此我们可以分配一个4K的扩展页，并从原始页链接到它。

​	**这些链接起来的页被称为溢出页(overflow page)**。为简洁起见，在本节中，我们将原始页称为主页(primary page)。

​	**大多数B树的实现最多只允许在节点中直接存储某个固定字节数量的载荷数据，并将其余字节溢出到溢出页**。这个数值是通过将节点大小除以扇出得到的。使用这种方法，我们根本不会遇到页面没有可用空间的情况，因为页面总是至少具有max_payload_size个字节。有关SQLite中溢出页的更多信息，请参见SQLite源代码存储库和MySQL InnoDB文档。

​	当插入的数据大于max_payload_size时，检查节点是否已经具有任何相关联的溢出页。如果溢出页已经存在并且有足够的可用空间，则要写入的数据中的额外字节将会溢出到该溢出页中；否则，将分配新的溢出页。

​	一个主页和一个溢出页，记录从主页指向溢出页，数据在它们之上是连续的。

​	**溢出页需要一些额外的记录，因为它可能会像主页一样变得碎片化，而我们必须能够通过回收空间来写入新的数据，或者在不需要溢出页时丢弃它**。

​	<u>当分配第一个溢出页时，其页ID存储在主页的头部。如果单个溢出页不够，则通过在前一个溢出页的头部保存下一个溢出页的ID的方式，将多个溢出页链接在一起。要查找给定的数据，我们可能不得不遍历多个溢出页</u>。

​	由于键通常具有很高的基数，所以只存储键的一部分是有意义的，因为大多数比较可以通过驻留在主页中的部分截断的键来进行。

​	<u>对于数据记录，我们必须定位其溢出部分，以便将其返回用户。然而，因为这是一个不那么频繁的操作，所以问题并不大</u>。

​	**如果所有的数据记录都过大，那么值得考虑采用针对长值的blob存储**。

## 4.2 二分搜索

## 4.3 传播分裂与合并

​	<u>正如我们在前几章中讨论过的，B树的分裂与合并可以传播到更高的层。为此，我们需要一条从将要分裂或合并的叶节点遍历回根节点的路径。</u>
​	<u>B树节点可以包含父节点指针。由于在通过较高层的页引用较低层的页时，一定会触发页换入，所以我们甚至没有必要将这些信息保存在磁盘上。</u>

​	就像同级指针(参见4.1.2节)一样，每当父节点发生变化，父节点指针就必须更新。当带有页标识符的分隔符键从一个节点转移到另一个节点时，比如父节点分裂、合并或再平衡时，都需要进行此更新。

​	一些实现(例如WiredTiger)将父指针用于叶节点的遍历以避免死锁，**死锁可能在使用同级指针时发生**(参见[MILLER78]、[LEHMAN81])。该算法不使用同级指针遍历叶节点，而是使用父指针，就像我们在图4-1中看到的那样。

​	要寻址和定位同级节点，我们可以递归地跟随父节点的指针。此时，搜索就会递归地向上继续，最终到达根节点，并继续向下返回到叶子层。

---

+ 导航信息

  ​	如果不存储和维护父节点指针，则我们还可以跟踪在遍历目标叶节点路径上的节点，并在发生级联分裂(插入时)或合并(删除时)的情况下，以相反的顺序跟踪父节点链路。

  ​	在进行可能导致B树结构变化的操作(插入或删除)时，我们首先从根节点到叶节点遍历树，找到目标节点或插入点。由于我们并不一定预先知道操作是否会导致分裂或合并(至少在找到目标叶节点之前不会)，所以必须收集导航信息(breadcrumb)。

  ​	<u>导航信息包含从根节点一路下来的引用，用于在传播分裂或合并时进行反向回溯</u>。对此，最自然的数据结构是栈。例如，PostgreSQL将导航信息存储在栈中，内部称之为BTStack。

  ​	如果节点被分裂或合并，则可以使用导航信息来为上拉到父节点的键查找插入点，并在必要时沿着树向上走，以将结构更改传播到更高层的节点。这个栈一般在内存中维护。

## 4.4 再平衡

​	一些B树的实现方案试图推迟分裂和合并操作，以便通过在层内再平衡各元素来平摊代价，或将元素从占用较多的节点转移到占用较少的节点中，通过这一方式尽可能推迟分裂或合并。虽然维护代价可能更高一些，但这有助于提高节点利用率以及减少树的层数。

​	我们可以在插入和删除操作期间执行负载平衡[GRAEFE1]。为了提高空间利用率，我们并不在溢出时拆分节点，而是可以将一些元素转换到同级的另一个节点中，并为插入腾出空间。类似地，在删除过程中，我们可以选择从相邻节点中移动一些元素，而不是合并同级节点，以确保节点至少是半满的。

​	B*树持续在相邻节点之间分发数据，直到两个同级节点都已满[KNUTH98]。另外，该算法不是将单个节点拆分为两个半空节点，而是将两个节点拆分为三个节点，每个节点是三分之二满的。 SQLite在实现中使用此变体。<u>这种方法通过推迟分裂来提高平均占用率，但需要额外的跟踪和平衡逻辑。而更高的利用率也意味着更高效的搜索，因为树的高度更低，通往被搜索叶节点的路径上的页也更少</u>。

​	**负载平衡**是许多数据库实现中运用的一种有用技术。例如， SQLite实现了同级节点平衡算法，该算法与我们在本节中描述的算法有些接近。平衡可能会给代码增加一些复杂度，但是因为它的用例是隔离的，所以可以将此技术作为一种优化手段在后续进行实现。

## 4.5 仅在右侧追加

​	许多数据库系统使用单调自增的数值作为主索引的键。这为优化创造了机会，因为所有的插入都发生在索引的末尾(在最右边的叶子中)，所以大多数分裂发生在每层的最右节点上。此外，由于键是单调递增的，所以考虑到追加相对更新和删除的比例很低，相对于键随机排列的情况而言，非叶页上的碎片化程度更低。

​	PostgreSQL称这种情况为**快速路径(fastpath)**。当插入的键严格地大于最右页中的第个键，并且最右页有足够的空间来容纳新插入的条目时，新条目被插入缓存的最右叶中的适当位置，并且可以跳过整个读取路径。

​	SQLite也有类似的概念，称为**快速平衡(quickbalance)**。当条目插入最右端并且目标节点已满(即它在插入时成为树中最大的条目)时，它不再平衡或拆分节点，而是分配新的最右节点并将其指针添加到父节点(关于在 SQLite中实现平衡的更多信息，请参见44节)。即使这会使新创建的页面几乎为空(而不是在节点分裂的情况下的一半为空)，节点也很可能很快就会被填满。

---

+ 批量加载

  ​	如果我们已经预先对数据进行了排序，并且想要批量加载(bulk load)它们，或者不得不进行树的重建(例如，为了碎片整理)，那么可以进一步拓展仅在右侧追加的技术。由于创建树所需的数据已经被排序，所以在批量加载过程中，我们只需要将数据项追加到树中最右边的位置。

  ​	在这种情况下，我们可以完全避免分裂和合并，并自下而上地构建树，逐层写出，或者在有足够的指针指向已经写出的低层节点时立即写出高层节点。

  ​	实现批量加载的一种方法是在叶子层上按页写入预排序数据(而不是插入单个元素)。在叶子页被写入之后，我们将它的第一个键传播给父页，并使用一个标准算法来构建更高的B树层级[RAMAKRISHNAN03]。由于追加的键是按顺序给出的，所以所有的分裂都发生在最右节点上。

  ​	由于B树总是从底(叶子)层开始构建，所以在开始构建较高层的节点之前，我们可以写出完整的叶子层。这使得在构造更高层节点时所有子指针都已经准备好了。这种方法的主要好处是，我们不必在磁盘上执行任何分裂或合并，同时，在构造时我们只需在内存中保留最小部分的树(即当前填充叶节点的所有父节点)。

  ​	不可变B树可以用同样的方式创建，但与可变B树不同的是，它不需要为后续的修改预留空间，因为树上的所有操作都是最终的(final)。所有页都可以被完全填满，这提高了页面占用率并达到了更好的性能。

## 4.6 压缩

​	存储原始的、未压缩的数据会导致显著的开销，因此许多数据库提供了压缩数据的方法以节省存储空间。这里显然是要在访问速度和压缩率之间进行权衡：**较大的压缩率可以提升数据存储空间利用率，允许你在一次访问中获取更多的数据，但这可能需要更多的内存和CPU周期来进行压缩和解压缩**。

​	我们可以在不同的粒度级别上进行压缩。尽管压缩整个文件可以产生更好的压缩率，但由于整个文件必须在更新时被重新压缩，所以它的应用有限。更细粒度的压缩通常更适合较大的数据集。<u>压缩整个索引文件既不切实际也难以高效实现：为了寻址一个特定页，必须访问整个文件或其包含压缩元数据的段(以便定位一个已压缩的段)，然后将其解压缩并使其可用</u>。

​	另一种选择是**按页压缩数据**。它很适合我们所讨论的情形，因为到目前为止我们讨论的算法都使用固定大小的页。页可以独立地进行压缩和解压缩，从而允许你将压缩与页的加载和刷盘耦合起来。然而，这样一来，被压缩的页只能占用磁盘块的一部分。并且，由于传输通常以磁盘块为单位进行，所以可能需要换入额外的字节[RAY95]。在图4-10中，你可以看到压缩页a)占用的空间小于磁盘块，当我们加载该页时，我们还会换入属于另页的额外字节。对于跨越多个磁盘块的页，如图4-10b，我们必须读取一个附加的块。

​	<u>另一种方法是**仅压缩数据**，要么按行(压缩整条数据记录)，要么按列(对每一列进行压缩)。在这种情况下，页的管理和压缩是解耦的</u>。

​	在撰写本书时参考的大多数开源数据库都使用了可插拔的压缩方法，其使用现成的库例如 Snappy、zLib、lz4等。

​	由于压缩算法会根据数据集和潜在目标(例如压缩比、性能或内存开销)而产生不同的结果，所以在本书中我们将不再进行深入的比较和对实现细节的讨论。针对不同的块大小评估不同的压缩算法的综述有很多(例如 Squash Compression Benchmark)，其通常集中在四个衡量指标上：<u>内存开销、压缩性能、解压缩性能和压缩比。在选择压缩库时这些指标是非常重要的</u>。

## 4.7 清扫与维护

​	之前描述过的分槽页的设计(参见3.5节)需要在页上执行维护操作，以维持其良好的形态。例如，内部节点中后续的分裂和合并，或叶子层上的插入、更新和删除，可能导致页中有足够的逻辑空间但没有足够的连续空间，因为它已经碎片化了。图4-11展示了这种情况的一个例子:页中仍然有一些可用的逻辑空间，但是它是碎片化的并且被两个已删除的(垃圾)记录分割开来，在头部/单元格指针和单元格之间也有一些剩余的可用空间。

​	<u>B树的遍历始于根节点。如果一条数据记录可以通过从根节点向下追踪指针到达，那么称它为存活的(即可寻址的)。而不可寻址的数据记录则称为垃圾数据：这些记录没有被任何地方引用，也不能被读取或解释，因此它们的内容是作废的</u>。

​	你可以在图4-11中看到这一区别：仍然具有指向它们的指针的单元格是可寻址的，而这便与已移除或覆盖的单元格不同。由于性能，通常不会对垃圾区域进行填零操作，因为这些区域最终还是会被新数据所覆盖。

### 4.7.1 更新和删除导致的碎片

> ​	有些数据库依赖于垃圾收集，并将已删除和更新的单元格留在原位，以便进行多版本并发控制(参见5.3.6节)。在更新完成之前，单元格对于并发执行的事务保持可访问性，一旦没有其他线程访问它们，就可以立即回收单元格。一些数据库维护跟踪幽灵记录(ghost record)的数据结构，一旦所有可能已经看到它们的事务完成，这些幽灵记录就会被回收[WEIKUM01]。

​	由于删除操作仅丟弃单元格偏移量，而非搬运剩佘单元格或物理地移除目标单元格以释放占用的空间，所以释放的字节可能最终分散在整个页上。在这种情况下，我们说页是碎片化的(fragmented)，需要进行碎片整理。

​	要进行写操作，我们通常需要可以放入单元格的一块连续的空闲区域。要将空闲碎片重新组合在一起来满足写入的需要，我们必须重写页面。

​	插入操作保留元组的插入顺序。这对空间碎片并没有显著的影响，但是如果将元组按顺序排列，则将有助于在顺序读取时进行高速缓存预取。

​	更新操作主要适用于叶子层：内部页的键多用于引导导航，其仅定义了子树的边界。此外，更新是在每个键的基础上执行的，除非创建溢出页，否则这通常不会导致树中的结构更改。然而，在叶子层上，更新操作不会改变单元格顺序，并会试图避免页重写。这意味着，虽然单元格的多个版本中只有一个能被寻址，但是可能最后所有版本都被保存了。

### 4.7.2 页的碎片整理

​	负责空间回收和页重写的过程称为压缩(compaction)、清扫(vacuum)或直接称为维护(maintenance)。如果页中没有足够的物理可用空间，则可以在写入时同步进行页重写(以避免创建不必要的溢出页)，但压缩通常是指一个单独的异步过程，负责遍历页、进行垃圾回收并重写其内容。

​	这个过程回收死亡的单元格所占用的空间，并按逻辑顺序重写单元格。当页被重写时，它也可能被移动到文件中的新位置。未使用的内存页变为可用状态并归还至页缓冲区。新的可用磁盘页的ID被添加到空闲页列表(free page list，有时称为空闲列表(freelist)中。这些信息必须被持久化，才能在节点崩溃和重启时幸存下来，并确保可用空间不会丢失或泄露。

## 4.8 本章小结

​	在本章中，我们讨论了基于磁盘实现B树的相关概念：

+ 页头

  该页中保存的是什么信息。

+ 最右指针

  它们不是与分隔键配对的，以及如何处理它们。

+ 高键

  确定可以存储在节点中的最大的键。

+ 溢出页

  允许你使用固定大小的页来存储超大的、变长的记录。

​	之后，我们又介绍了一些与从根到叶遍历相关的细节：

+ 如何使用间接指针进行二分搜索
+ 如何使用父指针或导航信息跟踪树的层次结构

​	最后，我们介绍了一些优化和维护技术：

+ 再平衡

  在相邻节点之间移动元素，以减少分裂和合并的次数。

+ 仅在右侧追加

  追加新的最右单元格，而不是在假设它会很快被填满的情况下拆分它。

+ 批量加载

  一种从已排序数据中高效地从零开始构建B树的技术。

+ 垃圾收集

  一种重写页、将单元格按键顺序排列，并回收被不可寻址单元格占用的空间的过程。

​	这些概念应该能够弥合B树基础算法与真实世界的实现之间的鸿沟，并帮助你更好地理解基于B树的存储系统是如何工作的。

# 5. 事务处理与恢复

​	在本书中，我们自底向上地介绍了数据库系统中的各种概念：我们首先介绍了存储结构。现在，我们开始介绍负责缓冲区管理、锁管理和恢复的更高层级的组件，这些是理解数据库事务的前提。

​	事务是指数据库中一个不可分的逻辑工作单元，它允许你将多个操作表示为一个步骤事务执行的操作包括读取和写入数据记录。数据库事务必须遵从原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)和持久性(Durability)，这些属性通常合称为ACID[HAERDER83]。

+ 原子性

  事务的步骤是不可分的，这意味着，事务中的所有步骤要么全都执行成功，要么全都不执行。换句话说，事务不应当被部分应用。每个事务要么成功提交(使事务内写操作产生的所有更改变得可见)，要么中止(回滚所有尚不可见的事务副作用)。提交是事务的最后一个操作。中止后可以重试该事务。

+ 一致性

  一致性是一种特定于具体应用的保证：事务只能将数据库从一个有效状态带到另个有效状态，并保持所有的数据库不变量(比如约束、引用完整性等)。<u>一致性是定义最弱的属性，可能因为它是唯一一个由用户控制而不是仅凭数据库自身保证的属性</u>。

+ 隔离性

  多个并发执行的事务应该能够互不干扰地运行，每个事务都像没有其他事务在同时执行一样。隔离性定义了何时以及哪些对数据库状态的更改可以对并发事务可见。出于性能原因，许多数据库使用的隔离级别要弱于这里给出的隔离性定义。根据并发控制的实现，一个事务所做的更改可能对其他并发事务可见或不可见参见5.3.4节)。

+ 持久性

  一旦事务被提交，所有数据库状态的修改都必须被持久化到磁盘上，即使之后发生断电、系统故障或崩溃也不受影响。

​	除了在磁盘上组织和保存数据的存储结构之外，在数据库系统中实现事务还需要多个组件协同工作。在节点内部，事务管理器负责协调、调度和跟踪事务及事务的各个步骤。

​	<u>锁管理器</u>可保护对资源的访冋，并防止那些可能破坏数据完整性的并发访问。毎当请求加锁时，锁管理器会检査这个锁是否已被其他事务以共享或排他模式持有，如果请求的访问级别没有冲突则允许本次访问。由于排他锁在任意时刻最多只能被个事务持有，所以其他请求加锁的事务必须等待锁被释放，或者中止事务并稍后重试。一旦锁被释放或事务终止，锁管理器就会通知其中一个挂起的事务，让它获得锁并继续执行。

​	<u>页缓存</u>充当了持久性存储(磁盘)和存储引擎其余部分之间的中介。它将状态更改暂存在内存中，同时也用于缓存那些尚未与持久性存储同步的页。一切数据库状态的更改都首先被应用在缓存的页上。

​	<u>日志管理器</u>记录了已应用在缓存页上的操作(日志条目)，这些操作尚未与持久性存储同步，而日志可以确保这些操作不会在崩溃时丢失。换句话说，在数据库启动期间，我们利用日志来重新应用这些操作并重建缓存状态。日志条目也可以用来撤销已中止的事务所做的更改。

​	<u>分布式(多分区)事务</u>需要额外的协调机制以及远程执行。我们将在第13章中讨论分布式事务协议。

## 5.1 缓冲区管理

​	大多数数据库都采用双层存储体系：较慢的持久性存储(磁盘)和较快的主內存(RAM)。<u>为了减少对持久性存储的访问次数，页面被缓存在内存中。当存储层再次请求该页时，将返回其缓存副本</u>。

​	假设没有其他进程修改磁盘上的数据，则内存中的缓存页可以被重用。这种做法有时也称为虚拟磁盘[BAYER72]。仅当内存中没有页副本可用时，对虚拟磁盘的读取才会访问物理存储。更多时候，我们称上述机制为页缓存或缓冲池。页缓存负责缓存从磁盘读取的页。如果数据库崩溃或意外关闭，则缓存的内容也将丢失。

> ​	由于页缓存(page cache)这个词可以更好地反映这个结构的目的，所以本书也默认使用这一名称。术语缓冲池(buffer pool)听起来像是主要用来暂存和重用空缓冲区而不共享其中的内容，这可能是页缓存很有用的一部分功能，或者甚至可以作为单独的组件，但这个词不能准确表达出页缓存完整的目的。

​	缓存页的问题不仅限于数据库，操作系统同样具有页缓存的概念。操作系统将空闲的内存段用来透明地缓存磁盘内容，以提高IO系统调用的性能。

​	**将未缓存的页从磁盘加载进来的过程称为换入(page in)。缓存的页一旦被更改过就成了脏页，直到这些更改被刷写(fush)到磁盘上**。

​	由于页缓存的内存区通常比整个数据集小得多，所以页缓存终究会被填满。为了换入新页，必须将其中某个页换出(evict)缓存。

​	页缓存的主要功能可以总结为以下几点：

+ 在内存中保留被缓存的页的内容。
+ 把对磁盘页的修改缓冲起来，并且修改的是缓存的版本。
+ 当被请求的页不在内存中且可用空间足够时，页缓存会将其换入并返回缓存的版本。
+ 如果请求的页在缓存中，则直接返回缓存的版本。
+ **如果可用空间不足以放下新的页，则某些其他页会被换出，被换出的页的内容会被刷写回磁盘**。

> + 绕过内核的页缓存
>
> 很多数据库系统使用O_ DIRECT标志打开文件。该标志允许I/O系统调用绕过内核的页缓存直接访问磁盘，并使用数据库专用的缓冲区管理。
>
> Linus Torvalds批评过O_DIRECT的使用，因为它不是异步的，而且没有预读之类的方法来告知内核有关访问模式的信息。但是，在操作系统提供更好的机制之前，O_DIRECT仍然很有用。
>
> 我们可以通过fadvise这个系统调用从某种程度上控制内核如何将页换出页缓存，但这仅仅是让内核考虑我们的意见，并不保证它一定会发生。为了避免在执行I/O时发生系统调用，我们可以使用内存映射，但那样我们也失去了对缓存的控制。

### 5.1.1 缓存语义

​	所有对缓冲区的更改都保留在内存中，直到最终被写回到磁盘上。由于不允许任何其他进程修改磁盘上的这些文件，所以这个同步过程是单向的：**只能从内存同步到磁盘，而不能反过来进行**。页缓存让数据库能够更好地控制内存管理和磁盘访问。你可以将它视作内核页缓存在应用程序中的等效实现：它直接访问块存储设备，实现类似的功能，也达到了类似的目的。它是对磁盘访问的抽象，并将逻辑写操作与物理写操作分离。

​	<u>页缓存使得我们可以将树部分保留在内存中，而无须修改算法本身或是在内存中物化对象。我们要做的仅是把磁盘访问替换成对页缓存的调用</u>。

​	当存储引擎访问(或请求)页时，我们首先检査其内容是否已被缓存。如果该页在缓存中，则直接返回缓存的页。如果该页未被缓存，则页缓存会将其逻辑地址或页号转换为物理地址，并将它的内容加载到内存，然后返回已缓存的版本给存储引擎。一旦返回，这个存有缓存页内容的缓冲区就称为被引用的(referenced)，存储引擎必须在用完之后将其归还给页缓存或解除引用。若想让页缓存不要换出某些页，则可以将它们固定(pin)。

​	**如果某个页被修改了(例如追加了一个新的单元格)，则该页被标记为脏页。页上的脏标志位表示其内容与磁盘不同步，必须将其刷写到磁盘上才能保证持久性**。

### 5.1.2 缓存回收

​	**如果毎次换出页都刷写磁盘，则其性能可能会很差。因此某些数据库使用单独的后台进程，该进程循环检査可能被换出的脏页，更新其磁盘上的版本**。例如， PostgreSQL中的个后台刷写器就是用来做这件事情的。

​	另一个要记住的重要属性是持久性：如果数据库崩溃，则所有未刷写的数据都会丢失。为了确保所有更改都被持久化，检查点进程会协调刷写进程。检査点进程控制预写日志(WAL)和页缓存，并确保两者协同工作。<u>只有当缓存页完成刷写之后，相关操作的日志记录才能从WAL中丢弃。在上述过程完成后才能将脏页换出缓存</u>。

​	这意味着需要在多个目标间做一些权衡：

+ 推迟刷写以减少磁盘访问次数。
+ 提早刷写以让页能被快速地换出选择要换出的页，并以最优的顺序刷写。
+ 将缓存大小保持在其内存范围内。
+ 避免因数据没有被持久化到主存储中而丢失它们。

​	我们探索的几种技术可以帮助改善前三个特性，同时使我们保持在另外两个特性的范围之内。

### 5.1.3 在缓存中锁定页

​	在B树上，随着节点层次的降低，毎层的节点数量呈指数级增加，并且较高层次的节点仅占整棵树的一小部分，因此这部分节点可以永久驻留在内存中，而其他部分则可以按需换入。这意味着，在执行一次查询时我们不必进行h次磁盘访问(如2.3.3节中所述，h表示树的高度)，只有在访问那些未被缓存的低层页时才会真正读取磁盘。

---

+ 预取和立即换出缓存

  ​	页缓存也允许存储引擎对预取和换出进行细粒度的控制，存储引擎可以指示页缓存在访问页之前提前加载页。例如，当在范围扫描中遍历叶节点时，可以预加载下个叶节点。同样，如果后台维护进程加载了某个页，则可以在进程完成后立即将其换出缓存，因为它不太可能被进行中的查询用到。某些数据库(例如PostgreSQL使用循环缓冲区(即先进先岀的页置换策略)进行大规模顺序扫描。

### * 5.1.4 页置换

+ FIFO、LRU

  ​	最简单的页置换策略是先进先出(FIFO)。FIFO按插入顺序维护一个页ID的队列，将新页添加到队列尾部。每当页缓存已满时，FIFO从队列头部取出元素，这也就是最早被换入的页。由于它不考虑后续的页访问，而是仅考虑页换入时间，所以该策略对于大多数真实系统而言是不切实际的。例如，根节点和最上层节点的页最先被换入，根据此算法，这些页也会被最先换出缓存，尽管从树结构中能明显看出这些页很快就会被再次换入。

  ​	个对FIFO算法的自然扩展是最长时间未使用策略(LRU[ TANENBAUM14]。LRU也按插入顺序维护一个换出候选队列，但是当我们重复访问某个页时，LRU会将其放回队列尾部，就像首次换入时那样。然而，并发环境下每次访问都要更新引用和重新链接节点可能代价较高。此外还有一些基于LRU的缓存换出策略。例如，2Q(双队列LRU)维护两个队列，在初次访问时放入第一个队列，在后续访问时将它们移入第二个热队列，从而区分最近访问的页和经常访问的页[ JONSON94]。LRU-K通过跟踪最近的K次访问来识别频繁用到的页，并使用此信息来估计访问时间[ ONEIL93]。

+ CLOCK

  ​	在某些情况下，效率可能比精度更重要。 CLOCK算法变体常常被用作LRU的一种更加紧凑、更加缓存友好且并发性更好的替代品[SOUNDARARARJAN06]。例如， Linux使用 CLOCK算法的一种变体。

  ​	CLOCK-sweep算法将页的引用和与之关联的访问位保存在环形缓冲区中。一些变体使用计数器而不是比特位来描述频率。每当访问某个页面时，都将它的访问位设置为1。CLOCK算法的工作原理是循环检查环形缓冲区上的访问位：

  + 如果访问位为1且该页未被引用，则将其置为0并检查下一页。
  + 如果访问位已经为0，则将该页作为一个要换出的候选，并安排在后续将其换出。
  + 如果页当前正被引用，则它的访问位保持不变。算法假定被访问页的访问位不可为0，因此不会被换出缓存，这使得被引用的页更不可能被置换。

  ​	**使用环状缓冲区的一个优点是：时钟指针和缓冲区内容都可以用比较-置换(CAS)原子操作来修改，无须额外的加锁机制**。该算法很容易理解和实现，经常被用于教科书[TANENBAUM14]和真实系统中。

  ​	LRU并非对于所有数据库来说都是最佳的置换策略。有时，考虑将使用频率而非最近使用时间作为预测因子可能更贴合实际。最后，对于负载很重的数据库系统，最近使用时间可能不太具有参考性，因为它只表示项目被访问的顺序。

+ LFU

  ​	为了改善这一情况，我们可以追踪页引用事件而不是页换入事件。其中一种实现是跟踪具有最小使用频率(LFU)的页。

  ​	TinyLFU是一种基于频率的页置换策略，它正是这样做的：TinyLFU不根据页换入的时间来换出页，而是根据使用频率对页进行排序。它被用于一个流行的Java库 Caffeine中。

  ​	TinyLFU使用频率直方图[CORMODE11]来维护一个紧凑的缓存访问历史记录，因为出于实用性考虑，保存完整历史记录的代价太高。

  ​	元素可能位于以下三个队列中的某一个：

  + 入场(admission)队列：维护新加入的元素，用LRU策略实现。
  + 考察(probation)队列：其中的元素最有可能被换出缓存。
  + 保护(protected)队列：其中的元素将在队列中保留更长的时间。

  ​	**TinyLFU不是选择要换出哪些元素，而是选择要保留的元素**。访问频率相对较高的元素会被晋升到考察队列中。在后续访问中，元素可以从考察队列移至保护队列。如果保护队列已满，则必须将其中某个元素放回考察队列。经常访问的元素有较高的机会被保留，而不经常访问的元素则更有可能被换出缓存。

  ​	还有许多其他算法可用于最优的缓存换出。页置换策略的选择对于査询延迟和I/O操作次数有重大影响，因此必须慎重考虑。

  > [LFU算法族：LFU算法_夜雨落花的博客-CSDN博客_lfu](https://blog.csdn.net/weixin_38569499/article/details/113768134)
  >
  > [一个LFU算法就值30K吗？太牛了_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1H3411i7L1)

## * 5.2 恢复

​	数据库系统构建在硬件层及软件层之上，而这些硬件和软件自身可能存在稳定性和可靠性问题。无论是数据库系统本身还是下层的软件和硬件组件都有可能会发生故障。数据库实现者必须考虑这些故障场景，并确保已经"承诺"写入的数据确实已经写入了。

​	**预写日志(Write-Ahead Log，WAL，也称为提交日志)是一种仅追加的辅助磁盘数据结构，用于崩溃和事务的恢复**。<u>页缓存允许我们在内存中缓冲对页面内容的更改，而在将缓存的内容刷写回磁盘之前，WAL是保留操作历史的唯一磁盘副本</u>。许多数据库系统使用仅追加的预写日志，例如 PostgreSQL和MySQL。

​	预写日志的主要功能可以概括为：

+ 在允许页缓存将页上的修改缓存起来的同时，保证数据库系统仍然具有持久性语义。
+ **<u>在那些受操作影响的缓存页被同步到磁盘上之前，将所有操作持久化到磁盘上。每个修改数据库状态的操作必须先写日志到磁盘上，然后才能修改相关页的内容</u>**。
+ 当发生崩溃时，使系统可以从操作日志中重建内存中丢失的更改。

​	除此以外，预写日志在事务处理中也起着重要的作用。它确保将数据存储到持久性存储中，即使发生崩溃也依然可用：通过重放日志便可以恢复未提交的数据，这样数据库就可以完全恢复到崩溃前的状态。在本节中，我们会经常提到 ARIES(Algorithm for Recovery and Isolation Exploiting Semantics，一种利用语义的恢复和隔离算法)，这是一种先进的、被广泛使用和引用的算法[MOHAN92]。

---

+ PostgreSQL与`fsync()`

  ​	<u>PostgreSQL使用检査点来确保索引和数据文件已被完整地更新到日志文件中的某条记录</u>。检査点进程会定期刷写所有脏页(即修改过的页)，它通过执行`fsync()`内核调用来将脏页的内容同步到磁盘，同时清除页上的脏标志。可以想见， fsync函数如果无法刷新磁盘上的页，则会返回一个错误。

  ​	<u>在Linux和其他少数操作系统中，即使发生了I/O错误， fsync也会从刷新失败的页中清除脏标志</u>。此外，错误只会被报告给故障发生时**打开着**的文件描述符，因此fsync将不会返回在调用它的文件描述符打开之前发生的任何错误[CORBET18]。

  ​	<u>由于检查点进程不会在任何时刻都保持所有文件都打开的状态，所以可能会丢失错误通知。由于脏页标志已经被清除了，所以检查点进程会认为数据已成功写入磁盘，然而事实上可能并非如此</u>。

  ​	考虑到潜在的可恢复故障的存在，上述行为的组合可能会导致数据丢失甚至数据库损坏。这样的行为很难被检测到，并且它们可能导致数据库进入无法恢复的状态。有时，即使想触发此类行为也不那么容易。**在设计恢复机制时我们一定要格外小心，要仔细考虑并尝试测试每种可能的故障场景**。

### 5.2.1 日志语义

​	**预写日志是仅追加的，并且已写入的内容是不可变的，因此所有对日志的写入都是顺序的**。由于WAL是一个不可变的、仅追加的数据结构，所以读取方可以安全地访问写入边界之前的内容，同时写入方可以继续将数据追加到日志尾部。

​	WAL由日志记录组成，每条记录都有一个唯一的、单调递增的日志序列号(LSN)。通常，LSN由一个内部的计数器或时间戳表示。由于日志记录不一定占据整个磁盘块，所以其內容会被缓存在日志缓冲区中，并在强制刷盘(force)操作时被刷写到磁盘上。**强制刷盘操作发生在日志缓冲区被填满时，也可能被来自事务管理器或页缓存的请求所触发**。<u>各日志记录必须以LSN的顺序刷写到磁盘上</u>。

​	除了单独的操作记录外，WAL还会保存事务完成的记录。**只有当事务提交记录完成刷盘之后，才能将该事务视为已提交**。

​	<u>系统在回滚或恢复期间也有可能发生崩溃，为了确保这种情况下系统能继续正常工作，某些系统会在撤销操作时记录补偿日志记录(CLR)并将其存储在日志中</u>。

​	WAL通常与主存储结构紧密耦合，它提供了允许在到达检查点时对其进行修剪(trim)的接口。日志记录的正确性对于数据库十分关键，要正确处理它并不容易：日志修剪与确保数据已到达主存储结构之间细微的不一致都可能导致数据丢失。

​	**检査点使得我们知道某个特定标记之前的日志记录都已被持久化且不再需要了**，这大大减少了数据库启动过程中的工作量。<u>**强制将所有脏页刷写到磁盘上的过程通常称为同步检查点(sync checkpoint)**，因为它完全同步了主存储结构</u>。

​	**<u>把全部内容刷写到磁盘上是不现实的，而且需要暂停所有正在运行的操作，直到检查点完成</u>**。因此大多数数据库都实现了模糊检查点(fuzzy checkpoint)，在这种情况下，日志头部的last_ checkpoint指针记录了最后一次成功的检查点信息。模糊检查点以特殊的begin_checkpoint日志记录开始(标志着检查点的开始)，结束于end_checkpoint日志记录，其中包含脏页的信息以及事务表的内容。在刷写完该记录中提到的所有页之前，检查点都被视为不完整的。**页是异步刷新的，一旦完成，last_checkpoint指针将被更新为begin_checkpoint日志记录的LSN，如果发生崩溃，则恢复过程将从此处开始**[MOHAN92]。

### * 5.2.2 操作日志与数据日志

​	一些数据库系统，例如System R[CHAMBERLIN81]，使用影子页(shadow paging)——一种写时复制(copy-on-wite)技术，来确保数据持久性和事务原子性。新内容被存放在一个新的、未发布的影子页中，并通过指针翻转使其可见，从旧页切换到包含更新内容的新页。

​	**任何状态变化都可以由前像(before-image)和后像(after-Image)或相应的重做(redo)和撤销(undo)操作表示**。<u>对前像应用重做操作会生成后像，类似地，对后像应用撤销操作会产生一个前像</u>。

​	我们可以使用物理日志(保存对完整页状态或字节级的更改)或逻辑日志(保存在当前状态上执行的操作)将数据记录或页从一个状态变成另一个状态，无论是正向还是逆向。另一个很重要的事情是，<u>我们必须记录下在应用这些物理日志或逻辑日志之前页的确切状态</u>。

​	**<u>物理日志记录了前像和后像，受操作影响的整个页都要被记录下来。逻辑日志记录了要对页应用哪些操作，例如“向键Y插入数据记录Ⅹ”，以及相应的撤销操作，例如“删除与Y关联的值”</u>**。

​	**<u>在实践中，许多数据库结合使用上述两种方法：使用逻辑日志记录执行撤销操作(以提升并发)，使用物理日志记录执行重做操作(以缩短恢复时间)[MOHAN92]</u>**。

### * 5.2.3 steal和force策略

​	为了确定应何时将内存中的更改刷写到磁盘上，数据库定义了steal/no-steal和force/no-force策略。这些策略主要用于页缓存，但是最好在“恢复”这一上下文中讨论，因为它们会对能够使用哪种恢复方法产生显著影响。

+ <u>在**事务提交之前**允许刷写事务修改过的页，这被称为steal策略。而no-steal策略不允许将未提交的事务内容刷写到磁盘</u>。

  之所以称为steal策略，是因为它将脏页偷窃(steal)出来，将其中的内容刷写到磁盘上，并从磁盘加载另一个页到该位置。

+ <u>force策略要求在**事务提交前**将事务修改的所有页刷写到磁盘上。而在no-force策略中即使事务修改的某些页尚未刷写到磁盘上，该事务也可以提交</u>。

  这里 force的含义是强制(force)脏页在事务提交前必须将其刷写到磁盘上。

​	理解steal和fore策略非常重要，因为它们会影响事务的撤销和重做。

+ **撤销操作：回滚那些已提交事务强制刷盘的页上的更新。**
+ **重做操作：将已提交事务执行的更改应用到磁盘上**。

​	<u>若使用no-steal策略，那么只需要重做日志就可以实现恢复：旧副本包含在磁盘的页上，而修改被存储在日志中[WEIKUM0]</u>。

​	若使用no-force策略，我们就可以通过推迟对页的若干个更新来缓冲它们。由于这段时间内必须将页缓存在内存中，所以可能需要一个更大的页缓存。

​	<u>在使用force策略时，崩溃恢复无须任何其他工作即可重建已提交事务的结果，因为这些事务修改的页都已被刷写到磁盘。该方法的主要缺点是，由于在事务提交中必须要进行IO操作，所以提交时间更长</u>。

​	更一般地说，直到事务提交前，我们需要保存有足够的信息来撤销它的结果。**如果事务中涉及的页都已经刷写到磁盘，那么我们需要在日志中保留撤销信息，以在事务提交之前确保它能被回滚；否则，我们必须保留重做日志**。在这两种情况下，只有当撤销或重做记录被写入日志文件后，事务才能提交。

### * 5.2.4 ARIES

​	**ARIES是一种steal/no-force的恢复算法。它使用物理重做日志来提高恢复期间的性能(因为能更快地应用更改)，并使用逻辑撤销日志来提高正常操作期间的并发(因为逻辑撤销操作可以独立地应用到页中)。它使用WAL来实现在恢复时重放历史，从而完整地重建数据库状态(未提交事务的修改已被撤销)，并在撤销期间构建补偿日志记录[MOHAN92]**。

​	当数据库崩溃后重启时，恢复过程分为三个阶段：

1. **分析阶段：识别出页缓存中的脏页以及崩溃时正在进行的事务**。

   脏页的信息用于标识重做阶段的起点。进行中事务的清单用于在撤销阶段中回滚未完成的事务。

2. **重做阶段：重放历史记录直到崩溃点，并将数据库恢复到先前的状态**。

   此阶段会处理未完成的事务以及那些已提交但尚未将修改刷写到持久化存储的事务。

3. **撤销阶段：回滚所有未完成的事务，并将数据库还原到最后的一致状态。**

   所有操作均按反向时间顺序回滚。<u>为了防止数据库在恢复过程中再次崩溃，撤销事务所做的操作也会被记录到日志中，以避免重复应用</u>。

​	ARIES使用LSN来识别日志记录，通过脏页表来追踪运行中事务修改过的页，并使用物理重做、逻辑撤销和模糊检査点。虽然描述该系统的论文发表于1992年，但其中大多数概念、方法和范式在今天看来仍然很有意义。

## * 5.3 并发控制

​	当在1.1节中讨论数据库的架构时，我们提到事务管理器和锁管理器协同工作以处理并发控制。并发控制是一组用于处理并发事务间交互的技术。这些技术可以大致分为以下几类:

+ 乐观并发控制

  乐观并发控制(OCC)允许多个事务执行并发的读取和写入操作，最后确定其执行结果能否被串行化(serializable)。换句话说，事务不会彼此阻塞，而是保留其操作历史，并在提交前检查这些历史操作是否存在冲突的可能。**如果会产生冲突，则中止其中某一个冲突的事务**。

+ **多版本并发控制**

  多版本并发控制(MVCC)允许一条记录同时存在多个时间戳的版本，通过这种方式保证事务读到的是数据库过去某个时刻的一致视图。<u>MVCC可以使用验证技术来实现，即只允许多个更新或事务提交中的某一个获胜，也可以使用无锁技术(例如时间戳排序)或基于锁的技术(例如两阶段锁)</u>。

+ 悲观并发控制

  悲观(也称为保守)并发控制(PCC)既有基于锁的实现，也有不加锁的实现，它们的主要区别在于如何管理和授权对共享资源的访问。<u>基于锁的实现要求事务维护数据库记录上的锁，以防止其他事务修改被加锁的记录或访问当前事务正在修改的记录，直到锁被释放为止</u>。不加锁的实现根据未完成事务的调度，维护读取与写入的操作列表以限制事务的执行。**悲观的调度可能导致死锁**：多个事务需要互相等待对方释放锁才能继续执行。

​	在本章中，我们重点介绍节点本地的并发控制技术。第13章将会介绍分布式事务以及其他方法，例如确定性并发控制(参见13.4节)。

​	在进一步讨论并发控制之前，我们需要明确要解决的问题集，并讨论事务操作会以怎样的方式重叠以及重叠会产生什么样的后果。

### 5.3.1 可串行化

​	事务包括对数据库状态的一系列读取和写入操作以及业务逻辑(应用于读取内容上的转换)。调度(schedule)是指数据库视角上执行**<u>一组</u>**事务所需的操作列表（即仅包含与数据库状态交互的操作，例如读、写、提交和中止），因为我们假定其他所有的操作都没有副作用(换句话说，不影响数据库状态)[MOLINA08]。

​	如果一个调度中包含其中每个事务的所有操作，则称它是完整的。一个正确的调度在逻辑上等效于原始操作列表，但是**可以并行执行其中某些部分，也可以出于优化目的对其进行重新排列，只要不违反ACID性质并保证各事务的结果正确即可**[WEIKUM01]。

​	如果一个调度中的事务完全独立且无交错地执行，则我们称它为串行的调度：每个事务在下一个事务开始之前已完全执行。相比于多个事务的各种交错执行，串行执行很容易进行论证。但是，总是一个接一个地执行事务将会大大限制系统吞吐量并损害性能。

​	我们需要找到一种方法，它能够并发执行事务操作，同时保持串行调度的正确性和简单性。可串行化的调度可以满足这一要求。**<u>如果一个调度等效于同一组事务的某一完整串行调度，则该调度是可串行化的。换句话说，它产生的结果与我们以某种顺序一个接个地执行一组事务的结果相同</u>**。

### 5.3.2 事务隔离

​	事务型数据库系统有着不同的隔离级别。隔离级别指定了事务的各部分如何以及何时可以被其他事务看到。换句话说，隔离级别描述了将事务与其他并发事务隔离的程度，以及执行过程中可能发生哪些种类的异常(anomaly)。

​	实现隔离需要付出一定的代价：为了防止其他事务读到不完整或临时的写入，我们需要额外的协调和同步机制，而这会对性能产生负面影响。

### * 5.3.3 读异常与写异常

​	SQL标准[MELTON06]提到并描述了在执行并发事务期间可能发生的**读异常**：**脏读、不可重复读和幻读**。

​	**脏读(dirty read)**是指一个事务能读到其他事务未提交的更改。例如，事务T1更新了条用户记录的地址字段，事务T2在T1提交前就读到了已被更新的地址。事务T1中止并回滚其执行结果。然而T2却已经能够读到该值，由此看出，它访问到了一个从未提交过的值。

​	**不可重复读(nonrepeatable read，有时也称为模糊读(fuzzy read))**，是指同一事务两次查询同一行却得到不同的结果。例如，事务T读取一行，然后事务T2修改该行并提交了这一更改。如果T3在完成执行之前再次查询同一行，则结果将会和前一次查询不同。

​	如果我们在事务执行过程中使用范围读取(即不是读取单个数据记录，而是读取一个范围内的记录)，则可能会看到幻记录。**幻读是指事务两次查询同样的行集合却得到不同的结果**。<u>它与不可重复读类似，但仅适用于范围查询</u>。

​	同样地，也存在具有类似语义的**写异常**：**丟失更新，脏写和写偏斜**。

​	丢失更新(lost update)发生在事务T1和T2同时尝试更新V的值时。T1和T2读取V的值。T1更新V并提交，之后T2也更新V并提交。由于这两个事务不知道彼此的存在，所以如果允许二者都提交，则T1的结果将被T2的结果覆盖，T1的更新将丢失。

​	**脏写(dirty write)指的是某个事务拿到了一个未提交的值(即脏读)，对其进行修改并保存。换句话说，事务结果来自从未提交过的值**。

​	**<u>写偏斜(write skew)是指各个单独的事务都遵守要求的约束，但它们的组合却违反了这些约束</u>**。例如，事务T1和T2修改两个账户A1和A2的值。最初A1有100元，A2有150元。账户的值可以为负，只要两个账户之和为非负数就行，即要满足A1+A2≥0。T1和T2分别尝试分别从A1和A2取出200元。在两个事务开始时，A1+A2=250元，因此共有250元可用。两个事务都认为它们没有违反约束，可以提交。提交后，A1为-100元，A2为-50元，这显然违反了账户之和为非负数的要求[FEKETE04]。

### * 5.3.4 隔离级别

​	最低(换句话说，最弱)的隔离级别是读未提交(read uncommitted)。在此隔离级别下事务系统允许一个事务观察到其他并发事务的未提交更改。也就是说，脏读是允许的。

​	我们可以避免某些异常情况。例如，我们可以确保某一事务只能读到已提交的更改。但是，不能保证事务在再次读取同一条数据记录时还能看到相同的值。如果两次读取之间存在已提交的修改，则同一事务中的两次查询会得到不同的结果。换句话说，脏读是不允许的，但幻读和不可重复读是允许的。该隔离级别称为读已提交(read commited)。如果我们进一步禁止不可重复读，则会得到可重复读(repeatable read)的隔离级别。

​	最强的隔离级别是可串行化(serializability)。正如我们在5.3.1节中讨论的那样，它可以确保事务结果以某种顺序呈现，就好像事务是串行执行的(即时间上没有重叠)。禁止并发执行将对数据库性能产生很大的负面影响。<u>只要事务的内部约束保持不变并且可以并发执行，就可以对事务进行重新排序，但它们的结果必须以某种串行顺序呈现</u>。

|          | 脏读 | 不可重复读 | 幻读 |
| -------- | ---- | ---------- | ---- |
| 读未提交 | 允许 | 允许       | 允许 |
| 读已提交 | -    | 允许       | 允许 |
| 可重复读 | -    | -          | 允许 |
| 可串行化 | -    | -          | -    |

​	没有依赖关系的事务可以以任何顺序执行，因为它们的结果是完全独立的。不同于可线性化(将在分布式系统的上下文中讨论，参见11.5.2节)，**可串行化是指按任意顺序执行多个操作，它并不暗示或强制某一特定的事务执行顺序**。**<u>ACID中的隔离性意味着可串行化</u>**[BAILIS14a]。<u>然而，实现可串行化需要协调，换句话说，并发执行的事务必须进行协调以确保遵守约束，并在发生冲突时强加某一串行顺序</u>[BAILIS14b]。

​	一些数据库使用**快照隔离(snapshot isolation)**。在快照隔离中，一个事务可以观察到所有在该事务开始前已提交的事务所做的状态更改。每个事务都会获取一个数据快照，并在快照上执行查询。快照在事务执行期间不会发生变化。**只有当事务中修改的值没有被其他事务修改时，才可以提交事务，<u>否则它将被中止并回滚</u>**。

​	**如果两个事务试图修改同一个值，则只有其中一个事务能够提交**。这能防止丢失更新异常。例如，事务T1和T2都尝试修改V。它们从快照读取Ⅴ的当前值，该快照包含所有在事务开始前已提交的事务的更改。首先尝试提交的事务会成功提交，而另一个事务则必须中止。**<u>失败的事务将会重试，而不是覆盖已修改的值。</u>**

​	**<u>快照隔离中可能发生写偏斜异常：如果两个事务分别读取本地状态、修改不同的记录并且遵守本地的约束，则二者都可以提交[FEKETE04]。我们将在13.7节里，在分布式事务的上下文中更详细地讨论快照隔离</u>****。

### * 5.3.5 乐观并发控制

​	乐观并发控制假设事务冲突很少发生，并且不同于加锁阻塞事务执行的方式，我们可以通过在**<u>事务提交前</u>验证事务来防止并发事务间发生读或写冲突**，确保可串行化。一般来说，事务执行分为三个阶段[WEIKUM01]：

+ 读阶段

  事务在其私有上下文中执行各步骤，此时一切更改都对其他事务不可见。在该步骤完成后，我们就知道了事务的所有依赖条件(读集合)，以及事务会产生的所有副作用(写集合)。

+ 验证阶段

  检査并发事务的读集合和写集合，査看其操作之间是否存在可能违反可序列化性质的冲突。如果事务读取的某些数据现在已过期，或者事务的写入将覆盖在其读阶段提交的事务所写入的某些值，则清除事务的私有上下文、重启读阶段。换句话说，**验证阶段确定提交事务是否遵守ACID性质**。

+ 写阶段

  如果验证阶段未发现任何冲突，则事务可以将其写集合从私有上下文提交到数据库状态中。

​	验证有两种实现方式：检查与已提交事务的冲突(后向式)，以及检查与当前处于验证阶段的事务的冲突(前向式)。**<u>不同事务的验证阶段和写阶段应当原子地完成</u>**。**在验证其他事务时，不允许提交任何事务**。由于验证阶段和写阶段通常比读阶段短，因此这是可以接受的妥协。

​	后向式并发控制确保任意一对事务T1和T2都具有以下性质：

+ 如果T1在T2的读阶段开始之前提交，则T2可以提交。
+ 如果T1在T2的写阶段之前提交，则T1的写集合与T2的读集合不相交。换句话说T1不能写入任何T2看到的值。
+ 如果T1的读阶段在T2的读阶段之前完成，则T2的写集合与T1的读或写集合均不相交。换句话说，两个事务操作独立的数据集合，因此两者都可以提交。

​	由于重试会对性能产生明显的负面影响，所以在验证通常是成功的因而无须重试事务的情况下，这种方法是高效的。当然，**<u>乐观并发仍然具有临界区，同一时刻只能进入一个事务</u>**。

​	**要想让某些操作具有非排他性的所有权，另一条途径是使用读写锁(允许读取方共享访问)或可升级锁(在需要时将共享锁转换为排他锁)**。

### 5.3.6 多版本并发控制

​	多版本并发控制是数据库中实现事务一致性的一种方法，它允许记录存在多个版本，并使用单调递增的事务ID或时间戳。这使得读写操作在存储层面上只需要最小限度的协调，因为读操作可以继续访问旧的值，直到新的值被提交。	

​	MVCC区分**已提交**和**未提交**的版本，对应于已提交和未提交事务的值的版本。最后提交的值的版本也就是值的**当前版本**。一般在MVCC中，<u>事务管理器的目标是确保任一时刻**最多**只有一个未提交的值版本</u>。

​	根据数据库实现的隔离级别，读操作也许能访问或不能访问未提交的值[WEIKUMO1]。多版本并发可以用加锁、调度和冲突解决技术(例如两阶段锁)来实现，也可以用时间戳排序来实现。**实现<u>快照隔离</u>是MVCC的一大使用场景**[HELLERSTEINO7]。

### * 5.3.7 悲观并发控制

​	悲观并发控制方案比乐观方案更保守。这种方案在事务运行时确定其间的冲突并阻塞或中止执行。

​	时间戳排序是最简单的悲观(无锁)并发控制方案之一，其中每个事务都有一个时间戳，**事务操作能否执行取决于是否已经提交过时间戳更晚的事务**。为了实现这一点，事务管理器需要维护每个值的max_read_timestamp和 max_write_timestamp，它们分别描述了并发事务执行的读和写操作。

​	如果读操作的时间戳小于读到的值的max_write_timestamp，则会导致当前事务中止。因为已经有一个较新的值存在，若允许该操作则会违反事务顺序。

​	<u>类似地，时间戳小于max_read_timestamp的写操作将与最近的读操作冲突。但是这种情况是被允许的，因为我们可以安全地忽略过期的写入值</u>。这个猜想通常被称为**托马斯写规则**[THOMAS79]。每当进行读或写操作时，相应的最大时间戳都会被更新。中止的事务会以新的时间戳重新开始，否则新的事务必定会被再次终止[RAMAKRISHNAN03]。

> [托马斯写入规则 | 程序员笔记 (knowledgedict.com)](https://www.knowledgedict.com/tutorial/dbms-thomas-write-rule.html)

### * 5.3.8 基于锁的并发控制

​	基于锁的并发控制方案是悲观并发控制的一种形式，它对数据库对象显式地加锁，而不是用时间戳排序之类的协议来制定操作的顺序。使用锁的缺点包括**锁竞争**和**扩展性问题**[REN6]。

​	使用最广泛的基于锁的技术之一是**两阶段锁(2PL)**，它将锁管理分为两个阶段：

+ 增长阶段(也称为扩张阶段)，此阶段将获取事务所需的所有锁，并且<u>不释放任何锁</u>。
+ 收缩阶段，此阶段释放增长阶段获得的所有锁。

​	由上述两个定义可以推断出一条规则：事务一旦释放了哪怕一个锁，就不能再获取任何锁。值得注意的是，2PL并不阻止事务在这两个阶段中执行事务步骤，但是某些2PL变体(例如保守2PL)确实增加了这些限制。

> ​	尽管名称很相似，但**两阶段锁**和**两阶段提交**是两个完全不同的概念(参见13.2节)。
>
> + **两阶段提交是一种用于分布式多分区事务的协议**。
> + **而两阶段锁是一种并发控制机制，常用于实现可串行化**。

---

+ 死锁

  ​	在锁协议中，事务尝试获取数据库对象上的锁，如果无法立刻获得锁，则事务必须等待直到锁被释放。可能会发生这样的情况：两个事务都在尝试获取所需的锁以继续执行，而最后都在等待对方释放其持有的某个锁。这种情况称为死锁。

  ​	最简单的处理死锁的方法是引入超时机制并中止长时间运行的事务，假定它们可能处于死锁状态。另一种策略，即**<u>保守2PL，要求事务在执行任何操作之前先获取所有锁，如果做不到就中止事务</u>**。但是，这些方法极大地限制了系统的并发，因此数据库系统大多使用事务管理器来检测或避免(或者说防止)死锁。

  ​	死锁检测通常是用等待图(waits-for-graph)实现的，等待图跟踪进行中的事务间的关系并建立其间的等待关系。

  ​	等待图中的环表明出现了死锁：事务T1正在等待T2，而T2又正在等待T1。死锁检测可以定期(每隔一段时间一次)或持续(每次等待图更新时)进行[WEIKUM01]。其中一个事务(通常是最近尝试获取锁的事务)会被中止。

  ​	<u>**为了避免死锁、限制锁的获取以确保不会产生死锁，事务管理器可以用事务时间戳来确定事务优先级**</u>。较低的时间戳通常意味着较高的优先级，反之亦然。

  ​	如果事务T1尝试获取T2当前持有的锁，并且T1具有更高的优先级(它在T2前开始)，那么我们可以使用以下限制之一来避免死锁[RAMAKRISHNAN03]：

  + 等待-死亡(wait-die)

    允许T1阻塞以等待锁。否则(如果T2在T1之前开始)，T1将中止并重新启动。换句话说，事务只能被时间戳更高的事务阻塞。

  + 伤害-等待(wound-wait)

    T2中止并重新启动(T1伤害T2)。否则(如果T2在T1之前开始)，则允许T1等待。换句话说，事务只能被时间戳更低的事务阻塞。

  ​	事务处理需要一个调度器来处理死锁。同时，闩锁(将在稍后介绍)依靠程序员来保证不会发生死锁，而不依赖于死锁避免机制。

---

+ 锁

  ​	<u>如果两个事务同时提交且修改了重叠的数据段，则它们中的任何一个都不应当观察到另一个事务的**部分结果**，这样才能保持逻辑上的一致性</u>。

  ​	<u>类似地，同一事务中的两个线程必须观察到**相同的数据库内容**，并且可以访问彼此的数据</u>。

  ​	在事务处理过程中，保护逻辑和物理数据完整性的机制有所不同。**负责逻辑和物理完整性的两个概念分别是锁(lock)和闩锁(latch)**。其在命名上有点令人迷惑，因为这里说的闩锁在系统编程中通常称为锁，但在本节中我们将阐明它们的区别和含义。

  ​	**锁用于隔离和调度重叠的事务、管理数据库内容(而非内部存储结构)，并且锁是在<u>键上获取</u>的。锁可以保护某个特定的键(无论该键存不存在)或一个范围内的键。<u>锁通常在树实现之外进行存储和管理，它表示一个较高层级的概念，由数据库的锁管理器管理</u>**。

  ​	**<u>锁比闩锁更重量级，且在事务执行期间一直持有</u>**。

---

+ 闩锁

  ​	另一方面，闩锁用于保护物理表示：在插入、更新和删除操作期间，叶子页的内容会被修改；在叶子页发生下溢或上溢时，页的分裂与合并向上传播，非叶子页的内容以及树结构也会被修改。<u>在这些操作期间，闩锁保护了树的物理表示(页内容及树结构)，并且它是在**页的级别上**获取的</u>。**在访问任何页前必须先加闩锁，以确保并发安全**。**<u>无锁并发控制技术也必须使用闩锁</u>**。

  ​	<u>由于叶节点上的修改可能会传播到B树的更高层，所以可能要在多个层上获得闩锁</u>。**执行中的查询不应当观察到处于不一致状态的页，例如不完整的写入或尚未完成的节点分裂**(此时数据可能同时出现在源节点和目标节点中，或者还未传播到上级节点)。

  ​	同样的规则也适用于上级或同级指针的更新。**一个通用的规则是尽可能缩短持有闩锁的时间(即只在读取或更新页时)以提高并发性**。

  ​	并发操作之间的干涉可以大致分为三类：

  + **并发读**，多个线程访问同一页而不做修改。
  + **并发更新**，多个线程试图对同一页进行修改。
  + **写时读**，一个线程试图修改页内容，而另一个线程试图读取同一页。

  上述场景也适用于和数据库维护进程重叠的访问(比如4.7节中描述的后台进程)。

---

+ 读写锁

  ​	最简单的闩锁实现将会授予请求线程**排他性**的**读写**访问权限。但是，大多数时候，我们不需要把所有进程相互隔离。例如，并发地读取页不会有什么问题，因此，**我们只要确保并发的写操作不重叠，以及读操作与写操作不重叠**。为了达到这种粒度级别，我们可以使用读写锁(又称为RW锁)。

  ​	读写锁允许多个读操作同时访问冋该对象，而只有写操作(通常相对较少)必须获得对该对象的排他性访问。**读写锁的兼容性：只有读操作（和另一个读操作）可以共享锁所有权，而读写操作的其他组合都要获得排他性所有权**。

  ​	**除了<u>防止页缓存重复加载页</u>，访问同一页的重叠读操作无须同步，因此它们可以以共享模式安全地并发进行**。

  ​	**一旦出现写入，我们则需要将其与并发的读取以及其他写入隔离开**。

  > 忙等和排队
  >
  > ​	为了管理对页的并发访问，我们可以使用阻塞算法：将线程置为等待状态，一旦可以继续执行时再将其唤醒。此外，也可以使用忙等(busy-wait)算法。忙等算法允许线程等待一小段时间，而不是将控制权直接交还给调度器。
  >
  > ​	排队通常使用比较-替换(CAS)指令实现，**CAS指令用于保证锁获取和队列更新操作的原子性**。<u>如果队列为空，则线程立即获得访问权限。否则，线程将自己追加到等待队列中并自旋等待一个变量，**该变量只能由队列中前一个线程更新**</u>。**排队有助于减少获取和释放锁的CPU工作量**[MELLORCRUMMEY91]。

---

+ 闩锁耦合

  ​	<u>加闩锁最直接的方法是获取从根到目标叶节点途中的所有闩锁</u>。这会导致并发性能瓶颈，而且在大多数情况下可以避免。持有闩锁的时间应尽可能短。为了达到这一目标，其中一种优化方式称为闩锁耦合(latch coupling，英文中也称为latch crabbing，其中crab是螃蟹的意思，因为螃蟹走路的样子和这里交替加锁释放的过程很相似。)[RAMAKRISHNAN03]。

  ​	闩锁耦合是一种非常简单的方法：它允许持有闩锁的时间更短，并在当前操作明显不再需要它们时立即释放。

  + **在读取路径上，一旦找到子节点并获得它的闩锁，就可以释放父节点的闩锁**。

  + **在插入时，如果能保证操作不会引起能传播到当前节点的结构变化，则可以释放父节点上的闩锁**。换句话说，如果子节点还没满，则可以释放父闩锁。

  + **类似地，在删除时，如果子节点包含足够多的元素，或者该操作不会导致节点合并，则可以释放父节点上的闩锁**。

  ​	插入过程中从根到叶的加锁过程：

  ​	a)获取根节点上的写闩锁。

  ​	b)找到下一级节点并获取它的闩锁。检査该节点是否有可能发生结构变化。<u>由于节点还没满，所以可以释放父节点的闩锁</u>。

  ​	c)继续处理下一层：获取写闩锁，检査目标叶节点是否有可能发生结构变化，然后释放父节点的闩锁。

  ​	这种方法是乐观的：**<u>大多数插入和删除操作都不会引起向上传播多个层的结构变化。事实上，对于越高的层，结构变化的可能性越低</u>**。大多数操作仅需要目标节点上的闩锁，只有相对少数的情况下才要保留父节点的闩锁。

  ​	<u>如果子页尚未加载到页缓存中，则我们可以为之后要加载的页加闩锁，**也可以释放父节点的闩锁并且当页加载后再重启从根到叶的加锁过程，从而减少锁竞争**</u>。

  ​	**重启从根到叶的遍历听起来很昂贵，但实际上这个操作很少会执行，并且我们可以采用某些机制来检测自上次遍历之后高层的节点是否发生过结构变化**[GRAEFE10]。

  > 闩锁升级和指针追逐
  >
  > ​	除了在遍历期间获取排他性闩锁，还可以使用闩锁升级(latch upgrading)。这种方法<u>沿着搜索路径获取共享锁，并在必要时将其**升级为排他锁**</u>。
  >
  > ​	写操作一开始仅在叶节点上获取排他锁。如果叶节点需要分裂或合并，则沿着树向上，将父节点的共享锁升级为排他锁，从而获得树上受影响部分(即那些也要被分裂或合并的节点)的排他所有权。在较高层上，可能会有多个线程同时尝试获取排他锁，**<u>此时未抢到锁的线程必须等待或重试</u>**。
  >
  > ​	**<u>你可能已经注意到，目前为止描述的机制都会首先获取根节点上的闩锁</u>**。**每个请求都要经过根节点，因此根节点很快就会成为瓶颈**。
  >
  > ​	同时，根节点始终是最后一个分裂的，因为只有当所有的子节点都填满时才会发生分裂。这就意味着**<u>根节点始终可以乐观地加锁，而很少会付出重试(指针追逐(pointer chasing))的代价</u>**。

---

+ Blink树

  ​	Blink树构建在**B***树的基础上(参见4.4节)并添加了**高键**(参见4.1.4节)和同级链接(sibling link)指针[LEHMAN81]。<u>高键表示**子树**中可能的最高键</u>。除根节点外，Blink树中的每个节点都有两个指针：子节点指针和同级链接指针，后者指向同一层中的右侧相邻节点。

  ​	Blink树允许存在所谓的**半分裂状态**，这时该节点已被同级指针引用，但没有被上一层的子节点指针引用。<u>通过检查节点高键，可以识别出半分裂状态。如果要查找的键大于节点的高键(违反了高建的约束)，则査找算法可以断定树结构已被并发地修改，因此它根据同级链接指针找到右侧相邻节点并继续査找</u>。

  ​	为了获得最佳性能，最好尽快将指针添加到父节点中，但是无须终止并重启整个査找过程，因为树中的所有元素都是可以被访问的。<u>这样做的好处是，**即使子节点会分裂，遍历子节点时也无须保留父节点的锁**：我们可以通过同级链接指针使新节点可见，并懒惰地更新父节点的指针而不失正确性[GRAEFE10]</u>。

  ​	<u>尽管这比直接从父节点下降遍历效率略低，并且需要访问一个额外的页，但这能保证遍历过程的正确性，同时简化了并发访问</u>。**考虑到分裂是相对少见的操作，并且B树很少收缩，因此这种情况十分罕见，其成本微不足道**。

  ​	这种方法有很多好处：

  + **它减少了锁竞争，在分裂期间不用保留父节点的锁**，并把<u>在树结构变化期间持有锁的数量减少到一个常数</u>。
  + 更重要的是，**读操作和树结构变化可以并发地进行**，并防止死锁——并发的修改操作在向父节点传播时可能引起死锁。

## 5.4 本章小结

​	在本章中，我们讨论了负责事务处理和恢复的存储引擎组件。在实现事务处理时，我们面临两个问题：

+ 为了提高效率，我们要允许事务并发执行。
+ 为了保持正确性，我们要确保并发执行的事务遵守ACID性质。

​	并发事务执行可能导致多种读异常和写异常。

+ 隔离级别描述并限制了这些异常是否可以出现。
+ **并发控制机制决定了事务如何被调度和执行**。

​	页缓存负责减少磁盘访问的次数：它将页缓存在内存中，并允许对其进行读写访问。当缓存达到容量时，页将被换出并刷写回磁盘。

​	**为了确保节点崩溃时不会丢失未落盘的更改，以及为了支持事务回滚，我们会记录预写日志**。

​	页缓存和预写日志使用force和steal策略进行协调，确保在不失持久性的前提下，毎个事务可以髙效执行并且能够回滚。

# 6. B树的变体

​	B树的变体有几个共同点：树结构、通过分裂和合并实现平衡，以及査找和删除算法。在不同的实现之中，并发性、磁盘页表示、同级节点之间的链接和维护进程等细节则可能有所不同。

​	在本章中，我们将讨论几种可用于实现高效B树的技术和使用它们的数据结构：

+ **写时复制B树**，其结构类似于B树，但其中的节点是<u>不可变</u>的，并且不会原地更新。相反，页被复制、更新并写入新的位置。
+ **惰性B树**，通过缓冲对节点的更新，减少来自后续相同节点写操作的IO请求数量。在下一章中，我们还将介绍双组件LSM树(参见7.1.1节)，它扩展了缓冲以实现<u>完全不可变</u>的B树。
+ **FD树**则采用不同的缓冲方法，有点类似于LSM树(参见7.1节)。FD树在一个小型B树中缓冲更新。一旦这个树被填满，它的内容就会被写入一个<u>不可变</u>的运行实例中。更新以级联的方式在<u>不可变</u>运行实例的层之间传播，从较高的层传播到较低的层。
+ **Bw树**将B树节点分成几个较小的部分，以仅追加的方式进行写入。它通过批处理不同节点上发生的更新，减少了少量但频繁的写操作的成本。
+ **缓存无关B树**(cache-oblivious B-Tree)则允许<u>以类似于内存中数据结构的构建方式处理磁盘上的数据结构</u>。

## * 6.1 写时复制

​	有些数据库并不构建复杂的锁机制，而是使用写时复制(copy-on-write)技术来保证并发操作时的数据完整性。在这种情况下，每毎当页即将被修改时，就会先复制其内容，然后修改复制的页而不是原来的页。这样便创建出了一个平行的树状层次结构。

​	**旧版本的树对于与写入者、并发的读取者而言仍然是可访问的，<u>而访问修改后的页的写入者必须等待之前的写操作完成</u>**。在创建新的页层次结构之后，指向最顶层页的指针将被自动更新。写时复制，一个与旧树平行的新树被创建出来，它们之间重用了未被修改的页。

​	由于必须复制整个页的内容，所以这种方法的一个明显的缺点是它需要更多的空间(即使旧版本只会保留很短的时间：在使用旧页的并发操作完成之后，那些页可以被立即回收)和处理器时间。由于B树通常是较浅的，所以这种方法的简洁性和优势往往仍然大于其缺点。

​	**<u>这种方法最大的优点是读取者不需要同步</u>**。因为已写入的页是不可变的，可以在不需要额外加锁的情况下被访问。因为写操作是针对复制页进行的，所以读取者不会阻塞写入者。任何操作都不会得到一个处于不完整状态的页。即使是系统崩溃也不会使页处于损坏状态，因为只有当所有页的修改都完成时，最顶端的指针才会切换。

---

+ 实现写时复制：LMDB

  ​	一个使用写时复制的存储引擎是Lightning Memory-Mapped Database(LMDB)，它是OpenLDAP项目使用的键值存储。得益于其设计架构，<u>LMDB不需要页缓存、预写日志生成检查点或压实操作</u>。

  ​	**LMDB被实现为单层数据存储，这意味着它直接通过内存映射来满足读写操作，而不需要额外的应用程序层缓存**。这也意味着页不需要额外的物化，可以直接从内存映射进行读取，而无须将数据复制到中间缓冲区。在更新过程中，从根节点到目标叶节点的路径上的每个分支节点都被复制并可能被修改：更新所传播到的节点被修改，而其余节点将保持不变。
  
  ​	<u>LMDB只保存两个版本的根节点：最新版本和提交新更改的版本</u>。这已经足够了，因为所有写操作都必须经过根节点。在创建新根之后，旧根将无法进行新的读写操作。引用旧树节的读操作一旦完成，其页就会被回收，并且可以被重新使用。由于LMDB的仅追加设计，它不使用同级指针，所以在顺序扫描时必须上升到父节点。
  
  ​	在这种设计中，将陈旧数据留在复制出的节点中是不明智的：已经有一个副本可以用于MVCC，并满足正在进行的读取事务了。**这种数据库结构本质上是多版本的，读取者可以在没有任何锁的情况下运行，因为它们不会以任何方式干扰写入者**。
  
  > [lmdb 数据库 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/70359311)

## 6.2 抽象节点更新

​	**为了更新磁盘上的页，我们必须首先更新它在内存中的表示形式**。然而，有多种方法可以表示内存中的一个节点：我们可以直接访问节点的缓存版本，或者通过包装器(wrapper)对象访问，抑或在实现语言中构建它的内存表示。

​	在具有非托管内存模型的语言中，存储在B树节点中的原始二进制数据是可以被重新解释的，且可以被原生指针所操作。在这种情况下，节点是按照结构体定义的，这些结构体使用指针和运行时类型转换来操作背后的原始二进制数据。最常见的情况是，它们指向页缓存管理的内存区域，或者使用内存映射。

​	另一种方法是将B树节点物化为语言原生的对象或结构体。这些结构体可用于插入、更新和删除。在刷写期间，更改应用于内存中的页，随后应用于磁盘上的页。这种方法的优点是简化了并发访问，因为对底层原始页的更改与对中间对象的访问是分开管理的。但是这会导致更高的内存开销，因为我们必须在内存中存储同一页的两个版本(原始进制版本和语言原生版本)。

​	第三种方法则是通过包装器对象来提供对节点背后缓冲区的访问，在B树中的更改被执行后该包装器对象会立即物化这些更改。这种方法最常用于具有托管内存模型的语言中。包装器对象将更改应用到相应的缓冲区。

​	单独管理磁盘上的页、它们的缓存版本以及它们在内存中的表示形式使得它们可以具有不同的生命周期。例如，我们可以缓冲插入、更新和删除操作，并在读取过程中协调内存中的更改和磁盘上的原始版本。

## 6.3 惰性B树

​	一些算法(在本书中我们称之为惰性B树)降低了更新B树的成本，并使用更轻量级的、并发友好且更新友好的内存结构来缓冲并延迟传播更新。

> 惰性B树：这不是一个公认的名称，但是由于我们在这里讨论的B树变体均拥有一个属性——在中间结构中缓冲B树更新，而不是直接将它们应用到树中——因此，我们将使用术语惰性，它精确地定义了这个属性。

### 6.3.1 WiredTiger

​	让我们来看看如何使用缓冲实现惰性B树。为此，我们可以在B树节点被换入后立即在内存中物化它们，并使用该结构存储更新，直到我们准备好刷写它们。

​	MongoDB现在默认的存储引擎WiredTiger也使用了类似的方法。它的行存储B树的实现针对内存和磁盘页使用了不同的格式。在持久化内存页之前，它们必须经过一个协调的过程。

​	一个干净的页仅由一个索引组成，该索引最初是从磁盘上的页镜像构造出来的。更新首先被保存到更新缓冲区中。

​	在读取过程中会访问更新缓冲区：其内容将与原始磁盘页中的内容进行合并，以返回最新的数据。当刷写页时，更新缓冲区内容将与页内容协调然后保存在磁盘上，以覆盖原始页。如果协调后页的大小大于最大值，则将其拆分为多个页。更新缓冲区使用跳表(skiplist)来实现，跳表具有类似于搜索树的复杂度[PAPADAKIS93]，但具有更好的并发性[PUGH90a]。

​	Wiredtiger中的干净页和脏页都有内存中的版本，并且都引用磁盘上的基镜像。脏页还有一个更新缓冲区。

​	<u>其主要优点是页更新和结构修改(分裂和合并)**由后台线程执行**，而读写进程不必等待它们完成</u>。

> [WiredTiger系列1:数据页详解_Alive的博客-CSDN博客_wiredtiger](https://blog.csdn.net/qq_35192280/article/details/112971258)
>
> [WiredTiger 原理知识_q936889811的博客-CSDN博客_wiredtiger](https://blog.csdn.net/q936889811/article/details/88534783)

### 6.3.2 惰性自适应树

​	除了在单个节点上缓冲更新，我们还可以将节点分组到子树中，并为每个子树附加用于批量操作的更新缓冲区。在这种情况下，更新缓冲区将跟踪对子树顶部节点及其后代节点所执行的所有操作。这种算法称为惰性自适应树(Lazy- Adaptive Tree，LA树)[AGRAWAL09]。

​	在插入数据记录时，首先将一个新条目添加到根节点更新缓冲区。当此缓冲区被填满时，我们将这些更改进行复制并传播到较低层的缓冲区，并清空当前缓冲区。如果较低层的缓冲区也填满了，那么这个操作可以递归地继续，直到最终到达叶节点。

​	缓冲区具有层次依赖关系，并且是级联的：所有更新都从较高层的缓冲区传播到较低层的缓冲区。当更新到达叶子层时，将在叶子层执行批量的插入、更新和删除操作，同时将所有更改应用于树的内容及其结构。由于分裂和合并也会批量地传播到更高层，所以可以在一次运行中更新多个页，从而产生更少的磁盘访问和结构更改。

​	这里描述的缓冲方法是通过批量处理写操作来优化树的更新时间，但方式略有不同。这两种算法都需要在内存缓冲结构中进行额外的査找，并与陈旧的磁盘数据进行合并协调。

## 6.4 FD树

​	**缓冲是一种在数据库存储中被广泛使用的思想：它有助于避免许多小的随机写操作，而作为替代，它会执行单个较大的写操作**。在机械硬盘上，由于磁头的寻道，随机写入会较慢。<u>在固态硬盘上，虽然没有要移动的部件，但是额外的写IO会增加垃圾收集的代价</u>。

​	维护B树需要大量的随机写入—叶子级别上的写入、分裂和合并可以一路传播到父节点——但是如果我们可以完全避免随机写入和节点更新会怎么样？

​	到目前为止，我们已经讨论了通过创建辅助缓冲区来缓冲对单个节点或节点组的更新。另一种方法是通过使用仅追加存储和合并过程将不同节点的更新归并在一起，这一想法也启发产生了LSM树(参见7.1节)。这意味着我们执行的任何写入都不需要定位目标节点：所有更新都只是追加操作。将这种方法用于索引的一个例子叫作闪存盘树(Fash Disk Tree，FD树)[LI10]。

​	FD树由一个小的、可变的头部树(head tree)和多个不可变的有序段构成。当随机I/O产生的时候，这种方法将暴露的区域限制在头部树上：一个对更新进行缓冲的小型B树。一旦头部树被填满，它的内容就被转移到不可变的有序段中。如果新写入的有序段的大小超过阈值，则其内容将与下一层合并，从上到下逐层传播数据记录。

### 6.4.1 分段级联

​	为了维护层之间的指针，FD树使用了一种叫作分段级联(fractional cascading)的技术[CHAZELLE86]。这种方法有助于降低在级联排序数组中定位数据项的成本：你可以执行log(n)个步骤以在第一个数组中査找被搜索项，但后续搜索的成本要低得多，因为它们从与上一层最接近的匹配项开始搜索。

​	通过在相邻层数组之间建立桥(bridge)来构建层之间的捷径，以最小化间隙：没有来自更高层的指针的元素被分在一组。桥是通过将元素从较低层拉到较高层(如果它们不存在的话)并指向在较低层数组中被拉出的元素的位置来构建的。

​	[CHAZELLE86]解决了计算几何中的搜索问题，它描述了双向桥以及用于恢复间隙不变式的算法，我们在这里不会对此进行讨论。我们只描述适用于数据库存储和FD树的部分。

​	我们可以创建一个从高层数组的每个元素到下一层最近的元素的映射，但是这会给指针及其维护带来太大的开销。如果只映射在较高层上已经存在的数据项，则我们可能会最终陷入元素之间间隙太大的状况。为了解决这个问题，在低层数组中，每隔N-1个数据项就将一个数据项拉到高层数组中。

​	为了在所有这些数组中搜索元素，我们在最高层上执行二分搜索，之后在下一层上的搜索空间会显著减小，因为现在我们可以通过桥的转发而找到被搜索项的近似位置。这允许我们连接多个有序段，并减少在其中搜索的成本。

### 6.4.2 对数级的有序段

​	FD树结合了分段级联与创建对数级的有序段：一组不可变的排序数组，它们的大小以系数k递增，并且它们是通过将上一个层与当前层合并而创建的。

​	最高层的有序段是在头部树变满的时候创建的：其叶子的内容被写入第一层。一旦头部树再次填满，它的内容将与第一层项合并。合并后的结果将替换第一个有序段。当较高层有序段的长度达到阈值时，会创建较低层有序段。如果已存在较低层的有序段，则将其替换为与较高层有序段的内容合并后的结果。这个过程与LSM树中的压缩相当类似，在LSM树中，不可变的表内容被合并以创建更大的表。

​	为了使所有有序段中的项都可寻址，FD树使用了一种修改版的分段级联，其中来自较低层页的头元素被传播为指向较高层的指针。通过使用这些指针，其降低了在较低层树中搜索的成本，因为搜索已经在较高层上部分完成，从而可以从最接近的匹配继续。

​	由于FD树不在原地更新页，并且可能在几个层上存在相同键的数据记录，所以FD树通过插入墓碑(tombstone，FD树的论文中将其称为过滤器条目)来进行删除。该墓碑指示与相应键相关联的数据记录已被删除，必须丢弃较低层中该键的所有数据记录。当墓碑一直传播到最低层时，它们便可以被丢弃了，因为此时可以保证已经没有需要它们遮蔽的数据项了。

## 6.5 Bw树

​	在原地更新的B树的实现中，**写放大**是最显著的问题之一：对一个B树页后续的更新可能需要在毎次更新时更新其磁盘驻留页副本。第二个问题是**空间放大**：我们保留了额外的空间以实现更新。这也意味着，为了传输承载着所请求数据的每个有用字节，我们不得不额外传输一些空字节以及页的其余部分。第三个问题是**解决并发问题和处理闩锁的复杂性**。

​	为了同时解决这三个问题，我们必须采取一种与目前已经讨论过的方法完全不同的方法。**缓冲更新有助于减轻写放大和空间放大问题，但不能解决并发问题**。

​	我们可以通过使用仅追加存储来对不同节点进行批量更新，将节点链接成链，并使用内存数据结构——该结构允许通过单个CAS操作在节点之间建立指针，从而使树无锁。这种方法称为Buzzword树(Bw树)[LEVANDOSKIIA4]。

### 6.5.1 更新链

​	Bw树区分基节点的写入与修改。修改(增量节点)形成一个链：一个从最新修改到旧修改的链表，该链表末尾则是基节点(base node)。每个更新都可以单独存储，而不需要重写磁盘上的现有节点。增量节点(delta node)可以表示插入、(与插入无法区分的)更新或删除。

​	由于基节点和增量节点的大小不太可能是页对齐的，所以将它们连续存储是有意义的，而且由于在更新期间基节点和增量节点都不会被修改(所有修改只是将一个节点插入现有链表头部)，所以我们不需要预留任何额外的空间。

​	将节点作为逻辑实体而不是物理实体是一种有趣的范式转变：我们不需要预先分配空间，不需要节点具有固定的大小，甚至不需要将它们保持在连续的内存段中。这当然有一个缺点：在读取过程中，必须遍历所有增量并将其应用到基节点上，以重建实际的节点状态。这有点类似于LA树所做的事情(参见6.3.2节)：更新与主结构分开存放，并在读取时重放。

### 6.5.2 用CAS控制并发

​	维护一个允许将数据项插到子节点之前的磁盘树状结构的成本相当高：它要求我们不断地更新父节点指向最新增量的指针。这就是为什么由增量节点和基节点链接在一起而组成的Bw树节点具有逻辑标识符，并使用一个从标识符到其在磁盘上位置的内存映射表。使用这种映射还帮助我们摆脱闩锁：Bw树不是在写入时获取独占所有权，而是对映射表中的物理偏移量使用CAS原子操作。

​	要更新一个Bw树节点，该算法执行以下步骤：

1. 通过从根到叶遍历树来定位目标逻辑叶节点。映射表包含了到更新链中的目标基节点或最新增量节点的虚拟链接。
2. 使用指向在步骤1中定位的基节点(或指向最新的增量节点)的指针来创建一个新的增量节点。
3. 用指向步骤2期间创建的新增量节点的指针来更新映射表。

​	步骤3中的更新操作可以使用CAS这个原子操作来完成，因此与指针更新并发的所有读取都被安排在写入之前或之后，读取者或写入者都不会被阻塞。在此之前的读取是沿着旧指针进行的，看不到新的增量节点，因为它尚未被放置。而在此之后的读取则是沿着新指针进行的，可以观察到更新。如果两个线程试图将一个新的增量节点放置到同个逻辑节点，那么只有其中一个线程可以成功，而另一个线程必须重试该操作。

### 6.5.3 结构修改操作

​	Bw树的逻辑结构类似于B树，这意味着节点仍然可能增长得太大(溢出)或缩小到几乎为空(下溢)，并且需要结构修改操作(Structure Modification Operation，SMO)，例如分裂和合并。这里分裂与合并的语义与B树相似(参见2.3.6节和2.3.7节)，但它们的实现方式不同。

​	分裂SMO是从合并被分裂节点的逻辑内容开始的，也就是将其增量应用于其基节点，并创建一个包含分裂点右侧元素的新页。在此之后，该过程分两步进行[WANG18]：

1. 分裂——一个特殊的分裂增量节点被附加到要分裂的节点，以通知读取者正在进行分裂。这个分裂增量节点持有一个中点分隔符键以使分裂节点中的记录变得无效，并且其还持有一个到新的逻辑同级节点的链接。
2. 父节点更新——在这一点上，情况类似于Blink树的半分裂(参见5.3.8节)，因为该节点通过分裂增量节点的指针而变为可用，但其尚未被父节点引用，并且读取者必须通过旧节点遍历同级指针以到达新创建的同级节点。一个新节点作为子节点被添加到父节点，这样读取者就可以直接到达它，而不是通过分裂节点被重定向，此时分裂就完成了。

​	更新父指针是一种性能优化：即使父指针从未更新，所有节点及其元素仍可访问。**Bw树是无闩锁的，因此任何线程都可能碰到不完整的SMO**。在继续之前，线程需要通过拾取并完成一个多步SMO来进行协作。下一个线程将跟随已放置的父指针，而不必遍历同级指针。

​	合并SMO的工作方式也是类似的：

1. 删除同级节点——创建一个特殊的删除增量节点并将其追加到右同级节点，它代表合并SMO的开始，并标记右同级节点以便删除。
2. 合并节点——在左同级节点上创建一个合并增量节点，指向右同级节点的内容，并使其成为左同级节点的逻辑组成部分。
3. 父节点更新——此时，可以从左边的同级节点访问右边同级节点的内容。要完成合并过程，必须从父节点中删除到右同级节点的链接。

​	并发SMO需要在父节点上放置一个附加的中止增量节点，以防止并发分裂和合并[WANG18]中止增量节点与写入锁的工作方式类似：一次只能有一个线程具有写访问权限，任何试图向该增量节点追加新记录的线程都将被中止。在SMO完成时，可以从父节点删除该中止增量节点。

​	Bw树的高度在根节点分裂期间增长。当根节点变得太大时，它会一分为二，并且创建个新的根来代替旧的根，旧的根和一个新创建的兄弟节点将作为新根的子节点。

### 6.5.4 合并和垃圾收集

​	增量链可以变得任意长而不需要任何附加动作。增量链越长，读取的开销就越大，因此我们需要尝试将增量链的长度保持在合理的范围内。当它达到一个可配阈值时，我们通过将基节点的内容与所有增量合并(consolidate)来重建节点，从而生成一个新的基节点。随后将新节点写入磁盘上的新位置，并更新映射表中的节点指针以指向该节点。我们将在7.7节中更详细地讨论这个过程，因为底层的日志结构存储负责垃圾收集、节点合并和移动。

​	一旦节点被合并，它的旧内容(基节点和所有增量节点)就不能再从映射表中被寻址。然而，我们不能立即释放其占用的空间，因为其中一些可能仍被正在进行的操作所使用。由于读取者没有闩锁(读取者不必穿过或注册任何类型的屏障来访问节点)，我们需要找到其他方法来跟踪存活的页。

​	为了将可能已经访问过某个特定节点的线程与可能还未看到它的线程区分开，Bw树使用了一种称为基于 epoch的回收(epoch-based reclamation)的技术。如果某些节点与增量节点由于在某个epoch期间进行了合并替换而从映射表中被移除，则原始节点将被保留，直到在同一epoch或更早的epoch期间启动的每个读取者都完成了读操作为止。此后可以安全地对它们进行垃圾收集，因为可以保证之后的读取者从未看到过这些节点在这些读取者启动时，那些节点已经是不可寻址的。

​	Bw树是一个有趣的B树变体，其在如下几个重要方面进行了改进：写放大、非阻塞访问和缓存友好性。实验性的存储引擎Sled实现了它的一个修改版本。而CMU数据库组开发了Bw树的一个内存版本，称为OpenBw树，并发布了一个实用的实现指南[WANG18]。

​	在本章中，我们只涉及较高级的与B树有关的Bw树概念，我们将在77节中继续讨论它们，包括关于底层日志结构存储的讨论。

## 6.6 缓存无关B树

​	块大小、节点大小、高速缓存行对齐和其他可配参数会影响B树的性能。一种称为缓存无关结构的新数据结构类型[DEMAINE02]给出了渐近最优的性能，而无须考虑底层的内存层次结构和对上述参数的调整。这意味着该算法不需要知道缓存行、文件系统块和磁盘页的大小。<u>缓存无关结构的设计使得其在多种不同配置的机器上无须修改便能很好地运行</u>。

​	到目前为止，我们主要是从双层存储层次结构来看待B树的(除了6.1节中描述的LMDB)。**B树节点存储在磁盘驻留页中，使用页缓存使得可以在主内存中对它们进行高效的访问**。

​	这个层次结构的两层分别是页缓存(更快速，但是空间受限)和磁盘(一般较慢，但是有更大的容量)[AGGARWAL88]。在这里，我们只有两个参数，这使得设计算法变得相当容易，因为我们只需要两个特定于某层的代码模块来处理与该层相关的所有细节即可。

​	**磁盘被划分成块，数据在磁盘和缓存之间以块为单位进行传输：即使算法必须在块中定位单个项目，也必须加载整个块。这种方法叫作缓存感知(cache-aware)**。

​	在开发性能要求高的软件时，我们通常要基于更复杂的模型来编程，此时我们需要考虑到CPU缓存，有时甚至会考虑到磁盘层次结构(如热/冷存储或构建HDD/SSD/NVM层次结构，并将数据从一层淘汰至另一层)。在大多数情况下，这些努力很难通用化。在1.2节中，我们谈到了访问磁盘比访问主存慢几个数量级，这激发了数据库实现者来针对这种差异进行优化。

​	缓存无关算法允许我们以两层存储模型对数据结构进行推断，同时具备多级层次结构模型的优点。这种方法可以没有特定平台的参数，但是仍保证了层次结构的两个级别之间的传输数量在一个常数内。如果数据结构专门为在任何双层存储层次结构中最优化运行进行了优化，则它也可以在任何两个相邻存储层次结构上最优化运行。这是通过尽可能地在最高缓存层下进行工作来实现的。

---

+ van Emde Boas布局

  ​	一个缓存无关B树由一个静态B树和一个打包的数组结构组成[BENDER05]。静态B树使用van Emde Boas布局构建。它在中间层的边上将树分裂开，然后以类似的方式递归分裂每个子树，最后得到大小为sqr(N)的子树。这种布局的关键思想是，任何递归树都存储在一个连续内存块中。

  ​	为了使数据结构动态化(即允许插入、更新和删除)，缓存无关树使用打包数组(packed array)数据结构，该数据结构使用连续的内存段来存储元素，但为将来插入的元素保留间隙。间隙之间的间隔基于密度阈值(density threshold)。

  ​	这种方法可以用较少的移动将数据项插入树。如果不存在空隙，则必须移动数据项，以便为新插入的元素腾出空隙。当打包数组变得太密集或太稀疏时，为了增大或缩小数组，必须重建它的结构。

  ​	静态树则被用作底层打包数组的索引，并且必须根据被移动的元素进行更新，以指向底层中的正确元素。

  ​	这是一种有趣的方法，其中的思想可以被用来构建高效的B树实现。它允许以非常类似于主存结构的方式构造磁盘结构。然而，到本文撰写之日为止，作者还不知道有任何学术界之外的缓存无关B树的实现。

  ​	一个可能的原因是这样一种假设：如果缓存加载被抽象掉，那么当数据以块的形式加载和写回时，分页和换出缓存仍然对结果有负面影响。另一个可能的原因是，从块传输的角度来看，缓存无关B树与缓存感知B树是相同的。当更高效的非易失性字节可寻址存储设备普及时，情况可能会改变。

## 6.7 本章小结

​	B树的原始设计有几个缺点，这些缺点在机械磁盘上没有太大影响，但在固态硬盘上使用时会使其效率降低。**<u>B树有很高的写放大(由页重写引起)和很高的空间开销，因为B树必须在节点中为将来的写入保留空间</u>**。

​	**可以使用缓冲来减少写放大**。惰性B树(如WiredTiger和LA树)将内存缓冲区附加到单个节点或节点组，通过缓冲页中的后续更新来减少所需IO操作的数量。

​	**为了减少空间放大，FD树使用了不可变性**：数据记录存储在不可变的有序段中，而可变B树的大小是有限的。

​	**Bw树也利用不可变性来解决空间放大问题**。B树节点和对它们的更新被存储在分离的磁盘位置上，并持久保存在日志结构存储中。与原始B树设计相比，其写入放大被减少，因为对属于单个逻辑节点内容的协调相对不频繁。Bw树不需要闩锁来保护页以免受并发访问的影响，因为内存中存储的是逻辑节点之间的虚拟指针。

# 7. 日志结构存储

​	当会计师不得不修改记录时，他们不是抹去现有的值，而是创建一条新的修正记录。当季度报告公布时，它可能包含一些小的修改，以纠正上一个季度的结果。为了得出最终损益，你必须依次处理这些记录并计算部分和[HELLAND15]。

​	类似地，**不可变的存储结构不允许修改现有文件：表只被写入一次而不会再被修改**。<u>取而代之的是，新的记录被附加到新的文件中。为了找到最终的值(或者确定它不存在)，必须**从多个文件中重构记录**</u>。**与之相反，可变存储结构将原地修改磁盘上的记录**。

​	不可变数据结构经常在函数式编程语言中使用，并且由于其**安全性**而变得越来越受欢迎：不可变结构一旦被创建就不会改变，它的所有引用都可以被并发地访问，并且它的完整性由不可修改这一事实来保证。

​	从高层级看来，在存储结构的内部和外部，对待数据的方式有严格的区别。**在内部，不可变文件可以保存多个副本，更新的副本将覆盖旧的副本，而可变文件通常只保存最新的值**。当被访问时，不可变文件被处理，冗余副本需要进行协调，最新的副本会返回客户端。

​	正如其他有关该主题的书籍和论文所做的那样，**我们使用B树作为可变数据结构的典型示例，而使用日志结构合并树(Log-Structured Merge Tree，LSM树)作为不可变结构的示例**。不可变LSM树使用仅追加存储和合并协调，而B树则在磁盘上定位数据记录并在文件中的原始偏移量上更新页。

​	原地更新存储结构针对读取进行了性能优化[GRAEFE04]：当在磁盘上定位数据之后，就可以将记录返回客户端。这是以牺牲写入性能为代价的：要在原地更新数据记录，首先必须在磁盘上进行定位。<u>另一方面，仅追加存储是对写入性能进行的优化。写操作不必在磁盘上找到记录来覆盖它们。然而，这是以读取性能为代价的，读取必须检索多个数据记录版本并对它们进行协调</u>。

​	到目前为止，我们主要讨论的是可变存储结构。在讨论写时复制B树(参见6.1节)、FD树(参见6.4节)和Bw树(参见6.5节)时，我们已经涉及了不可变性的主题，而实现不可变结构的方法还有更多。

​	<u>由于可变B树所采用的结构和构建方法，读写和维护期间的大多数IO操作都是随机的</u>。每个写操作首先需要找到保存数据记录的页，然后才能修改它。B树要求对已写入的记录进行节点分裂和合并。一段时间之后，B树的页可能需要进行维护。页的大小是固定的，并且为将来的写入保留了一些可用空间。**另一个问题是，即使只修改了页中的个单元格，也必须重写整个页**。

​	有一些替代方法可以帮助缓解这些问题，使一些IO操作顺序化，并避免在修改期间的页重写。这样做的方法之一就是使用不可变数据结构。在本章中，我们将关注LSM树它是如何构建的，它的属性是什么，以及它与B树有何不同。

## 7.1 LSM树

​	在讨论B树时，我们得出结论：使用缓冲可以改善空间开销和写入放大问题。通常，在不同的存储结构中有两种应用缓冲的方式：**推迟对磁盘驻留页的写入传播**(正如我们在6.4节和6.3.1节中看到的)，以及**使写操作顺序化**。<u>LSM树是最流行的不可变磁盘存储结构之一，它使用**缓冲**和**仅追加存储**来实现顺序写操作</u>。

​	LSM树是类似于B树的磁盘驻留结构的变体，其中节点完全被填满，并为顺序磁盘访问进行了优化。这个概念是在Patrick O'Neil 和 Edward Cheng的一篇论文中被首次提出的[ONEIL96]。日志结构合并树的名称取自日志结构文件系统，这种文件系统将磁盘上的所有修改写入一个类似日志的文件[ROSENBLUM92]。

> LSM树写入不可变的文件，并随着时间的推移将它们合并在一起。这些文件通常包含它们自己的索引，以帮助读取者高效地定位数据。尽管LSM树通常被用作B树的一种替代，但**B树通常被用作LSM树的不可变文件的内部索引结构**。

​	LSM树中的"合并"(merge)一词表示：由于它的不变性，需要使用类似于归并排序(merge sort)的方法合并树的内容。<u>这一过程会发生在回收冗余副本所占空间的维护期，也会发生在向用户返回内容之前的读取期</u>。

​	LSM树将数据文件写入推迟，并将更改缓冲在一个内存驻留表中。表中的内容随后将被写到不可变磁盘文件中。在文件完全持久化之前，所有数据记录仍然可以在内存中访间。

​	**保持数据文件的不变性有利于顺序写入：数据被一次性写入磁盘，文件是仅追加的**。可变结构也可以一次性预分配数据块(例如，索引顺序访问方法(ISAM)[RAMAKRISHNAN03，LARSON81])，但是后续访问仍然需要随机读写。而<u>不可变结构则允许我们按顺序放置数据以防止产生碎片。此外，不可变文件具有更高的数据密度：我们无须为之后将要写入的数据记录保留任何额外的空间，也不会因更新后的记录可能比最初写入的记录需要更多空间而为其保留额外的空间</u>。

​	由于文件是不可变的，所以插入、更新和删除操作不需要在磁盘上定位数据记录，这显著提高了写入的性能和吞吐量。**另一方面，重复内容是允许的，且要在读取时解决冲突**。LSM树在写远大于读的应用程序中特别有用。考虑到数据量和采集速率在不断增长，在现代数据密集型系统中这种情况是常见的。

​	<u>读和写在设计上并不交又，因此可以在没有段级锁的情况下读写磁盘上的数据，这大大简化了并发访问</u>。相反，可变结构采用层级锁和闩锁(你可以在5.3节中找到更多关干锁和闩锁的信息)来确保磁盘数据结构的完整性并允许并发读取，但**要求写入者对子树具有独占的所有权**。基于LSM的存储引擎使用数据和索引文件的线性化内存视图，并且只需对管理它们的结构进行并发访问保护。

​	B树和LSM树都需要一些内务处理(house keeping)来优化性能，但原因不尽相同。由于分配文件的数量稳定增长，所以LSM树必须合并和重写文件，以确保在读取过程中访问尽可能少的文件，因为请求的数据记录可能分布在多个文件中。另一方面，可变文件可能必须被部分或全部重写，以减少碎片并回收被更新或删除的记求所占用的空间。当然，内务处理的确切工作很大程度上取决于具体实现。

### * 7.1.1 LSM树的结构

> [LSM树详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/181498475)

​	我们从有序的LSM树[ONEILS96]开始介绍，其中文件保存排序后的数据记录。稍后，在7.4节中，我们还将讨论允许以插入顺序记录存储数据的结构，其在写入路径上具有一些明显的优势。

​	正如我们刚才所讨论的，LSM树由较小的内存驻留组件和较大的磁盘驻留组件组成。要在磁盘上写出不可变的文件内容，首先需要在内存中对其进行缓冲和排序。

​	<u>**内存驻留组件(通常称为memtable)是可变的**：它缓冲数据记录，并充当读写操作的目标。当其大小达到一个可配阈值时，memtable中的内容将会被持久化到磁盘上</u>。memtable的更新不需要磁盘访问，也没有相关的IO开销。需要一个单独的预写日志文件(类似于我们在5.2节中所讨论的)以保证数据记录的持久性。**在向客户端确认操作之前，数据记录会被追加到日志中并提交到内存**。

​	缓冲是在内存中完成的：所有读写操作都应用于一个内存驻留表，该表维护一个允许并发访问的有序数据结构，其通常是某种形式的内存排序树，或任何可以提供类似性能和特征的数据结构。磁盘驻留组件则是通过将内存中缓冲的内容刷写到磁盘来构建的。

​	<u>**磁盘驻留组件仅用于读取**：缓存的内容被持久化成文件，而**这些文件永远不会被修改**</u>。

​	这允许我们从简单操作的角度来考虑：**对内存中的表进行写操作，以及对磁盘和基于内存的表进行读、合并和文件删除操作**。

​	在本章中，我们将使用"表"这个词作为磁盘驻留表的简写。由于我们讨论的是存储引擎的语义，所以这个术语不会与数据库中的"表"概念混淆。

---

+ 双组件LSM树

  ​	我们要区分双组件和多组件的LSM树。双组件LSM树只有一个磁盘组件，由不可变段组成。这里的磁盘组件被组织为B树，它具有100%的节点占用率和只读的页。

  ​	内存驻留树的内容被分块刷写到磁盘上。在刷写期间，对于每个被刷写的内存驻留子树，我们在磁盘上找到一个相应的子树，并将内存驻留段和磁盘驻留子树的合并内容写入磁盘上的新段。

  ​	<u>在刷写子树之后，原来的内存驻留子树和磁盘驻留子树将被丢弃，并被它们合并后的结果所替换</u>。在替换后，该结果就可以从磁盘驻留树中先前存在的部分被寻址到。

  ​	合并可以通过同时步进两个迭代器来实现，这两个迭代器分别读取磁盘驻留树的叶节点和内存树中的内容。因为两个数据源都是有序的，所以要产生有序的合并结果，我们只需要在合并过程的每个步骤中知道两个迭代器的当前值。

  ​	这种方法是我们关于不可变B树的讨论的逻辑扩展与延续。写时复制B树(参见6.1节)使用B树结构，但它的节点没有被完全填满，它要求在根叶路径上复制页，并创建一个平行的树结构。在这里，我们做了类似的事情，但是因为我们在内存中缓冲写操作，所以磁盘驻留树的更新成本被均摊了。

  ​	在实现子树合并和刷写时，我们必须确保三件事：

  1. 刷新过程一旦开始，所有新的写操作都必须转到新的memtable。
  2. **在子树刷写期间，磁盘驻留子树和正在刷写的内存驻留子树都要保持可以被读取的状态**。
  3. 在刷写之后，如下操作必须以**原子**的方式执行：<u>发布合并的内容、丟弃原始的磁盘驻留与内存驻留的内容</u>。

  ​	尽管双组件LSM树对维护索引文件很有用，但在撰写本书时，作者还不知道有这样实现的系统。这可以用该方法的写放大特性来解释：合并相对频繁，因为它们是由memtable刷写所触发的。

---

+ 多组件LSM树

  ​	让我们考虑另一种设计，即多组件LSM树，它具有不止一个磁盘驻留表。在这种情况下，整个memtable内容在一次运行中被刷写。

  ​	我们很快就会发现，在多次刷写之后将会得到多个磁盘驻留表，而且它们的数量只会随着时间的推移而增长。**<u>由于我们并不总是确切地知道哪些表持有我们所需的数据记录，所以我们可能不得不访问多个文件来定位要搜索的数据</u>**。

  ​	<u>从多个数据源而不是仅从一个数据源读取数据的代价较大。为了缓解这个问题并将表的数量维持在最少，需要触发一个称为压实(compaction)的周期性合并过程(参见7.1.6节)</u>。压实选择几个表，读取它们的内容，合并它们，并将合并的结果写到新文件中。旧表在新表出现的同时被丢弃。

  ​	**数据首先缓存在内存驻留组件中。当它变得太大时内容会被刷写到磁盘上以创建磁盘驻留表。然后，多个表合并在一起，以创建出更大的表**。

  ​	本章的其余部分将会介绍多组件LSM树、块构建及其维护过程。

---

+ 内存表

  ​	刷写memtable可以被周期性地触发，也可以通过大小阈值来触发。**在其可被刷写之前，必须进行memtable切换：分配一个新的memtable，它成为所有新的写入操作的目标，而旧的memtable则变成刷写状态**。**<u>这两个步骤必须被原子地执行</u>**。在其内容被完全刷写之前，被刷写的memtable仍可用于读取。在此之后，旧的memtable将被丢弃，取而代之的是一个新写入的磁盘驻留表，该表可用于读取。

  + 当前的memtable

    接收写请求并服务读请求。

  + 正在刷写的memtable

    可用于读取。

  + 磁盘上的刷写目标

    不参与读取，因为其内容不完整。

  + 已刷写的表

    一旦被刷写的memtable被丢弃，就可以被用于读取。

  + 正在被压实的表

    当前正在被合并的磁盘驻留表。

  + 已被压实的表

    由已刷写的表或其他已压实的表所创建。

  ​	数据已经在内存中进行了排序，因此可以通过将内存驻留的内容依次写入磁盘来创建磁盘驻留表。**在刷写期间，正在刷写的memtable和当前的memtable都可供读取**。

  ​	**在完全刷写memtable之前，其内容唯一的磁盘驻留版本储存在预写日志中**。

  ​	**当memtable内容被完全刷写到在磁盘上时，日志可以被修剪(trim)，并且保存对被刷写过的memtable进行的操作的日志段可以被丢弃**。


### * 7.1.2 更新与删除

​	在LSM树中，插入、更新和删除操作不需要在磁盘上定位数据记录。但是，在读取的过程中需要对冗余的记录进行协调。

​	**从memtable中删除数据记录是不够的，因为其他磁盘或内存驻留表可能持有同一个键的数据记录。如果仅通过从memtable中移除数据项来实现删除，那么要么最终删除无效，要么我们会复活(resurrect)先前的数值**。

​	让我们考虑一个例子。被刷写的磁盘驻留表中包含与记录v1相关联的键k1，而memtable则保存其新值v2：

```pseudocode
	 磁盘表					memtable
| k1 | v1 |			| k1 | v2 |
```

​	<u>如果我们只是将v2从 memtable中移除并刷写它，我们就实际上复活了v1</u>，因为它成为与该键相关联的唯一值：

```pseudocode
	 磁盘表					memtable
| k1 | v1 |					∅
```

​	因此，**删除操作需要被显式地记录下来**。这可以通过插入一个特殊的删除条目(delete entry，有时称为**墓碑(tombstone)**或**休眠凭证(dormant certificate)**)来实现，该条目说明与此键对应的数据记录已被删除：

```pseudocode
	 磁盘表					memtable
| k1 | v1 |		 | k1 | <墓碑> |	
```

​	**协调过程如果看到墓碑，就会过滤掉其所遮蔽的值**。

​	<u>有时，删除连续范围的键而不是单一的键可能很有用，这可以使用**谓词删除**来完成，谓词删除的工作方式是在删除条目中附加一个谓词，该谓词根据常规的记录排序规则进行排序。**在协调期间，与谓词匹配的数据记录将被跳过而不会返回客户端**</u>。

​	谓词可以采用`DELETE FROM tab1e WHERE key ≥ "k2" AND key < "k4"`的形式，并接受任何范围匹配。 Apache Cassandra实现了这个方法并将其称为**范围墓碑**。<u>一个范围墓碑覆盖了一个键的范围而不是单个键</u>。

​	<u>在使用范围墓碑时，由于范围可能和磁盘驻留表边界重叠，所以必须仔细考虑解析规则</u>。例如，以下组合将从最终结果中隐去与k2和k3关联的数据记录：

```pseudocode
	 磁盘表1								磁盘表2
| k1 | v1 |		 | k2 | <起始_墓碑_包含> 	 |	
| k2 | v2 |		 | k4 | <终止_墓碑_不包含> |	
| k3 | v3 |
| k4 | v4 |
```

### 7.1.3 LSM树的查找

​	LSM树由多个组件组成。**查找过程通常会访问多个组件，因此在将其返回客户端之前，必须对其内容进行合并和协调**。为了更好地理解合并过程，我们来看看表在合并过程中是如何迭代的，以及冲突的记录是如何组合的。

### 7.1.4 合并迭代

​	由于磁盘驻留表上的内容是经过排序的，所以我们可以使用多路归并排序算法。例如，我们有三个数据源：两个磁盘驻留表和一个memtable。<u>**通常，存储引擎提供游标或迭代器来遍历文件内容**。此游标保存上次消耗的数据记录的偏移量，可以用来检查迭代是否完成，也可以用于抽取下一个数据记录</u>。

​	多路归并排序使用一个优先级队列，如最小堆(min-heap)[SEDGEWICK11]，该队列最多保存N个元素(其中N是迭代器的数目)，该队列对其内容进行排序并准备返回的下个最小的元素。每个迭代器的头被放入队列。队列头部的元素就是所有迭代器的最小值。

> ​	优先队列是一种数据结构，用于维护一个有序数据项队列。当常规队列按照添加的顺序(先进先出)保留项目时，优先级队列在插入数据项时进行重新排序，具有最高(或最低)优先级的数据项被放在队列的头部。这对于合并迭代特别有用，因为我们必须有序地输出元素。

​	当从队列中移除最小的元素时，检査与之相关联的迭代器是否有下一个值，然后将该值放入队列，队列将被重新排序以维持顺序。

​	因为所有迭代器中的内容都是有序的，所以从之前的最小元素对应的迭代器取出下一个值并插入优先队列也维持了一个不变式——队列仍然包含来自所有迭代器的最小元素。毎当其中一个迭代器耗尽时，就不再插入这个迭代器的值，但是算法仍然继续执行。该算法持续执行到査询条件被满足或所有迭代器耗尽为止。

​	在合并迭代过程中，可能会有多个数据记录具有相同的键。从优先级队列和迭代器不变式中可知，如果在每个迭代器中每个键只持有单个数据记录，而我们又最终在队列中发现多个数据记录且有相同的键，那么这些数据记录一定来自不同的迭代器。

​	让我们一步步地完成一个示例。作为输入数据，我们在两个磁盘驻留表上的选代器：

```pseudocode
迭代器1：							迭代器2:
{k2: v1} {k4: v2}		{k1: v3} {k2: v4} {k3: v5}
```

​	使用迭代器头部元素填充优先级队列：

```pseudocode
迭代器1：		迭代器2:							优先级队列：
{k4: v2}	{k2: v4} {k3: v5} 	{k1: v3} {k2: v1}
```

​	键k1是队列中最小的键，将其追加到结果集中。因为它来自迭代器2，所以我们将这个迭代器的下一个元素重新填充进队列：

```pseudocode
迭代器1：		迭代器2:		优先级队列：					合并后的结果集：
{k4: v2}	{k3: v5} 	{k2: v1} {k2: v4}		 {k1: v3}
```

​	此时，我们在队列中有两个记录为k2的键。可以肯定的是，由于前述的不变式，在任何迭代器中都没有其他记录具有相同的键。相同键的数据记录在被合并后会被追加到合并后的结果集中。

​	使用来自两个迭代器的数据重新填充这个队列：

```pseudocode
迭代器1：		迭代器2:		优先级队列：					合并后的结果集：
{}				{} 				{k3: v5} {k4: v2}		 {k1: v3} {k2: v4}
```

​	由于现在所有迭代器都是空的，所以我们将剩余的队列内容追加到结果集中：

```pseudocode
 合并后的结果集:
 {k1: v3} {k2: v4} {k3: v5} {k4: v2}
```

​	总结一下，创建合并迭代器必须重复以下步骤：

1. 用每个迭代器的第一项填充优先队列。
2. 从优先队列中取出最小的元素(队列头部)
3. 从相应的迭代器取出一个值重新填入优先队列，除非该迭代器耗尽。

​	合并迭代器与合并有序集合的时间复杂度相同。其内存开销为O(N)，其中N是迭代器的数目。平均情况下维护迭代器头部有序集合的时间复杂度是O(logN) [KNUTH98]。

### * 7.1.5 协调

​	合并迭代只是从多个数据源合并数据的一个方面。另一个重要方面是与同一键相关联的数据记录的协调和冲突解决。

​	不同的表可能持有相同键的数据记录，例如更新和删除，这时必须对它们的内容进行协调。前例中的优先级队列的实现必须能够允许插入与相同键关联的多个值，并触发协调。	

> ​	**如果记录不存在，则将其插入数据库，否则更新现有的值，这种操作称为upsert。<u>在LSM树中，插入和更新操作是无法区分的，因为它们并不试图在所有源中找到先前与该键相关联的数据记录并重新设置其值，所以我们可以说在默认情况下upsert记录</u>**。

​	为了协调数据记录，我们需要了解它们中的哪一个是优先的。数据记录持有这样做所必需的元数据，例如时间戳。我们可以通过比较时间戳来建立来自多个数据源的数据项之间的顺序，并找出哪个更新。

​	**被具有更高时间戳的记录所遮蔽的记录不会返回给客户端，在压实期间也不会被写入**。

### * 7.1.6 LSM树的维护

​	与可变B树类似，LSM树也需要维护。该过程的性质很大程度上受到这些算法所保持的不变式的影响。

​	<u>在B树中，维护过程收集未引用的单元格，对页进行碎片整理，并回收被移除和遮蔽的记录所占用的空间。在LSM树中，磁盘驻留表的数量不断增长，但可以通过触发周期性的压实来减少</u>。

​	压实会挑选多个磁盘驻留表，使用前面提到的合并和协调算法迭代它们的全部内容，并将结果写入新创建的表。

​	由于磁盘驻留表内容是有序的，而且由于归并排序的工作方式，压实操作有一个理论上的内存使用上限，因为它只在内存中保存迭代器头部元素。所有表的内容都是按顺序消耗的，合并的结果数据也是按顺序写出的。由于一些额外的优化，这些细节在不同的实现之间可能会略有差异。

​	**在压实过程完成之前，正在被压实的表仍可用于读取。这意味着在压实期间，磁盘上需要有足够的可用空间来写入压实的表**。

​	系统中可以同时执行多个压实操作。然而，这些并发的压实操作通常使用不相交的表集合。压实的写入者既可以将多个表合并为一个，也可以将一个表划分为多个表。

> + 墓碑和压实
>
> ​	**墓碑表示了正确协调所需的重要信息，因为某些其他表可能仍然保存着墓碑所遮蔽掉的过期数据记录**。
>
> ​	**在压实过程中，墓碑不会被立即丢弃。它们会被保留，直到存储引擎可以确定在任何其他表中都不存在时间戳更小且键相同的数据记录**。 RocksDB保留墓碑直到它们到达最底层。而 Apache Cassandra保留墓碑直到它们到达GC(垃圾收集)的宽限期，这是因为数据库的最终一致性，这样做可以确保其他节点观察到墓碑。**在压实过程中保留墓碑对于避免数据复活很重要**。

---

+ 分层压实

  ​	压实提供了多种优化机会，并且有许多不同的压实策略。其中一种常用的压实策略称为层级压实(levered compaction)，例如RocksDB就使用这一策略。

  ​	**层级压实将磁盘驻留表划分为多个层级。每个层上的表都有目标大小，每个层都有相应的序号(标识符**)。有些违反直觉的是，序号最高的层被称为最底层。为清楚起见，本小节避免使用“较髙层”和“较低层”这两个术语，并对层序号使用相同的限定词。也就是说，由于2大于1，所以2层有着高于1层的序号。术语“之前的”和“之后的”与层序号具有相同的顺序语义。

  ​	**0层表是通过刷写memtable的内容而创建的。0层中的表可能包含重叠的键范围。而0层上的表数量只要一达到某个阈值，它们的内容就会被合并，并创建一个层级为1的新表**。

  ​	<u>1层和所有序号更高的层上的表都不存在键范围上的重叠</u>，因此在压实期间必须将0层表分区，分裂成多个范围，然后将其与持有相应键范围的表进行合并。或者，压实可以读入所有0层和1层的表，然后输出分区过的1层的表。

  ​	在序号更高的层上进行压实是从两个范围重叠的连续层中挑选表，并在较高的层上生成个新的表。压实1层表和2层表的过程将产生一个位于2层的新表。根据表的分区方式，可以从一层中挑选多个表进行压实。

  ​	**在不同的表中保持不同的键范围减少了在读取期间要访问的表的数量。这是通过检查表的元数据并过滤掉范围不包含搜索键的表来实现的**。

  ​	**每一层都有表大小和最大表个数的限制。在1层或任何具有较高序号的层中，一旦表的数量达到一个阈值，来自当前层的表就与键范围重叠的下一层的表进行合并**。

  ​	层的大小在层与层之间是指数增长的：每一个层上的表都要比上一层上的表以指数级增大。**<u>这样，最新的数据总是在序号最低的层上，而较旧的数据逐渐迁移到序号较高的层上</u>**。

---

+ 按大小分层压实

  ​	另一种流行的压实策略称为按大小分层压实(size-tiered compaction)。在按大小分层压实中，磁盘驻留表不是根据其层级进行分组，而是按照大小进行分组：较小的表与较小的表分在一起，较大的表则与较大的表分在一起。

  ​	0层保存最小的表，这些表要么是从memtable中被刷写的，要么是通过压实过程来创建的。当表被压实时，所产生的合并表被写入持有相应大小的表的层。该过程持续递归地增加层，将较大的表压实并提升到更高的层，并将较小的表降级为较低的层。

  > ​	按大小分层压实的一个问题叫作**表饥饿**：
  >
  > ​	**<u>如果压实后的表仍然足够小(例如，数据记录被墓碑遮蔽，没有进入合并的表)，则更高的层可能产生压实饥饿，其墓碑将一直不被回收，从而增加读取的开销。在这种情况下，对于一个层将不得不进行强制压实，即使它没有包含足够数量的表</u>**。

  ​	还有其他一些常见的压实策略实现，可以针对不同的工作负载对其进行优化。例如，Apache Cassandra还实现了一个<u>时间窗口压实策略</u>，该策略对于具有生存期(time-tolive)的时间序列型工作负载(换句话说，<u>数据项必须在给定的一段时间之后过期</u>)特别有用。

  ​	<u>时间窗口压实策略将写入时间戳考虑在内，允许整个丢弃那些持有已过期时间范围的数据的文件，而不需要对它们的内容进行压实和重写</u>。

## * 7.2 读写放大与空间放大

> [写入放大 - 维基百科，自由的百科全书 (wikipedia.org)](https://zh.wikipedia.org/wiki/写入放大)

​	在实现最优压实策略时，我们必须考虑多个因素。

​	<u>一种方法是回收重复记录占用的空间，减少空间开销，但这会产生由不断重写表导致的更高的写放大。替代方案是避免连续重写数据，而这又增加了读放大(在读取期间协调关联到相同键的数据记录的开销)和空间放大(因为冗余记录会被保存更长时间)</u>。

> ​	数据库界的一大争议是B树和LSM树之中到底哪个的写放大较低。对于二者，理解写放大的来源是极其重要的。
>
> + **在B树中，写放大来自回写操作以及后续对同一节点的更新**。
> + **而在LSM树中，写放大是由在压实过程中将数据从一个文件迁移到另一个文件引起的。直接比较两者可能会导致不正确的结论**。

​	总结起来，当以不可变的方式在磁盘上存储数据时，我们面临三个问题：

+ **读放大**

  由为了检索数据而需要读取多个表所引起。

+ **写放大**

  由压实过程中不断进行的重写所引起。

+ **空间放大**

  由存储关联到同一键的多个记录所引起。

​	在本章的剩余部分，我们将逐一对这些问题进行讨论。

---

+ **RUM猜想**

  ​	有一种流行的存储结构开销模型考虑了如下三个因素：**读取(Read)、更新(Update)和内存(Memory)开销**。它被称为RUM猜想[ATHANASSOULIS16]。

  ​	**<u>RUM猜想指出，减少其中两项开销将不可避免地导致第三项开销的恶化，并且优化只能以牺牲三个参数中的一个为代价</u>**。我们可以根据这三个参数对不同的存储引擎进行比较，以了解它们针对哪些参数进行了优化，以及其中隐含着哪些可能的权衡。

  ​	<u>一个理想的解决方案是拥有最小的读取开销，同时保持较低的内存与写入开销。但在现实中，这是无法实现的，因此我们需要进行取舍</u>。

  ​	**B树是针对读进行优化的**。对B树的写入需要在磁盘上找到一个记录，而随后对同一页的写入可能需要多次更新该磁盘页面。还要为将来的更新和删除保留额外的空间，这会增加空间开销。

  ​	而LSM树则不需要在写入期间定位磁盘上的记录，也不用为以后的写入保留额外的空间。但由于存储冗余记录，所以仍然存在一些空间开销。<u>在默认配置中，由于必须通过访问多个表才能返回完整的结果，所以读取的成本更高</u>。但是，本章中讨论的优化有助于缓解这个问题。

  ​	正如我们在关于B树的章节中已经看到、在本章中将要看到的那样，有一些方法可以通过使用不同的优化手段来改善这些特性。

  ​	<u>这个开销模型并不完美，因为它没有考虑其他重要的指标，如延迟、访问模式、实现复杂度、维护成本以及与硬件相关的细节</u>。对于分布式数据库很重要的概念，如**一致性含义**和**复制开销**，也没有被考虑进去。然而，该模型可以被用作初步评估或当作一种经验法则，它有助于我们理解存储引擎必须提供什么。

## 7.3 实现细节

​	我们已经介绍了LSM树的基本动态特性：数据是如何被读取、写入和合并的。然而，还有许多LSM树实现的共同点是值得讨论的：内存驻留表和磁盘驻留表是如何实现的，二级索引是如何工作的，如何减少读取时要访问的磁盘驻留表的数量，以及关于日志结构存储的新想法。

### * 7.3.1 有序字符串表(SSTable)

​	到目前为止，我们已经讨论了LSM树的层次结构和逻辑结构(它是由多个内存驻留组件与磁盘驻留组件构成的)，但是还没有讨论磁盘驻留表是如何实现的，以及其设计是如何与系统的其余部分配合而发挥作用的。

​	**磁盘驻留表通常使用有序字符串表(Sorted String Table， Sstable)来实现**。顾名思义SSTable中的数据记录是按照键顺序进行排序和布局的。 SSTable通常由两个组件组成：**索引文件**和**数据文件**。

​	<u>索引文件是用能够以对数时间复杂度(如B树)或常量时间复杂度(如哈希表)进行查找的某种结构来实现的</u>。

​	<u>由于数据文件以键顺序保存记录，所以使用哈希表进行索引并不妨碍我们实现范围扫描，因为**访问哈希表只是为了定位范围中的第一个键**，并且在范围谓词仍然匹配的情况下，范围本身可以顺序地从数据文件中读出</u>。

​	索引组件保存键和数据入口(实际数据记录在数据文件中的偏移量)。数据组件由连起来的键值对组成。我们在第3章中讨论的单元格设计和数据记录格式大都适用于SSTable。这里的主要区别在于，单元格是按顺序写入的，并且在SSTable的生命周期中不被修改。由于索引文件持有的指针指向存储在数据文件中的记录，所以在创建索引时必须知道它们的偏移量。

​	<u>在压实期间，可以顺序读取数据文件而不对索引组件进行寻址，这是因为它们中的数据记录已经是有序的</u>。由于在压实过程中合并的表具有相同的顺序，并且合并迭代是保留顺序的，所以合并结果表也是通过在单次运行中按顺序写入数据记录而创建的。一旦文件被完全写入，它就被认为是不可变的，其驻留在磁盘上的内容也不会被修改。

> SSTable附加二级索引
>
> ​	LSM树索引领域的一个有趣的发展是在Apache Cassandra中实现的SSTable附加二级索引(SSTable-Attached Secondary Index， SASI)。为了允许除通过主键之外还可以通过其他任何字段来索引表内容，索引结构及其生命周期与SSTable的生命周期相耦合，并且根据SSTable来创建索引。**当memtable被刷写时，它的内容被写入磁盘，此时二级索引文件与SSTable主键索引也一起被创建出来**。
>
> ​	<u>由于LSM树在内存中缓冲数据，索引必须对驻留在内存中的内容和驻留在磁盘上的内容同样有效，所以SASI维护了一个单独的内存结构，为memtable内容建立索引</u>。
>
> ​	在读取过程中，通过搜索和合并索引内容来定位被搜索记录的主键，与LSM树中的査找工作方式类似，数据记录将会被合并和协调。
>
> ​	**将索引与SSTable生命周期同步的优点之一是，可以在memtable刷盘或压实期间创建索引**。

### * 7.3.2 布隆过滤器

​	**LSM树中读放大的来源是，我们必须寻址多个磁盘驻留表，以便完成读取操作。这是因为我们不一定能预先知道一个磁盘驻留表是否包含要搜索的键指向的数据记录**。

​	<u>防止表査询的方法之一是在元数据中存储其键的范围(存储给定表中的最小和最大键)，并检査要搜索的键是否在该表的范围之内</u>。这一信息是不精确的，它只能告诉我们数据记录是否可能会出现在表中。为了改进这种情况，包括 Apache Cassandra和 ROCKSDB在内的许多实现都使用一种称为布隆过滤器( Bloom Filter)的数据结构。

> ​	**概率型数据结构通常比它对应的“常规”数据结构具有更高的空间效率**。
>
> ​	例如，要检查集合成员身份、基数(找出集合中不同元素的数量)或频率(找出某个元素出现的次数)，我们必须储存所有集合元素，然后遍历整个数据集以找到结果。而**概率结构则允许我们储存近似信息并执行查询，从而产生带有不确定因素的结果。**
>
> ​	关于这类数据结构的一些众所周知的例子有**布隆过滤器(用于集合成员判断)**、 **HyperLogLog(用于估计基数)**[FLAJOLET12]和**Count Min Sketch(用于估计频率)**[CORMODE12]。

​	布隆过滤器是Burton Howard Bloom在1970年提出的[BLOOM70]，它是一种空间效率很高的概率型数据结构，可以用来测试元素是否是集合的成员。**它可能产生假阳性匹配返回“元素在集合中”的结果，而实际上该元素却不在集合中)，但不会出现假阴性匹配(若返回“元素不在集合中”的结果，则保证该元素肯定不是集合的成员)**。

​	换句话说，可以使用布隆过滤器来判断键是否可能在表中或肯定不在表中。在查询期间跳过布隆过滤器返回“不匹配”的文件，而只访问其余文件，以查明数据记录是否确实存在。**使用与磁盘驻留表相关联的布隆过滤器能显著减少读取过程中要访问的表的数量**。

​	<u>**布隆过滤器使用一个大的比特数组和多个哈希函数构建。将这些哈希函数应用于表中记录的键，并将哈希值作为数组下标来将其对应比特位设置为1。如果哈希函数所确定的所有比特位都为1，则表示该搜索键在该集合中可能是存在的**</u>。

​	<u>在查找过程中，当检查布隆过滤器中的元素是否存在时，需要再次计算键的哈希函数：如果所有哈希函数确定的位都为1，则返回肯定的结果，说明该项目有一定概率是集合中的成员；如果至少有一个位为0，则我们可以肯定地说该元素不存在于集合中</u>。

​	应用于不同键的哈希函数可能返回相同的比特位并导致哈希冲突，比特位为1仅表示某个哈希函数为某个键产生了该比特位上的一个置位。

​	<u>假阳性的概率是通过配置比特集的大小和哈希函数的数量来控制的：在较大的比特集中，冲突的概率较小；同样，若拥有更多的哈希函数，则我们也可以检查更多的比特位，这也将产生一个更精确的结果</u>。

​	较大的比特集占用较多的内存，而用较多的哈希函数计算结果又可能会对性能产生负面影响，因此我们必须在可接受的概率和产生的开销之间进行权衡。**概率可以从预期的集合大小中计算出来。因为LSM树中的表是不可变的，所以集合大小(表中键的数目)是预先知道的**。

​	让我们来看一个简单例子。我们有一个16位的比特数组和3个哈希函数，它们产生key1的值3、5和10。我们现在在这些位置上设置比特位。添加下键，用哈希函数产生key2的值5、8和14，我们也为key2设置比特位。

​	<u>现在，我们尝试检查key3是否存在于集合中，用哈希函数生成3、10和14。由于在添加key1和key2时所有三个位都被置位，所以我们会遇到这样一种情况，即布隆过滤器返回一个假阳性报告：key3从未被添加到那里，但所有被计算的位都被置位</u>。但是，由于布隆过滤器只声明元素可能在表中，所以这个结果是可以接受的。

​	如果我们尝试査找key4并由哈希函数得到值5、9和15，那么我们会发现只有第5位被置位，而其他两位未置位。<u>即使只有其中一位未置位，我们也可以肯定该元素从未被添加到过滤器中</u>。

### * 7.3.3 跳表

> [Skip list - Wikipedia](https://en.wikipedia.org/wiki/Skip_list)
>
> [跳表_百度百科 (baidu.com)](https://baike.baidu.com/item/跳表/22819833?fr=aladdin)
>
> [关于redis中zset底层跳表的理解 - Code2020 - 博客园 (cnblogs.com)](https://www.cnblogs.com/cxy2020/p/13799047.html)

![img](https://upload.wikimedia.org/wikipedia/commons/2/2c/Skip_list_add_element-en.gif)

​	有许多不同的数据结构用于在内存中保存有序数据，其中一种结构由于其简单性越来越受欢迎，它被称为跳表(skiplist)[PUGH90b]。<u>从实现的角度来看，跳表并不比单链表复杂多少，而其概率复杂度保证与搜索树接近</u>。

​	跳表不需要为插入和更新而旋转或者移动元素，而是使用概率性平衡。跳表通常不如内存B树那样缓存友好，因为跳表的节点很小，而且在内存中是随机分配的。有些实现通过使用松散链表(unrolled linked list)来对这种情况进行改进。

​	跳表由一系列高度不同的节点组成，构建了链接在一起的层次结构，允许跳过数据项的范围。每个节点拥有一个键，并且与链表中的节点不同，有些节点不止有一个后继节点。高度为h的节点与高度小于等于h的一个或多个前导节点所链接。最低层上的节点可以与任意高度的节点所链接。

​	<u>节点高度由随机函数确定，并在插入时计算出来</u>。具有相同高度的节点形成层。层数设有上限以邐免无限增长，并且最大高度是基于该结构可以容纳的数据项的数量来选择的。每一层节点的数量相比上一层是以指数级减少的。

​	首先跟随最高层上的节点指针进行査找。搜索一旦遇到一个节点的键大于所搜索的键，就会先回到其前序节点，跟随其前序节点低一层的链接到达一个新的节点。换句话说如果搜索的键大于当前节点键，则搜索会继续向前。如果搜索的键小于当前节点的键，则从前一个节点开始在低一层上继续搜索。递归地重复这个过程，直到找到所搜索的键或其前序节点。

​	在插入过程中，使用上述算法找到插入点(持有键的节点或其前序节点)，并且创建新节点。为了构建树状层次结构并保持平衡，**使用基于概率分布生成的随机数来确定节点的高度**。持有比新节点小的键的前序节点将被链接到新节点。其较高层的指针则保持不变。新创建的节点中的指针链接到每一层上相应的后继节点。

​	在删除过程中，被删除节点的前序指针被放置到相应层上的前置节点上。

​	<u>我们可以通过实现一个线性化方案来创建一个**跳表的并发版本**，该方案使用一个额外的fully_linked标志来确定节点指针是否被完全更新。可以使用**CAS**[THERLIHY10]来设置此标志。因为必须在多个层上更新节点指针才能完全恢复跳表的结构，所以这个标志是必需的</u>。

​	在具有非托管内存模型的语言中，可以使用引用计数或危险指针来确保当前引用的节点在被并发访问时不会被释放[RUSSELL2]。**这种算法是无死锁的，因为节点总是从更高层开始访问**。

​	Apache Cassandra将跳表用于二级索引memtable实现。WiredTiger在一些内存操作里使用了跳表。

### 7.3.4 磁盘访问

​	由于大多数表的内容都驻留在磁盘上，并且存储设备通常允许按块访问数据，所以许多LSM树的实现依赖于页缓存，并通常将其用于磁盘访问和中间缓存。在5.1节中描述的许多技术，如页换出和固定，仍然适用于日志结构存储。

​	**最显著的区别是内存中的内容是不可变的，因此不需要额外的锁或闩锁来控制并发访问**。引用计数用于确保当前访问的页不会被从内存中换出，并且保证在压实过程中发出的请求在底层文件被移除之前均已经完成。

​	**另一个区别是LSM树中的数据记录不一定是页对齐的，指针可以使用绝对偏移量而不是页ID来实现寻址**。与磁盘块不对齐的记录，有些记录跨越页边界，需要在内存中加载多个页。

### 7.3.5 压缩

​	我们已经在B树的上下文中讨论过压缩(参见4.6节)。类似的思想也适用于LSM树。<u>这里的主要区别在于LSM树表是**不可变**的，并且通常是在一次请求中写入的</u>。当按页压缩数据时，压缩后的页不是页对齐的，因为它们的大小比未压缩的小。

​	为了能够处理被压缩的页，我们需要在写入它们的内容时跟踪它们的地址边界。<u>我们可以用零来填充压缩的页，使它们与页大小对齐，但这样我们就失去了压缩的好处</u>。

​	**为了使被压缩的页可寻址，我们需要一个间接层来存储<u>偏移量</u>和<u>压缩页的大小</u>**。压缩后的页一定比原始的小，否则压缩它们就没有意义了。

​	在压缩和刷盘期间，压缩的页被顺序地追加，并且压缩信息(原始未压缩的页偏移量和实际压缩的页偏移量)存储在单独的文件段中。在读取过程中，将压缩后的页偏移量及其大小查询出来，之后在内存中解压缩并物化。

## 7.4 无序LSM存储

​	到目前为止讨论过的大多数存储结构都是有序地存储数据的。

​	可变和不可变的B树页FD树中的有序段、LSM树中的SSTable都是<u>按**键顺序**存储数据记录</u>的。这些结构保留顺序的方式有所不同：

+ B树的页是原地更新的
+ FD树的有序段是通过合并两个有序段来创建的
+ 而SSTable通过在内存中缓冲和排序数据记录而创建。

​	在本节中，我们将讨论按随机顺序存储数据记录的结构。**无序存储一般不需要单独的日志，并且允许我们按<u>插入顺序</u>存储数据记录以减少写入的开销**。

### * 7.4.1 Bitcask

> [Bitcask 存储模型 - 如果的事 - 博客园 (cnblogs.com)](https://www.cnblogs.com/chenny7/p/4572381.html)
>
> [优雅的Bitcask/BeansDB - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/53682577)

​	Bitcask是Rik中使用的存储引擎之一，它是一个无序的日志结构存储引擎[SHEEHY0b]。与此前讨论的日志结构存储实现不同，**它不使用memtable进行缓冲，而是将数据记录直接存储在日志文件中**。

​	<u>为了使值可被搜索，Bitcask使用一种名为keydir(键目录)的数据结构，它保存与键相应的**最新数据记录**的引用。旧的数据记录可能仍然在磁盘中，但不会被keydir引用，并且它们会在压实过程中被垃圾收集</u>。

​	**keydir被实现为内存中的哈希表，并且<u>在启动期间必须从日志文件中重建</u>**。

​	在一次写入中，键和数据记录被顺序地追加到日志文件中，并且会在keydir中放入指向新写入的数据记录位置的指针。

​	读取时会査看keydir以定位要搜索的键，并跟随指向对应日志文件的指针以定位数据记录。**由于在任何给定时刻都只能有一个值与keydir中的键相关联，所以点查询不必合并来自多个数据源的数据**。

​	日志文件保存数据记录，而keydir则指向与每个键相关联的最新活动数据记录。数据文件中被遮蔽的记录(被后来的写入或删除所取代的记录)显示为灰色。

​	在压实过程中，所有日志文件的内容被顺序读取、合并、写入到新的位置，只保留活动数据记录，丟弃被遮蔽的数据。使用指向移动过的数据记录的新指针来更新keydir。

​	**数据记录直接存储在日志文件中，因此不必维护单独的预写日志，这减少了空间开销和写放大**。这<u>种方法的缺点是它**只支持单点查询，不允许范围扫描**，因为数据项在keydir和数据文件中都是无序的</u>。

​	这种方法的优点是简单，其**拥有良好的单点查询性能**。尽管存在多个版本的数据记录，但只有最新的一个版本由keydir处理。

​	然而，<u>这种方法**必须将所有键保留在内存，并且在启动时需要重建keydir**，这些限制可能使其无法支持某些用例</u>。虽然其非常适合单点查询，但它不支持范围查询。

### 7.4.2 WiscKey

​	范围査询对于许多应用程序都很重要，如果有一种存储结构既有无序存储的写入和空间优势，同时还支持执行范围扫描就好了。

​	WiscKey[LU16]通过在LSM树中保存键排序并在称为vLog(值日志)的无序仅追加文件中保存数据记录，来将排序与垃圾收集解耦。这种方法可以解决讨论Bitcask时提到的两个问题：需要将所有键保留在内存中，以及需要在启动时重新构建哈希表。

​	**vLog文件保存无序数据记录。键存储在排序的LSM树中，指向日志文件中最新的数据记录**。

​	由于键通常比与其相关联的数据记录小得多，所以压实它们的效率要高得多。这种方法对于更新和删除较少的场景特别有用，在这种情况下，垃圾收集不会释放太多的磁盘空间。

​	这里的主要挑战是，**由于vLog数据是未排序的，所以范围扫描需要随机IO**。 WiscKey在范围扫描时使用固态硬盘的内部并行性来并行预取数据块，以减少随机O的开销。<u>在数据块传输方面，其成本仍然很高：在范围扫描期间，要获取一条的数据记录，必须读取该数据记录所在的整个页</u>。

​	在压实过程中，vLog文件的内容被顺序读取，并在合并后写入新的位置。指针(键LSM树中的值)被更新以指向这些新位置。为了避免扫描整个vLog的内容， WiscKey使用头部(head)和尾部(tail)指针持有有关vLog段的信息，这些vLog段中仍保留着存活的键。

​	**由于vLog中的数据未排序，并且不包含存活信息，所以必须扫描键树以査找哪些值仍然是存活的**。在垃圾收集期间执行这些检查引入了额外的复杂度：<u>传统的LSM树可以在压实期间直接解析文件内容，而无须处理键索引</u>。

## * 7.5 LSM树中的并发

​	LSM树中的并发挑战主要与切换表视图(在刷写和压缩过程中更改的内存驻留表和磁盘驻留表的集合)和日志同步有关。 memtable通常也是并发访问的(除了ScyllaDB这样的核心分区存储)，但是并发内存数据结构不在本书的讨论范围之内。

​	在**刷写期间**，必须遵循以下规则：

+ **新的memtable必须对读写可用**。
+ **旧的(正在刷写的)memtable必须对读保持可见。**
+ **正在刷写的memtable必须写到磁盘上。**
+ **"丢弃已经刷写的memtable"与"创建刷写磁盘驻留表"这两个操作必须被原子地执行。**
+ **预写日志中，记录之前曾应用于被刷写memtable的操作的日志段必须被丢弃**。

​	例如， Apache Cassandra通过使用**操作顺序屏障**来解决这些问题：在memtable刷写之前，所有已经接受的写操作都必须等待。这样，刷写进程(作为消耗者)就知道哪些其他进程(作为生产者)依赖于它。

​	更一般地，我们有以下同步点：

+ **memtable切换**

  在此之后，所有的写操作都去到新的memtable，使其成为主memtable，而旧的memtable仍可用于读操作。

+ **刷写完成**

  <u>在表视图中用刷写完的磁盘驻留表来替换旧的memtable</u>。

+ **预写日志截断**

  丢弃持有与被刷写memtable相关联的记录的日志段。

​	这些操作隐含着严苛的正确性要求。继续写入旧的memtable可能导致数据丢失(例如，如果写入已被刷写的memtable段)。类似地，<u>在驻留磁盘的对应部分准备就绪之前，如果旧的memtable不提供读取，则将导致读取到不完整的结果</u>。

​	在压实期间，表视图也会更改，但这里的过程稍微简单一些：旧的磁盘驻留表被丢弃而被压实的版本则被添加进来。**旧表必须保持可访问性，直到新表完全写入并准备好代替它进行读操作**。<u>同一个表参与多个并行压实的情况也必须避免</u>。

​	<u>**在B树中，日志截断必须与从页缓存中刷写脏页相协调，以保证持久性**。在LSM树中，我们有一个类似的要求：写入操作在memtable中被缓冲，并且它们的内容在完全刷盘之前是不具备持久性的，因此**日志截断必须与memtable刷盘相协调**。刷盘一旦完成，日志管理器就会得到最新的已刷写日志段的信息，因此可以安全地丢弃其中的内容</u>。

​	**不将日志截断与刷盘同步也会导致数据丢失**：如果在刷盘完成之前丢弃了日志段，并且在此时节点崩溃，那么这些日志内容将不会被重放，从而导致该段的数据也不会被恢复。

## 7.6 日志堆叠

​	许多现代文件系统都使用日志结构：它们在内存段中缓冲写操作，并在缓冲满时以仅追加方式将内容刷写到磁盘上。固态硬盘也使用日志结构存储，以处理小的随机写入、最小化写入开销、均衡磨损以及延长设备寿命。

​	随着固态硬盘变得更经济，**日志结构存储(Log-Structured Storage，LSS)**系统开始流行。LSM树和固态硬盘是一个很好的搭配，因为<u>序列化的工作负载和仅追加写入有助于减少原地更新带来的放大，而**原地更新会对固态硬盘的性能产生负面影响**</u>。

​	如果将多个日志结构系统堆叠在一起，那么我们可能会遇到几个使用LSS要试图解决的问题，包括写放大、碎片化和糟糕的性能。至少，在开发应用程序时，我们需要牢记**固态硬盘闪存转换层**和文件系统[YANG14]。

### * 7.6.1 闪存转换层

> [Flash memory controller - Wikipedia](https://en.wikipedia.org/wiki/Flash_memory_controller#Flash_translation_layer_(FTL)_and_mapping)
>
> [闪存转换层_百度百科 (baidu.com)](https://baike.baidu.com/item/闪存转换层)

​	在固态硬盘中使用日志结构映射层是由两个因素驱动的：

1. 小型随机写入必须在物理页中一起被批处理；
2. **固态硬盘是通过使用编程/擦除周期(program/erase cycle)来工作的。只能对先前擦除过的页进行写操作意味着非空页(未被擦除的页)不能被编程(换句话说，写入)**。

​	单个页不能被擦除，只有块中的页组(通常拥有64-512个页)才能被一起擦除。页被分组成多个块。**闪存转换层(FTL)将逻辑页地址转换到它们的物理位置，并跟踪页的状态(存活、丟弃或空)**。当FTL耗尽空闲页时，它不得不执行垃圾收集并擦除丢弃的页。

​	**不能保证数据块内所有即将被擦除的页都是被丢弃的。在擦除数据块之前，FTL必须将它里面的存活页转移到一个包含空页的数据块中**。

​	当所有的存活页被转移后，数据块便可以被安全地擦除，其空闲页就可以用于写操作。<u>由于FTL知道页状态和状态转换，并且拥有所有必要的信息，所以它还负责固态硬盘的**磨损均衡**</u>。

> **磨损均衡将负载均匀地分布在介质上，从而避免热点**。
>
> 在热点上的编程/擦除周期数高，因而会导致数据块过早失效。这是必需的，因为**闪存内存单元只能承受有限数量的编程一擦除周期，均匀地使用内存单元有助于延长器件的寿命**。

​	总结起来，在固态硬盘上使用日志结构存储的动机是将小的随机写入缓冲在一起进行批处理以摊销IO成本，这通常会**减少操作的数量，进而减少触发垃圾收集的次数**。

### 7.6.2 文件系统日志记录

​	在此之上，许多文件系统也使用日志记录技术进行写缓冲，以减少写放大，并以最优化的方式使用底层硬件。

​	日志堆叠(log stacking)以几种不同的方式体现。首先，每一层都必须进行该层自己的信息记录，而且最常见的情况是，<u>底层日志不会暴露必需的信息给上层，从而避免重复工作</u>。

​	由于各层之间不会沟通与LSS相关的调度(例如，丢弃或重新定位段)，所以低层子系统可能对丟弃的数据或即将丢弃的数据执行冗余操作。同样，由于没有单一的标准段大小，所以可能会出现未对齐的高层段占据多个低层段的情况。所有这些开销都是可以减少或完全避免的。

​	即使我们说日志结构存储都是关于顺序IO的，我们也必须记住数据库系统可能有多个写入流(例如，<u>日志写操作与数据记录写操作并行</u>)[YANG14]。当在硬件层上考虑时，交错的顺序写入流可能没有转换成相同的顺序模式：块不一定会按写入顺序放置。

​	这就导致了碎片化，而这则是我们想避免的。为了减少交错，一些数据库厂商建议将日志保存在单独的设备上，以隔离工作负载，并能够独立地考虑它们的性能和访问模式。然而，更重要的是**保持分区与底层硬件的对齐**[NTEL14]，并**保持写入与页大小的对齐**KM12]。

## 7.7 LLAMA与精心堆叠

​	在6.5节中，我们讨论了B树的一个不可变版本，称为Bw树。Bw树是构建在无闩锁、日志结构、访问方法感知(Latch-free，Log-Structured， Access-Method Aware， LLAMA)之上的存储子系统。这种分层使得Bw树可以动态地增长和收缩，同时使树的垃圾收集和页管理是透明的。这里，我们最感兴趣的是访问方法感知部分，它展示了软件层之间协调带来的好处。

​	回顾一下，一个Bw树的逻辑节点由物理增量节点的链表，即一个从最新节点到最旧节点的更新链所组成，这个链表以基节点结束。逻辑节点使用内存中的映射表链接，该表指向磁盘上最新更新的位置。键和值被添加到逻辑节点或从逻辑节点中移除，但是它们的物理表示保持不变。	

​	日志结构将缓冲节点更新(增量节点)存储到4MB大小的刷写缓冲区中。页一旦被填满，就会被刷写到磁盘上。垃圾收集会周期性地回收未使用的增量节点和基节点所占用的空间，并移动存活的节点以释放碎片化的页。

​	在没有访问方法感知的情况下，交替产生的属于不同逻辑节点的增量节点会按照插入顺序被写入。 LLAMA中的Bw树感知可以将几个增量节点合并到单一的连续物理位置。<u>如果增量节点中的两个更新相互抵消(例如，一个插入后面接着一个删除)，则也可以执行它们的逻辑合并，并且可以只持久化后面的删除操作</u>。

​	LSS垃圾收集还会负责合并逻辑Bw树节点的内容。这意味着垃圾收集不仅可以回收可用空间，而且可以显著减少物理节点的碎片。如果垃圾收集只是连续重写了几个增量节点，则它们仍将占用相同数量的空间，并且读取者还需要执行将增量更新应用到基节点的工作。同时，如果较髙层的系统合并了节点并将它们连续地写入新位置，则LSS仍然不得不对旧版本进行垃圾收集。

​	通过了解Bw树的语义，可以将几个增量节点重写为单个基节点，其中所有增量都已在垃圾收集期间进行了应用。这减少了用于表示这个Bw树节点所需的总空间和读取页所需的延迟，同时回收了被丢弃的页所占用的空间。

​	如果仔细考虑，那么堆叠可以产生许多好处。没有必要总是建造紧密耦合的单层结构。好的API和对外公开正确的信息可以显著提高效率。

---

+ 开放通道固态硬盘

  ​	<u>堆叠软件层的另一种选择是跳过所有间接层而直接使用硬件</u>。例如，直接面向开放通道固态硬盘进行开发，可以避免使用文件系统和闪存转换层。这样，我们就可以避免至少两层日志，并且可以更好地控制磨损均衡、垃圾收集、数据分布和调度。使用这种方法的实现之一是开放通道固态硬盘上基于LSM树的KV存储( LSM Tree- based KV Store on Open-Channel SSD，LOCS)[ZHANG13]。使用开放通道固态硬盘的另一个例子是Lightn，它是在Linux内核中实现的[BJORLING17]。

  ​	闪存转换层(FTL)通常处理数据分布、垃圾收集和页移动。开放通道固态硬盘越过FTL直接暴露其内部信息、驱动器管理和IO调度。虽然从开发人员的角度来看需要关注更多的细节，但这种方法可能会带来显著的性能改进。你可以使用O_ DIRECT标志来绕过内核页缓存，这将提供更好的控制，但需要手动的页管理。

  ​	<u>软件定义闪存(Software Defined Flash，SDF)[OUYANGI4]是一种**软硬件协同设计**的开放通道固态硬盘系统，它开放了一种考虑固态硬盘特性的非对称IO接口。SDF的读写单元的大小不同，并且写单元大小对应于擦除单元大小(数据块)，这大大降低了写放大。这种设置对于日志结构存储非常理想，因为只有一个软件层执行垃圾收集和转移页。此外，开发人员可以利用固态硬盘的内部并行性，因为SDF中的每个通道都作为一个单独的块设备被暴露出来，这可以用来进一步提高性能</u>。

  ​	将复杂度隐藏在一个简单的API后面听起来可能很吸引人，但是在软件层具有不同语义的情况下可能会引起一些复杂问题。暴露一些下层系统的内部情况可能有利于更好地进行集成。

## 7.8 本章小结

​	**日志结构存储(LSS)**使用广泛：从闪存转换层到文件系统和数据库系统。**在内存中，将小的随机写入放在一起进行批量处理有助于减少写放大**。为了回收被移除段所占用的空间，LSS定期触发垃圾收集。

​	LSM树借鉴了LSS的一些思想，构建以日志结构方式管理的索引结构：写入在内存中成批处理并刷写到磁盘；被遮蔽数据记录在压实过程中被清理。

​	重要的是要记住许多软件层都在使用LSS，并确保层的堆叠是最佳的。或者，我们可以完全绕过文件系统层，直接访问硬件。

# 第一部分总结

​	在第一部分中，我们讨论了存储引擎。我们从高级的数据库系统架构和分类开始，介绍了如何实现磁盘存储结构，以及它们是如何与其他组件融为一体的。

​	我们从B树开始介绍了几个存储结构。这些讨论过的结构并不能代表整个领域，该领域还有许多其他有趣的发展。然而，这些例子仍然很好地阐述了我们在本部分开始时所确定的三个属性：**缓冲、不可变性和有序性**。这些属性对于描述、记忆和表达存储结构的不同方面很有用。

|            | 缓冲                             | 可变性 | 有序性                           |
| ---------- | -------------------------------- | ------ | -------------------------------- |
| B+树       | 否                               | 是     | 是                               |
| WiredTiger | 是                               | 是     | 是                               |
| LA树       | 是                               | 是     | 是                               |
| COW B树    | 否                               | 否     | 是                               |
| 2C LSM树   | 是                               | 否     | 是                               |
| MC LSM树   | 是                               | 否     | 是                               |
| FD树       | 是                               | 否     | 是                               |
| Bitcask    | 否                               | 否     | 否                               |
| WiscKey    | 是(仅使用缓冲来保持键的排序顺序) | 否     | 是(仅使用缓冲来保持键的排序顺序) |
| Bw树       | 否                               | 否     | 否(只有合并节点才持有有序记录)   |

​	**增加内存缓冲区总是对写放大有积极的影响**。在像 WiredTiger和LA树这样使用原地更新的数据结构中，内存缓冲区有助于通过合并多个相同页的写入来均摊它们的开销。换句话说，**缓冲有助于减少写放大**。

​	在不可变数据结构中(如多组件LSM树和FD树)，缓冲具有类似的积极作用，但是以未来发生的重写为代价，这些重写发生在将数据从一个不可变的层移动到另一个不可变层时。换句话说，**使用不可变性可能会将写放大延迟**。同时，**使用不可变性对并发性和空间放大有积极的影响**，因为所讨论的大多数不可变结构使用的页是全满的。

​	当使用不可变性时，除非我们也使用缓冲，否则最终会得到像Bitcask和WiscKey这样的无序存储结构(写时复制B树除外，它复制、重新排序和转移其页)。 WiscKey仅将键存储在有序的LSM树中，并允许使用键索引来以键的顺序获取记录。在Bw树中，一些节点(被合并的节点)按键的顺序保存数据记录，而其余的逻辑Bw树节点的增量更新可能分散在不同的页上。

​	可以看到，这三个属性可以混合和配对，以实现所需的特性。不幸的是，存储引擎设计通常涉及各种权衡：<u>**优化一种操作可能会增加另一种操作的开销**</u>。

​	利用这些知识，你应该能更仔细地研究大多数现代数据库系统的代码了。在本书中可以找到一些代码引用和出发点。了解并理解术语将使这一过程变得更加容易。

​	许多现代数据库系统都是由概率性数据结构驱动的[FLAJOLET12， CORMODE04]，有一项新的研究正在将机器学习的思想引入数据库系统[KRASKA18]。随着非易失性、字节可寻址存储设备变得越来越普遍和广泛，学术界和工业界将经历更多的变化IVENKATARAMAN1]。

​	了解本书中描述的基本概念应该有助于你理解和实现新的研究成果，因为这些研究一定也借鉴了并构建在相同的概念之上。**<u>了解理论和历史的最大好处是明白没有什么是全新的，正如本书的叙述所显示的，发展是渐进的</u>**。
